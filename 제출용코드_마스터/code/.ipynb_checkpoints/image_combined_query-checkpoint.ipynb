{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import gc\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import choice, seed, shuffle, random, sample\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, CuDNNGRU as GRU, CuDNNLSTM as LSTM, Dropout, BatchNormalization\n",
    "from keras.layers import Dense, Concatenate, Activation, Embedding, SpatialDropout1D, Bidirectional, Lambda, Conv1D\n",
    "from keras.layers import Add, Average, TimeDistributed, GlobalMaxPooling1D\n",
    "from keras.optimizers import Nadam, Adam, Adamax\n",
    "from keras.activations import absolute_import\n",
    "from keras.legacy import interfaces\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import Callback\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.initializers import he_normal\n",
    "# from keras_bert.bert import get_model\n",
    "from keras_bert.loader import load_trained_model_from_checkpoint\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "from keras.engine import Layer\n",
    "from keras.engine import InputSpec\n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.objectives import sparse_categorical_crossentropy\n",
    "from keras import activations, initializers, regularizers, constraints\n",
    "from keras.models import Model\n",
    "from tqdm import tqdm\n",
    "from model_utils import seq_gather, seq_and_vec, seq_maxpool\n",
    "from keras.models import load_model\n",
    "from keras_bert import get_custom_objects\n",
    "from keras_bert import Tokenizer\n",
    "from collections import defaultdict\n",
    "from eval import read_submission, get_ndcg\n",
    "from tqdm import tqdm, trange\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "KTF.set_session(sess)\n",
    "\n",
    "BERT_PRETRAINED_DIR = \"../data/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/\"\n",
    "VAL_ANS_PATH = '../data/valid/valid_answer.json'\n",
    "LABEL_PATH = '../data/multimodal_labels.txt'\n",
    "\n",
    "MAX_EPOCH = 20\n",
    "MAX_LEN = 20\n",
    "B_SIZE = 128\n",
    "FOLD_IDS = [-1]\n",
    "FOLD_NUM = 20\n",
    "THRE = 0.5\n",
    "SHUFFLE = True\n",
    "MAX_BOX = 10\n",
    "MAX_CHAR = 8\n",
    "PREFIX = \"[image_combined_query]\"\n",
    "SEED = 2018711118\n",
    "ACCUM_STEP = int(128 // B_SIZE)\n",
    "SAVE_EPOCHS=[10, 20, 35, 50, 80, 100]\n",
    "IMAGE_LABEM_CONCAT_TOKEN = \"###\"\n",
    "CONCAT_TOKE = \"[unused0]\"\n",
    "\n",
    "cfg = {}\n",
    "cfg[\"verbose\"] = PREFIX\n",
    "cfg[\"base_dir\"] = BERT_PRETRAINED_DIR\n",
    "cfg['maxlen'] = MAX_LEN\n",
    "cfg[\"max_box\"] = MAX_BOX\n",
    "cfg[\"max_char\"] = MAX_CHAR\n",
    "cfg[\"lr\"] = 1e-4\n",
    "cfg['min_lr'] = 6e-8\n",
    "cfg[\"opt\"] = \"nadam\"\n",
    "cfg[\"loss_w\"] =  20.\n",
    "cfg[\"trainable\"] = True\n",
    "cfg[\"bert_trainable\"] = True\n",
    "cfg[\"mix_mode\"] = \"\"\n",
    "cfg[\"unit1_1\"] = 128\n",
    "cfg[\"accum_step\"] = ACCUM_STEP\n",
    "cfg[\"cls_num\"] = 4\n",
    "cfg[\"raw_filename\"] = \"{}_{}oof{}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab():\n",
    "    if \"albert\"in cfg[\"verbose\"].lower():\n",
    "        dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab_international.txt')\n",
    "    else:\n",
    "        dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
    "    with open(dict_path, mode=\"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [l.strip() for l in lines]\n",
    "\n",
    "    word_index = {v: k  for k, v in enumerate(lines)}\n",
    "    return word_index\n",
    "\n",
    "\n",
    "word_index = get_vocab()\n",
    "cfg[\"x_pad\"] = word_index[\"[PAD]\"]\n",
    "tokenizer = Tokenizer(word_index)\n",
    "\n",
    "\n",
    "def get_label(path):\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        label2id = {l.split('\\n')[0].split('\\t')[1]:int(l.split('\\n')[0].split('\\t')[0]) for l in lines[1:]}\n",
    "        id2label = {int(l.split('\\n')[0].split('\\t')[0]):l.split('\\n')[0].split('\\t')[1] for l in lines[1:]}\n",
    "    return label2id, id2label\n",
    "\n",
    "\n",
    "label2id, id2label = get_label(LABEL_PATH)\n",
    "label_set = set(label2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 <class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>label_words</th>\n",
       "      <th>features</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8330</th>\n",
       "      <td>blank minimalist notepad</td>\n",
       "      <td>digital supplies###others###others###others</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.23326792, 0.0, 0.0, 0....</td>\n",
       "      <td>[[0.25375, 0.7025, 0.30625, 0.645, 0.152014062...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>uv for men and women glasses</td>\n",
       "      <td>bottle drink###accessories (jewelry, clothing ...</td>\n",
       "      <td>[[0.0, 0.0, 0.04133281, 0.0, 0.0, 0.256621, 0....</td>\n",
       "      <td>[[0.42875, 0.69625, 0.0575, 0.20625, 0.0397906...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3352</th>\n",
       "      <td>wool autumn sweater</td>\n",
       "      <td>top clothes (coat, jacket, shirt, etc.)###huma...</td>\n",
       "      <td>[[0.05242783, 0.0, 0.09163539, 0.0, 0.0, 0.189...</td>\n",
       "      <td>[[0.33375, 0.97375, 0.24125, 0.8125, 0.3656], ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             words  \\\n",
       "8330      blank minimalist notepad   \n",
       "9995  uv for men and women glasses   \n",
       "3352           wool autumn sweater   \n",
       "\n",
       "                                            label_words  \\\n",
       "8330        digital supplies###others###others###others   \n",
       "9995  bottle drink###accessories (jewelry, clothing ...   \n",
       "3352  top clothes (coat, jacket, shirt, etc.)###huma...   \n",
       "\n",
       "                                               features  \\\n",
       "8330  [[0.0, 0.0, 0.0, 0.0, 0.23326792, 0.0, 0.0, 0....   \n",
       "9995  [[0.0, 0.0, 0.04133281, 0.0, 0.0, 0.256621, 0....   \n",
       "3352  [[0.05242783, 0.0, 0.09163539, 0.0, 0.0, 0.189...   \n",
       "\n",
       "                                                    pos  \n",
       "8330  [[0.25375, 0.7025, 0.30625, 0.645, 0.152014062...  \n",
       "9995  [[0.42875, 0.69625, 0.0575, 0.20625, 0.0397906...  \n",
       "3352  [[0.33375, 0.97375, 0.24125, 0.8125, 0.3656], ...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/temp_data.pkl', 'rb') as outp:\n",
    "    train_data = joblib.load(outp)\n",
    "\n",
    "\n",
    "with open('../data/val_data.pkl', 'rb') as outp:\n",
    "    val_data = pickle.load(outp)\n",
    "    \n",
    "print(len(train_data), type(train_data))\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "train_data.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] l = ['.', '.', '.', '-0.1573', '-0.29517', '0.30453', '-0.54773', '0.098293', '-0.1776', '0.21662', '0.19261', '-0.21101', '0.53788', '-0.047755', '0.40675', '0.023592', '-0.32814', '0.046858', '0.19367', '0.25565', '-0.021019', '-0.15957', '-0.1023', '0.20303', '-0.043333', '0.11618', '-0.18486', '0.0011948', '-0.052301', '0.34587', '0.052335', '0.16774', '-0.21384', '0.055947', '0.24934', '-0.12179', '0.16749', '0.28922', '-0.033739', '0.3015', '-0.13241', '0.092635', '0.37155', '-0.2884', '-0.0052731', '-0.001005', '-0.51153', '-0.28476', '-0.20139', '0.11837', '-0.0055891', '0.43604', '0.16796', '-0.2701', '0.063957', '-0.093253', '-0.22079', '0.36501', '0.06545', '0.23941', '-0.19292', '0.098293', '0.12172', '-0.1168', '-0.027436', '0.20507', '-0.39139', '-0.23111', '0.46239', '0.22888', '-0.028415', '-0.1798', '0.23817', '0.28093', '-0.47935', '0.23177', '-0.35587', '0.14246', '0.11861', '0.011018', '0.091986', '0.0054809', '-0.39955', '-0.40183', '-0.10629', '-0.30851', '0.12383', '-0.16737', '-0.43569', '0.4211', '-0.57416', '-0.19964', '0.51312', '0.090747', '-0.21657', '0.043519', '0.24288', '0.081134', '0.49104', '-0.33342', '-0.31056', '-0.3136', '0.26931', '-0.14402', '0.33185', '-0.21662', '-0.072985', '0.080603', '-0.7266', '-0.098385', '-0.36233', '-0.25346', '0.1154', '0.25738', '0.15802', '-0.15633', '-0.024581', '0.35673', '0.31153', '0.33475', '-0.081155', '-0.3061', '0.019077', '-0.049047', '-0.11232', '-0.07417', '0.35596', '-0.2642', '0.012781', '-0.20715', '0.020223', '0.054534', '-0.28803', '0.42863', '-0.10312', '0.24771', '0.013196', '0.19768', '-0.013528', '-0.15134', '0.20307', '-0.028973', '-0.022706', '-0.29199', '-0.082062', '0.19048', '0.0053574', '0.14067', '-0.28675', '0.21343', '0.42428', '-0.28186', '-0.11801', '-0.45227', '-0.0067998', '0.044784', '-0.0062886', '0.25087', '0.34481', '-0.64459', '-0.20467', '0.35007', '0.1468', '-0.14007', '-0.0050219', '-0.24053', '0.41426', '-0.40902', '0.21141', '0.25726', '-0.4883', '0.027066', '0.56367', '-0.39594', '-0.035206', '0.63079', '0.14343', '0.038315', '0.32527', '-0.080335', '-0.20065', '-0.30848', '-0.0031591', '0.15296', '-0.21014', '0.42143', '-0.20944', '-0.069285', '0.13555', '-0.020401', '-0.22555', '0.33491', '0.16035', '0.17739', '-0.023627', '0.097575', '-0.19395', '-0.018754', '-0.119', '-0.0067027', '-0.4178', '0.29027', '0.13034', '-0.30212', '0.61173', '-0.39918', '-0.020191', '-0.34531', '-0.092082', '0.46818', '0.36671', '0.21021', '-0.053162', '-0.37872', '-0.14271', '-0.13604', '0.31715', '-0.17227', '-0.091266', '0.16417', '0.15069', '0.53556', '-0.29678', '0.13965', '-0.29788', '0.1282', '0.1971', '-0.045515', '-0.41355', '-0.050333', '-0.39015', '-0.29579', '-0.096145', '-0.03151', '0.053714', '-0.37309', '-0.36523', '-0.17235', '0.39251', '-0.065909', '-0.25267', '-0.34448', '-0.11503', '0.43665', '0.18832', '0.20631', '0.27801', '-0.046077', '0.13397', '-0.091953', '-0.098542', '0.15811', '0.2752', '0.081383', '0.32077', '-0.10028', '0.1088', '-0.24836', '0.10477', '0.15243', '-0.071302', '0.12861', '0.23061', '0.0074864', '0.090918', '-0.12269', '-0.14831', '0.010586', '0.35745', '-0.23412', '-0.23746', '-0.22646', '-0.27641', '-0.1634', '0.071909', '-0.093884', '0.21331', '-0.20627', '0.44406', '0.34691', '0.019064', '0.034657', '0.36789', '0.32276', '-0.31099', '-0.023443', '-0.77048', '-0.26001', '0.033961', '-0.13874', '0.051973', '-0.0090509', '0.27427', '0.046548', '-0.48214', '-0.1437', '-0.1975', '-0.038126', '-0.16555', '0.071697', '0.049449', '0.15386', '-0.81663']\n",
      "[!] l = ['at', 'name@domain.com', '0.0061218', '0.39595', '-0.22079', '0.78149', '0.38759', '0.28888', '0.18495', '-0.37328', '-0.60018', '0.19625', '0.42975', '0.17942', '0.06375', '-0.44127', '0.72035', '0.50539', '0.17985', '-0.71305', '0.11122', '0.19733', '0.063884', '0.023288', '0.017074', '0.04756', '-0.083167', '0.14506', '-0.21856', '-0.07979', '-0.058909', '-0.79864', '0.65868', '-0.45031', '0.41921', '-0.043908', '-0.059099', '0.21384', '-0.05214', '0.31267', '0.20417', '-0.66489', '-0.17885', '-0.13729', '-0.13825', '-0.59669', '-0.09297', '0.31121', '-0.027161', '0.17923', '0.43076', '0.1012', '0.32516', '-0.34159', '0.21662', '-0.31267', '-0.065984', '-0.35734', '-0.52104', '-0.16432', '-0.31183', '0.33793', '-0.3952', '0.11288', '0.10796', '0.16632', '-0.077719', '0.48287', '-0.50849', '0.37261', '0.27273', '0.36141', '-0.027295', '-0.44898', '0.48865', '0.13509', '0.0082683', '0.44873', '0.46155', '-0.28687', '0.28691', '-0.16943', '-0.4255', '-0.13788', '-0.84983', '0.38833', '-0.092206', '0.09504', '0.57431', '-0.55836', '0.37006', '-0.23685', '0.31727', '-0.29539', '0.40986', '-0.30945', '-0.4857', '0.52769', '-0.94486', '-0.22039', '0.1913', '0.014544', '0.37755', '-0.36721', '-0.07249', '-0.17701', '0.32814', '0.2158', '0.43346', '0.14774', '-0.59317', '-0.23463', '-0.064771', '-0.42996', '0.59558', '0.62189', '0.62726', '-0.97772', '-0.15548', '-0.25757', '0.57236', '0.7448', '0.33936', '0.088019', '0.12554', '0.44163', '0.48089', '0.14379', '0.44052', '0.0033798', '0.17334', '0.62277', '0.13859', '-0.46311', '-0.3785', '-0.09034', '-0.31398', '0.43096', '0.14501', '0.10217', '-0.30989', '-0.20551', '0.95437', '0.36931', '-1.0046', '-0.20179', '0.2239', '0.093057', '0.4248', '-0.14321', '-1.0822', '0.49819', '0.42587', '0.48591', '0.22223', '0.01698', '-0.082398', '0.69111', '0.3967', '-0.50514', '0.12603', '0.2714', '0.7082', '-0.079401', '-0.10775', '-0.48217', '0.85182', '-0.0047614', '0.06633', '0.065622', '-0.31762', '0.14979', '-0.48519', '0.31868', '0.66676', '-0.53971', '0.26185', '-0.025039', '-0.042315', '0.096601', '-0.80947', '0.11489', '0.22729', '0.15132', '0.48739', '-0.17873', '0.86134', '0.20061', '0.018419', '-0.57752', '0.31828', '0.46305', '0.31762', '-0.43985', '0.56173', '0.14558', '-0.23908', '0.94529', '0.35095', '0.09231', '0.38298', '0.18845', '0.12514', '0.11943', '-0.11206', '0.17589', '0.2973', '0.31892', '-0.015596', '0.32167', '0.066616', '-0.04847', '0.31435', '0.12557', '0.036991', '0.17287', '-0.055305', '0.87528', '0.11223', '0.28194', '-0.08516', '0.58705', '0.18693', '-0.32145', '0.46188', '0.51299', '-0.88555', '-0.46913', '-0.26764', '-0.51375', '-0.38804', '-0.78149', '-0.14336', '-0.25571', '0.12985', '0.33621', '-0.22846', '0.48309', '-0.47053', '-0.23177', '0.17509', '0.060625', '0.19792', '-0.10821', '-0.090677', '-0.18252', '0.73394', '-0.17721', '0.28438', '0.75051', '0.37784', '-0.26525', '0.15274', '-0.4612', '-0.60663', '-0.042408', '0.093703', '-0.59664', '0.097915', '-0.46943', '-0.18268', '-0.26516', '-0.14962', '0.05476', '-0.078979', '-0.50688', '0.39585', '0.69243', '0.039802', '0.13965', '0.21709', '0.023105', '0.19744', '0.45611', '0.26455', '-0.17963', '-0.2695', '0.16681', '0.27557', '-0.13141', '-0.12941', '-0.40996', '0.23409', '0.40783', '0.28148', '-0.15247', '0.17776', '0.077139', '0.30951', '0.21843', '-0.075161', '0.33983', '0.30904', '0.1404', '0.5608', '-0.020466', '0.29512', '-0.43178', '-0.31083', '-0.28874', '-0.12015', '0.38455']\n",
      "[!] l = ['0.20785', '0.2703', '0.93632', '-0.50861', '-0.36674', '-0.042177', '-0.37699', '0.051295', '0.61275', '-0.42422', '-0.034888', '-0.19135', '0.64608', '0.5798', '-0.076789', '0.26753', '-0.075789', '-0.35367', '0.19077', '0.14558', '0.0814', '0.24152', '0.046535', '0.32054', '-0.95148', '-0.45128', '-0.34587', '0.20727', '0.089065', '0.062543', '0.20752', '-0.29543', '0.79427', '0.49611', '-0.0011539', '0.15531', '0.21641', '0.14618', '0.29269', '0.054468', '0.51529', '-0.17125', '0.11044', '-0.14752', '-0.69757', '-0.026229', '-0.12325', '-0.12212', '0.11973', '0.19259', '0.40017', '-0.43404', '0.13098', '-0.061418', '-0.17673', '-0.81071', '-0.26014', '-0.47192', '0.61194', '-0.11891', '-0.35838', '-0.33293', '0.84064', '-0.22047', '-0.66912', '0.21979', '0.41868', '0.61312', '0.23073', '0.3371', '-0.12788', '-0.48915', '0.3037', '0.017401', '0.33886', '-0.35593', '-0.57258', '0.46135', '-0.057631', '-0.36917', '-0.53401', '-0.045035', '-0.22714', '0.13939', '-0.025597', '-0.12937', '0.70547', '-0.74619', '0.48864', '-0.13882', '-0.14221', '-0.15263', '-0.45785', '-0.33056', '0.45977', '0.58663', '0.35718', '0.12967', '0.23003', '0.63797', '0.2097', '0.44999', '0.70432', '0.66536', '0.073387', '-0.27004', '0.66423', '-0.95498', '-0.6618', '-0.26601', '-0.10039', '-0.20239', '0.11026', '-0.17554', '0.27676', '0.35229', '-0.55681', '0.30735', '-0.47876', '0.32921', '-0.84468', '-0.1626', '-0.028091', '0.065811', '-0.88467', '0.13294', '0.19156', '-0.57403', '0.22954', '0.19675', '0.51347', '-0.60816', '0.28218', '0.41214', '-1.0258', '0.32263', '-0.047107', '-0.42505', '-0.052501', '-0.59389', '0.53819', '0.039333', '-0.38898', '0.17609', '-0.14058', '0.46852', '-0.31404', '0.30144', '-0.39622', '0.33138', '-0.24075', '0.53604', '-0.27007', '0.78485', '0.38278', '0.71141', '-0.1428', '-0.64956', '0.20509', '-0.023663', '0.21153', '0.19399', '0.57702', '0.15783', '0.046646', '0.20196', '0.092888', '-0.31026', '-0.29377', '0.67172', '-0.21837', '0.26749', '0.60868', '0.019099', '0.28973', '-0.15444', '-0.39144', '-0.095387', '-0.28079', '-0.048308', '0.33878', '-0.44063', '0.30956', '-0.22347', '0.6032', '0.66655', '0.32759', '-0.1309', '0.093239', '-0.41096', '0.17765', '0.16993', '0.27993', '-0.71139', '0.70387', '0.061628', '0.56912', '0.35148', '-0.34838', '0.044756', '0.42752', '0.61708', '0.28497', '0.51549', '0.025433', '0.43062', '0.63013', '-0.35553', '-0.35362', '0.4635', '-0.12006', '0.46648', '-0.062701', '0.71364', '-0.30807', '0.081867', '-0.27198', '0.083946', '0.40306', '0.12492', '0.11022', '-0.77707', '0.29195', '0.10882', '-0.5508', '-0.44303', '-0.26718', '-0.039885', '-1.0369', '-0.093787', '0.2801', '0.64856', '-0.61635', '-0.34963', '-0.38582', '0.65866', '-0.16206', '0.13783', '-0.28909', '0.7419', '0.17221', '0.025555', '0.35493', '-0.092846', '0.028186', '0.084454', '-0.096491', '0.085913', '-0.36999', '0.19963', '-0.57976', '0.016966', '0.67201', '0.32469', '0.19614', '0.17132', '0.35131', '-0.19178', '-0.022078', '0.073446', '-0.24633', '-0.13463', '0.37415', '0.10554', '-0.77579', '0.27255', '0.082846', '-0.50072', '-0.017446', '0.22349', '-0.74991', '-0.35259', '-0.69896', '-0.33915', '0.11149', '-0.00077181', '-0.23194', '0.013805', '-0.4155', '0.28737', '-0.59196', '-0.033756', '-0.19848', '0.21965', '0.017639', '-0.057946', '-0.58171', '-0.10746', '-0.23369', '0.50115', '0.29547', '0.039525', '-0.53819', '-0.13933', '-0.069796', '-0.42341', '0.31655', '-0.22901', '0.51004', '-0.52537']\n",
      "[!] l = ['.', '.', '.', '.', '.', '-0.23773', '-0.82788', '0.82326', '-0.91878', '0.35868', '0.1309', '0.26195', '-0.30068', '0.42963', '-1.5335', '0.50492', '0.59069', '0.17763', '-0.010302', '-0.63371', '-0.040832', '0.69336', '-0.85472', '0.693', '-0.44824', '0.25476', '0.17091', '0.96056', '-0.26516', '0.17454', '-0.27371', '0.20591', '-0.60435', '-0.50923', '0.36049', '0.75756', '-0.79493', '0.12379', '0.40019', '0.56716', '0.19451', '0.15996', '-0.07882', '0.9756', '0.54761', '-0.25123', '-0.70006', '-0.78286', '-0.95702', '0.23045', '-0.3643', '0.80267', '-0.30315', '0.35283', '-0.19961', '0.71134', '-0.94116', '-0.086543', '0.17827', '0.40124', '-0.071006', '0.82202', '-0.51055', '0.47492', '-0.0582', '-0.2687', '0.94119', '0.0066621', '-0.53636', '-0.68859', '0.27149', '0.33482', '0.53987', '-0.72912', '-0.53719', '0.17591', '-0.52643', '0.42509', '0.12433', '-0.17178', '0.53168', '0.69058', '0.703', '0.17845', '-0.39243', '-0.50617', '0.39575', '0.18461', '-0.26065', '-0.61119', '0.013174', '0.32934', '-0.4861', '-0.46474', '0.69677', '0.47846', '-0.43841', '0.051551', '0.4195', '-0.36847', '0.57844', '0.053724', '-0.026579', '-0.5901', '0.73221', '-0.53987', '-0.3428', '-0.12343', '-0.51213', '-0.59237', '-0.14589', '0.094055', '-0.48848', '-0.09944', '0.092527', '0.76764', '-0.21207', '-0.2395', '0.27517', '0.27508', '0.29604', '0.01908', '-0.64152', '0.51208', '0.30948', '-0.47126', '0.86562', '-0.68993', '0.19003', '-0.78941', '-0.10668', '-0.64497', '0.078153', '-0.29598', '-0.2383', '0.40626', '0.066945', '0.021937', '-0.28291', '-0.34962', '0.098551', '-0.068353', '0.06824', '-0.17466', '-0.21469', '1.4612', '0.30606', '-0.28516', '0.078682', '0.54627', '-0.28879', '0.11363', '0.32769', '-0.38409', '-0.038104', '-0.70277', '-0.47623', '-0.29545', '0.1906', '1.2011', '0.38105', '0.24595', '-0.15847', '0.051696', '0.71491', '0.040783', '-0.24121', '-0.69558', '0.8648', '-0.18283', '-0.11746', '-0.058645', '0.033855', '-0.32785', '-0.23554', '-0.12762', '0.088837', '1.1886', '0.6515', '-0.32243', '0.20434', '-0.28283', '-0.076', '-0.5461', '0.66366', '-0.19004', '0.41687', '0.16786', '-0.57624', '0.0021248', '0.038208', '0.46581', '0.17952', '0.15208', '0.77239', '0.56825', '1.3564', '0.36707', '-0.90959', '-0.20369', '-0.30854', '0.43101', '-0.62625', '0.62072', '-0.48968', '-0.4026', '0.83121', '0.27788', '-0.63801', '-0.90269', '-0.26409', '0.55212', '-0.21899', '0.57153', '0.10422', '-0.23276', '0.32775', '0.15975', '0.52786', '-0.18071', '-0.66116', '-0.28231', '-0.95566', '0.32314', '-0.10176', '0.49961', '-0.59512', '0.30664', '0.17566', '-0.0082404', '-1.2304', '-0.18822', '-0.094328', '-0.25801', '0.07948', '0.59872', '-0.029741', '-0.14526', '-0.044699', '0.23121', '0.20907', '-0.442', '0.40599', '0.16151', '-0.49981', '0.13384', '0.35293', '0.034782', '0.513', '-0.39248', '0.39123', '0.28351', '0.62023', '0.6796', '0.83006', '0.48423', '0.70625', '-0.25091', '-0.23268', '0.15136', '-0.35641', '-0.45169', '-0.071751', '0.61275', '0.12885', '-0.62407', '0.38831', '-0.38358', '-0.40596', '0.022178', '0.44976', '-0.11437', '-0.43881', '-0.72876', '-0.76978', '-0.42424', '0.26061', '-0.83878', '0.7724', '-0.050794', '0.0083812', '0.18012', '-0.77772', '-0.22227', '0.79349', '0.3748', '-0.96256', '-0.97854', '-0.92611', '-0.65978', '0.080195', '-0.25126', '1.1857', '-0.72262', '1.0632', '0.50418', '0.07075', '-0.25384', '-0.57426', '-0.19791', '-0.68991', '0.061501', '-0.17089', '0.18609', '-0.78265']\n",
      "[!] l = ['to', 'name@domain.com', '0.33865', '0.12698', '-0.16885', '0.55476', '0.48296', '0.45018', '0.0094233', '-0.36575', '-0.87561', '-0.35802', '0.2379', '0.31284', '-0.081367', '0.061482', '0.81921', '0.77488', '0.68518', '-0.48005', '-0.012098', '0.53366', '0.038321', '0.26857', '0.56736', '0.20427', '0.2847', '0.68113', '-0.26921', '0.10099', '-0.33252', '-0.22999', '0.66003', '0.21833', '-0.086523', '-0.30044', '-0.42253', '0.47525', '-0.2165', '0.3268', '0.63515', '-0.15657', '-0.25835', '-0.11663', '-0.41092', '-0.73779', '-0.0015122', '0.14481', '0.13287', '0.26097', '0.58175', '0.29285', '0.27168', '-0.0058512', '0.27731', '-0.40565', '0.05047', '0.059203', '-0.39081', '-0.098029', '-0.13969', '0.42714', '-0.20103', '-0.019703', '1.1957', '0.36278', '-0.6468', '0.096856', '-0.54383', '0.29666', '-0.0098302', '0.5042', '-0.3419', '-0.25067', '0.72375', '0.81957', '0.57959', '-0.28619', '0.91511', '-0.49127', '0.42129', '0.11429', '-0.32411', '-0.092257', '-1.0342', '-0.25774', '-0.19044', '-0.34201', '1.2339', '-0.65392', '0.83096', '-0.3133', '-0.10256', '-0.075641', '0.88435', '-0.1656', '0.041536', '0.23504', '-0.59116', '-0.12573', '0.48002', '0.23561', '-0.16167', '-0.1404', '-0.45722', '-0.13186', '0.9173', '-0.78831', '-0.027372', '0.036396', '-0.49337', '-0.2556', '-0.15139', '-0.53403', '0.90196', '0.60318', '0.43602', '-0.075461', '-0.09196', '0.14248', '0.14335', '0.67287', '0.52725', '-0.21312', '-0.31636', '0.63391', '0.33626', '-0.037607', '0.60729', '0.46028', '0.0010946', '0.22783', '0.38501', '-0.85127', '-0.76092', '0.20159', '-0.13641', '-0.15588', '0.060973', '0.13442', '-0.25715', '0.054989', '0.82028', '0.32086', '-0.94738', '-0.55948', '-0.072619', '-0.31286', '0.53855', '-0.27876', '-0.45292', '1.0417', '0.59105', '0.10445', '-0.0042194', '-0.11733', '0.38836', '0.43267', '0.43228', '-0.3419', '0.091655', '0.47851', '0.62238', '0.091656', '-0.11656', '-0.6118', '0.076486', '-0.27552', '-0.33407', '0.30166', '-0.089347', '-0.4676', '0.14198', '0.2924', '0.69944', '-0.53839', '0.14589', '0.36273', '-0.18743', '-0.32278', '-0.46289', '-0.0079593', '0.025591', '0.47468', '0.79237', '-0.72242', '0.92688', '0.49471', '-0.39816', '-0.099986', '0.15334', '-0.050664', '0.74726', '-0.047998', '0.43063', '-0.1613', '-0.020673', '0.98916', '-0.27938', '0.10797', '0.55202', '0.37686', '0.17607', '-0.27058', '-0.1362', '-0.024761', '0.63829', '-0.0054866', '0.12085', '0.56029', '0.28192', '0.29992', '0.23869', '0.070296', '0.34272', '-0.11676', '-0.24377', '0.90668', '0.1251', '-0.12149', '-0.057172', '0.36016', '0.057005', '-0.33869', '0.14762', '0.91449', '-0.20497', '-0.64492', '-0.47259', '-0.43348', '-0.47966', '-0.84446', '-0.35207', '0.0050338', '-0.33834', '0.15072', '-0.9816', '0.13519', '-0.33266', '0.11602', '0.032961', '-0.58063', '0.039186', '-0.34515', '-0.088268', '-0.49642', '0.56537', '-0.13342', '0.29861', '0.4289', '0.72562', '-0.61233', '-0.41913', '0.19969', '-0.64272', '-0.62544', '0.58792', '-0.33489', '-0.066748', '-0.42025', '-0.25105', '-0.52392', '-0.17985', '-0.059297', '-0.28532', '-0.76441', '0.41743', '0.83472', '0.46515', '0.3741', '0.35699', '-0.11646', '0.10322', '-0.16215', '-0.30452', '-0.0082364', '-0.49549', '0.47146', '-0.054368', '-0.18286', '0.083092', '-0.2659', '0.449', '0.3066', '0.19909', '0.21744', '0.77927', '-0.09052', '-0.28405', '0.02556', '-0.27124', '-0.26324', '0.45414', '0.19379', '-0.21259', '-0.38467', '0.18494', '-0.26692', '-0.18104', '0.051336', '-0.25989', '-0.15024']\n",
      "[!] l = ['.', '.', '0.035974', '-0.024421', '0.71402', '-0.61127', '0.012771', '-0.11201', '0.16847', '-0.14069', '-0.053491', '-0.87539', '-0.13959', '0.29731', '0.072308', '-0.084514', '-0.1879', '0.12358', '0.37639', '-0.39238', '-0.01111', '-0.04924', '0.63649', '0.058814', '0.19076', '-0.20828', '-0.11036', '0.14934', '0.24667', '-0.39438', '0.22853', '-0.11201', '0.33539', '-0.32929', '-0.049727', '-0.090764', '0.29095', '0.27504', '0.22802', '-0.15616', '0.37302', '0.3752', '-0.3677', '0.1518', '-0.27551', '-0.63281', '-0.31298', '-0.22441', '-0.15435', '-0.64802', '0.28404', '0.12356', '0.0034255', '0.03094', '0.35345', '-0.46781', '0.59203', '-0.17966', '0.27702', '-0.46738', '0.19438', '0.21939', '-0.36743', '-0.084781', '0.03253', '-0.51323', '-0.55466', '0.49585', '0.066985', '0.47906', '-0.25118', '0.011123', '0.15605', '-1.0761', '0.60875', '-0.15764', '0.066122', '0.12779', '-0.089209', '0.4311', '0.045732', '-0.29364', '-0.19994', '-0.065952', '0.26236', '0.34039', '-0.4956', '-0.41187', '0.055566', '-0.69902', '-0.057696', '0.76519', '0.2018', '-0.34497', '-0.22707', '0.34316', '-0.16098', '0.42469', '0.0080257', '-0.33017', '-0.43485', '0.23581', '-0.71085', '0.27985', '-0.31261', '-0.012817', '0.48305', '-0.75151', '-0.02347', '-0.39653', '-0.86857', '0.2877', '0.26678', '0.22291', '-0.1736', '-0.12782', '0.35032', '0.27365', '0.28287', '0.093409', '-0.18104', '-0.088499', '0.1189', '0.026563', '-0.027942', '0.17254', '-0.032427', '-0.10745', '-0.30334', '-0.096047', '-0.45369', '-0.12113', '0.06388', '-0.31841', '0.0059388', '0.17693', '-0.21071', '-0.6171', '0.0018674', '0.27296', '0.18762', '-0.060409', '0.5949', '0.13994', '-0.25773', '0.20023', '0.4918', '0.010659', '0.046456', '0.4339', '-0.10386', '0.021517', '-0.42845', '-0.25458', '-0.1307', '0.28307', '-0.27127', '0.080445', '-0.31285', '-0.12807', '0.71382', '-0.086474', '-0.35553', '0.65281', '-0.33706', '0.38617', '-0.12551', '0.0056478', '0.10091', '-0.3638', '-0.033486', '0.32146', '-0.28719', '-0.24936', '0.65761', '0.24376', '-0.068703', '0.35459', '0.080304', '-0.30996', '-0.20199', '0.31482', '0.15092', '-0.25125', '0.76149', '-0.33679', '-0.079472', '-0.04', '-0.024693', '-0.55248', '0.38834', '0.14696', '0.50003', '-0.1377', '0.20994', '-0.53022', '-0.23712', '0.24392', '0.29524', '-0.20951', '0.11347', '0.16736', '-0.057263', '0.20945', '-0.49785', '0.22321', '-0.24234', '-0.076378', '0.42953', '0.71138', '0.12599', '-0.1331', '-0.13823', '-0.22315', '-0.17269', '0.43008', '-0.34042', '-0.23127', '0.66599', '0.15312', '0.47323', '-0.28108', '0.097872', '-0.33014', '-0.068678', '0.57197', '0.099838', '-0.6237', '-0.22572', '-0.59751', '-0.30157', '-0.1239', '0.0057373', '-0.058747', '-0.030736', '-0.10812', '0.17601', '0.26234', '0.15636', '-0.19436', '-0.097775', '0.15462', '-0.083865', '-0.15106', '0.27862', '0.28175', '-0.27084', '0.029867', '0.082898', '0.020298', '0.35015', '-0.027691', '-0.010642', '0.21173', '0.090988', '0.59747', '0.12784', '-0.31951', '0.26881', '-0.41771', '-0.2073', '-0.077332', '-0.32069', '-0.020763', '-0.24735', '-0.23254', '-0.005691', '-0.088722', '-0.13886', '-0.16886', '-0.13215', '-0.17242', '-0.1223', '-0.24484', '0.1374', '0.24458', '-0.1688', '0.64932', '0.051973', '-0.30896', '0.1567', '0.351', '0.081668', '-0.60955', '-0.54647', '-0.61302', '-0.48092', '0.22664', '-0.27639', '0.12523', '-0.1358', '0.3322', '0.54997', '-0.068345', '0.07199', '-0.11543', '0.19326', '-0.085861', '-0.1098', '0.034066', '-0.072258', '-0.58648']\n",
      "[!] l = ['.', '.', '.', '.', '0.033459', '-0.085658', '0.27155', '-0.56132', '0.60419', '-0.027276', '-0.093992', '0.068236', '-0.3961', '-0.83028', '0.17456', '0.46373', '0.13719', '0.25598', '-0.33885', '0.18365', '0.44451', '-0.8193', '-0.081032', '-0.070653', '0.11253', '-0.0087314', '0.45494', '-0.13481', '-0.19651', '-0.0098954', '0.34683', '-0.010663', '-0.10555', '-0.027425', '0.46831', '-0.29624', '0.0027633', '-0.18621', '0.32282', '0.20276', '0.5976', '-0.15331', '0.52121', '0.59813', '-0.28581', '-0.23798', '-0.078883', '-0.64561', '0.03057', '0.28304', '0.15654', '-0.18034', '0.48116', '0.39754', '0.2106', '-0.039421', '-0.31166', '0.38636', '0.64125', '-0.52607', '0.064417', '-0.23567', '0.37243', '-0.089502', '-0.39855', '-0.12211', '0.37331', '-0.45538', '-0.40342', '0.65258', '0.14624', '0.3247', '-0.41644', '0.15981', '0.092788', '-0.47863', '0.64507', '-0.013909', '0.21356', '0.39679', '0.52347', '0.16871', '-0.017134', '-0.57287', '-0.47366', '0.30996', '-0.32248', '-0.11949', '-0.48315', '-0.20478', '0.45759', '-1.0443', '-0.58684', '0.58544', '-0.081284', '-0.21224', '-0.25302', '0.90371', '-0.20399', '0.65895', '-0.11742', '-0.13352', '-0.091149', '0.11375', '-0.18618', '0.098569', '-0.2067', '-0.22156', '-0.1557', '-0.7965', '-0.23144', '-0.31047', '-0.46223', '0.26712', '0.31644', '-0.066401', '-0.40895', '0.11665', '0.50156', '0.47769', '-0.075403', '-0.482', '0.033416', '0.17506', '-0.063225', '0.19088', '0.039387', '0.2396', '-0.35331', '-0.09274', '-0.33705', '0.085312', '0.080227', '-0.020173', '0.507', '-0.072361', '0.13175', '-0.45573', '0.20334', '-0.056897', '-0.11733', '0.047485', '0.016908', '-0.24814', '0.71598', '-0.055958', '0.37822', '0.3392', '0.17434', '-0.37932', '0.03775', '0.019235', '-0.29447', '-0.18964', '-0.45798', '-0.16', '-0.056206', '0.042038', '0.64842', '0.47592', '-0.67641', '-0.57816', '0.769', '-0.096332', '-0.064562', '0.042617', '-0.23167', '0.51994', '0.10079', '0.15415', '-0.14515', '-0.21889', '0.23307', '-0.043176', '-0.41485', '0.1427', '0.84153', '-0.078821', '0.060877', '0.12004', '-0.26696', '-0.53933', '-0.36731', '-0.37563', '-0.35086', '-0.24055', '0.49705', '-0.34699', '0.021831', '0.53459', '0.2906', '-0.065185', '-0.19822', '0.33217', '0.28218', '0.34624', '0.36723', '-0.66741', '0.082508', '-0.10312', '0.27671', '-0.55848', '0.67853', '0.040049', '-0.0057962', '0.86106', '-0.40337', '-0.45425', '-0.48904', '-0.11567', '0.71084', '-0.00071633', '-0.075139', '0.29584', '-0.38594', '0.17853', '0.033168', '0.19186', '-0.10717', '-0.51614', '0.19278', '-0.43339', '0.51961', '0.10363', '0.28009', '-0.41613', '0.34869', '0.052315', '-0.026509', '-0.64101', '-0.047879', '-0.42739', '-0.018592', '0.18577', '-0.16994', '-0.18489', '-0.076386', '-0.28981', '0.26335', '0.21274', '0.11926', '-0.32697', '0.22216', '-0.24976', '0.36953', '0.24742', '0.37245', '0.37218', '-0.18512', '-0.23093', '0.035941', '0.2418', '0.058993', '0.16378', '0.43893', '0.19057', '-0.15457', '0.17481', '-0.30859', '-0.1335', '0.21542', '-0.44562', '0.48477', '0.28185', '-0.063253', '0.053603', '-0.05103', '-0.29387', '0.17704', '0.64601', '-0.11012', '-0.23288', '-0.60988', '-0.33054', '-0.25336', '-0.042175', '-0.43281', '-0.061033', '-0.097728', '0.42459', '0.051169', '-0.37042', '0.053402', '0.18471', '0.37567', '-0.66422', '-0.2099', '-0.92126', '-0.098194', '0.034713', '0.19768', '0.35687', '-0.03349', '0.72838', '0.1016', '-0.42271', '-0.19997', '-0.21866', '-0.2066', '-0.59319', '-0.052248', '-0.032041', '0.068443', '-0.92476']\n",
      "[!] l = ['email', 'name@domain.com', '0.33529', '0.32949', '0.2646', '0.64219', '0.70701', '-0.074487', '-0.066128', '-0.30804', '-0.71712', '-0.65856', '0.21812', '0.20661', '0.079468', '-0.37277', '0.69841', '0.40459', '0.10838', '-0.84972', '0.084111', '-0.099199', '-0.15883', '-0.15245', '0.10664', '0.2866', '0.058125', '0.10893', '-0.042369', '-0.10781', '-0.10683', '-0.61831', '0.71169', '-0.4143', '0.46387', '0.11437', '0.11797', '0.78043', '0.15162', '0.050014', '0.27046', '-0.15689', '-0.35901', '-0.22091', '0.2174', '-0.53654', '-0.26149', '0.071581', '0.28225', '0.61944', '0.56737', '0.054449', '0.63952', '-0.25328', '0.34196', '-0.39231', '0.14372', '-0.3836', '-0.43978', '-0.13577', '-0.38214', '0.58241', '-0.49185', '0.020045', '0.025588', '0.13079', '-0.43368', '0.39428', '-0.48619', '0.53205', '0.231', '0.026263', '0.20574', '-0.5681', '0.23058', '-0.20142', '0.23862', '0.34402', '0.53577', '-0.36288', '0.24647', '-0.15684', '-0.49296', '0.063751', '-0.35141', '-0.10532', '0.0045396', '-0.047133', '0.9011', '-0.60606', '0.073045', '-0.4576', '0.052548', '-0.4998', '0.55807', '-0.21151', '-0.60018', '1.0234', '-0.80815', '0.15263', '0.47984', '0.09278', '0.35607', '-0.28802', '-0.14008', '-0.12359', '0.48392', '-0.54092', '0.26861', '0.18011', '-0.5053', '0.35732', '0.027227', '-0.63815', '0.65042', '0.60165', '0.71557', '-0.58427', '0.0075552', '0.071666', '0.31442', '0.5006', '0.14557', '0.057359', '0.23515', '0.09242', '0.40623', '0.12675', '0.40275', '0.11623', '0.13235', '0.048337', '0.40902', '-0.46324', '-0.36735', '0.10782', '-0.1503', '0.20129', '0.1842', '-0.20326', '0.025293', '-0.60814', '0.94863', '0.19966', '-1.0322', '-0.33815', '0.42628', '-0.18858', '0.70273', '-0.10762', '-0.57743', '0.14396', '0.57879', '0.55755', '-0.10448', '-0.08187', '-0.0042659', '0.41304', '0.61282', '-0.27785', '0.19742', '0.62788', '0.44321', '-0.0018459', '0.076699', '-0.37352', '0.64692', '-0.21255', '0.12151', '0.14871', '-0.35841', '-0.14698', '-0.49289', '0.36204', '0.66804', '-0.49378', '0.21669', '0.021931', '-0.33109', '-0.00088334', '-0.48386', '0.64701', '0.10726', '0.26803', '0.54351', '-0.19216', '0.82757', '0.092724', '0.20233', '-0.057761', '0.14353', '0.35351', '0.053238', '-0.34339', '0.50812', '0.027739', '-0.17723', '0.91226', '0.24673', '-0.16193', '0.48266', '0.17719', '0.37488', '-0.13047', '-0.071644', '-0.050737', '0.36963', '0.54698', '-0.027903', '0.010671', '0.059521', '0.37979', '-0.0033416', '0.43249', '0.092561', '-0.10606', '0.13377', '0.72729', '-0.0078999', '0.30048', '0.12956', '0.82322', '0.71878', '-0.6804', '0.24552', '0.49127', '-0.62899', '-0.757', '-0.20259', '-0.43667', '-0.38061', '-0.97217', '-0.059481', '-0.1246', '0.23146', '0.11842', '-0.32492', '0.41028', '-0.43662', '-0.16969', '0.29064', '0.053265', '0.012703', '-0.050542', '-0.15929', '-0.14017', '0.46322', '-0.25027', '0.26577', '0.65584', '0.32285', '-0.3679', '0.1908', '-0.30116', '-0.61803', '-0.17922', '0.21711', '-0.26571', '-0.26099', '-0.44266', '-0.1252', '-0.43041', '-0.011423', '0.30861', '0.11921', '-0.24908', '0.58099', '0.73546', '-0.06392', '0.18363', '0.2339', '0.042055', '0.040574', '0.47928', '0.25323', '0.019253', '-0.2219', '0.36961', '0.021001', '-0.0050943', '-0.18382', '-0.7743', '0.24078', '0.31495', '0.35245', '-0.4387', '0.21064', '0.41219', '0.48009', '0.074987', '-0.11304', '0.37291', '0.43416', '0.15984', '0.30369', '-0.45189', '0.040446', '-0.33307', '-0.41444', '-0.13447', '0.09813', '0.30967']\n",
      "[!] l = ['0.39511', '0.37458', '0.24418', '-0.11774', '-0.22022', '-0.14198', '0.22348', '0.66478', '-0.055946', '-0.77057', '0.40113', '-0.16337', '-0.60489', '0.062868', '-0.041524', '-0.36733', '-0.098549', '-1.1798', '0.16591', '0.4049', '0.12931', '-0.21594', '-0.47684', '-0.68567', '0.1811', '-0.15327', '-0.17995', '-0.089259', '-0.28031', '0.29367', '0.48684', '-0.82487', '0.1316', '0.56742', '0.30317', '0.7958', '0.30638', '0.02231', '0.43904', '0.40641', '0.23217', '-0.28392', '-0.43397', '-0.15063', '-0.26302', '0.076595', '-0.13759', '-0.25822', '-0.54235', '0.05306', '-0.3238', '-0.44322', '-0.64328', '0.22687', '0.27035', '-0.42319', '0.25998', '-0.2115', '0.45676', '0.27369', '-0.54565', '-0.17972', '0.36615', '-0.26454', '-0.48054', '0.61096', '0.044384', '0.32142', '-0.36113', '0.082088', '-0.31467', '-0.79868', '0.20468', '0.55312', '-0.64281', '-0.27458', '0.081022', '0.2294', '0.008602', '-0.35629', '0.30687', '0.086602', '-0.25323', '0.40285', '-0.58385', '-0.47447', '0.68849', '-0.97303', '-0.33577', '0.037198', '0.2492', '-0.12925', '0.11656', '0.711', '-0.92086', '0.75021', '-0.48885', '-0.004171', '0.34382', '-0.050034', '-0.57947', '0.74101', '0.16386', '-0.58246', '-0.26408', '-0.24698', '-0.0078739', '0.14037', '-0.56284', '0.40255', '0.52512', '-0.38051', '-0.44113', '-0.1363', '0.45462', '0.72753', '-0.14613', '-0.15335', '0.69897', '-0.17242', '-0.10613', '-0.30009', '-0.046861', '-0.074899', '-0.421', '-0.29968', '-0.10574', '0.19671', '0.023128', '0.28823', '-0.16413', '0.24866', '0.058625', '0.2727', '0.71116', '-0.53553', '0.10069', '-0.31683', '0.26602', '-0.26045', '0.08303', '-0.40904', '0.108', '-0.1678', '0.3018', '0.25051', '0.14558', '0.20508', '-0.15123', '0.77621', '0.39206', '0.15488', '-0.15854', '0.58403', '-0.048212', '-0.32123', '-0.10678', '-0.3815', '1.0762', '-0.18073', '-0.13658', '0.33129', '0.26981', '0.94222', '0.43558', '0.49304', '0.2063', '-0.57149', '0.44812', '0.28879', '-0.07066', '0.33406', '0.16555', '0.78561', '0.4553', '-0.014587', '0.10383', '-0.46075', '-0.57311', '-0.54108', '-0.74793', '-0.17816', '0.15382', '-0.14445', '0.2642', '0.56389', '0.010614', '-0.42244', '0.034985', '-0.56275', '-0.092801', '0.093526', '-0.061915', '-0.20864', '0.30288', '-0.4971', '0.29478', '-0.31164', '0.43055', '-0.38597', '0.26982', '-0.060108', '0.28312', '-0.24847', '-0.0089418', '0.27059', '0.64203', '0.19168', '-0.48764', '-0.20838', '-0.092002', '-0.21289', '0.30248', '0.70845', '0.31045', '0.14009', '0.13846', '0.26079', '0.82657', '0.32691', '0.14173', '0.15955', '0.23693', '-0.042915', '-0.23096', '0.053735', '-0.46096', '-0.68943', '-0.37255', '0.30405', '-0.50226', '-0.4071', '-0.28779', '0.23124', '0.13958', '-0.45732', '0.68911', '-0.039353', '-0.23077', '0.87118', '0.58414', '-0.26771', '0.23637', '-0.47677', '-0.050091', '0.52326', '0.35715', '0.10493', '-0.09226', '0.20069', '0.32284', '-0.02508', '-0.61033', '-0.30003', '0.10674', '0.07727', '0.12009', '0.32834', '-0.028874', '-0.13639', '0.089067', '-0.15593', '0.30215', '-0.53289', '-0.0079732', '0.43263', '-0.41286', '-0.41525', '0.26726', '-0.14823', '-0.59373', '0.24645', '-0.42314', '-0.077037', '-0.47424', '0.34342', '0.34737', '-0.13609', '-0.053119', '-0.14525', '-0.32407', '-0.37664', '-0.77906', '-0.089391', '-0.16998', '-0.21994', '-0.64258', '-0.46323', '-0.88791', '0.39118', '0.21314', '-0.34914', '0.18718', '-0.45909', '0.46445', '0.19933', '0.28071', '0.1983', '0.12907', '-0.8389']\n",
      "[!] l = ['or', 'name@domain.com', '0.48374', '0.49669', '-0.25089', '0.90389', '0.60307', '0.11141', '-0.021157', '0.10037', '-0.15173', '-1.0246', '0.27841', '-0.062042', '-0.061043', '-0.47401', '0.60886', '0.29636', '-0.077564', '-0.81931', '-0.023403', '0.30916', '0.029043', '-0.34372', '-0.20211', '0.17291', '-0.49248', '0.19652', '0.30329', '-0.023517', '0.062654', '-0.56825', '0.20126', '-0.66776', '0.064812', '-0.024986', '-0.081007', '0.38214', '-0.0035715', '0.40572', '0.090373', '-0.023844', '-0.14541', '0.049812', '0.054826', '-0.53691', '-0.14741', '-0.097398', '0.028146', '-0.011163', '0.71235', '0.36371', '0.66517', '-0.043227', '0.34479', '-0.4558', '-0.012266', '-0.4921', '-0.98134', '-0.063421', '-0.11077', '0.46109', '-0.82556', '-0.25896', '0.48532', '0.31552', '-0.57685', '0.63555', '-0.40461', '0.41238', '0.0662', '0.17397', '-0.1019', '-0.65447', '0.20334', '0.053966', '-0.15827', '0.27588', '0.43867', '-0.081736', '0.47221', '-0.45256', '-0.43325', '0.14461', '-0.14118', '0.20053', '-0.3692', '0.018312', '0.8577', '-0.61049', '-0.15427', '-0.4262', '0.34196', '-0.36692', '0.51229', '-0.14486', '-0.73069', '1.002', '-1.0086', '-0.18131', '0.10296', '0.10914', '0.34994', '-0.094706', '-0.22878', '-0.24424', '0.43829', '0.063811', '0.22713', '-0.023387', '-0.38057', '0.20119', '-0.027965', '0.027987', '0.33908', '0.63356', '0.64577', '-0.73545', '-0.064695', '-0.072888', '0.49183', '0.26422', '0.2987', '0.17597', '0.22247', '0.082598', '0.13588', '0.15017', '0.25407', '0.27976', '0.49918', '0.4207', '0.2508', '-0.31198', '-0.2434', '0.079441', '-0.32652', '0.34065', '0.090272', '0.074694', '0.030794', '-0.69689', '1.2249', '0.24339', '-1.0911', '0.17123', '0.12347', '-0.22785', '0.36976', '-0.55589', '-0.87727', '0.33553', '0.18196', '0.3075', '0.054316', '-0.14845', '-0.30466', '0.33621', '0.6362', '-0.38048', '0.15098', '0.2354', '0.81349', '-0.30932', '-0.13298', '0.11503', '0.82272', '0.35549', '0.059715', '0.23187', '-0.15168', '0.17065', '-0.15352', '0.43389', '0.64511', '-0.033239', '0.26057', '-0.081056', '-0.43354', '-0.067527', '-0.27017', '0.3825', '0.078534', '0.19403', '0.92968', '0.061512', '0.84848', '0.29075', '0.48908', '0.043084', '0.19724', '0.81964', '0.11703', '-0.029999', '0.85199', '0.076113', '-0.56209', '0.86088', '0.50691', '0.10746', '0.45046', '-0.0016683', '0.51397', '0.14412', '-0.45557', '0.093795', '-0.023058', '0.20497', '-0.23176', '0.19408', '0.096653', '0.52317', '-0.1884', '0.04104', '0.046953', '-0.11417', '0.035494', '0.5119', '0.23042', '0.19461', '0.36794', '0.63884', '0.52288', '-0.33988', '0.60725', '0.35293', '-0.73891', '-0.3898', '-0.23657', '-0.5773', '-0.24254', '-0.98802', '0.27399', '-0.26853', '0.23769', '0.21178', '-0.08188', '0.29739', '-0.50025', '-0.26005', '0.43288', '-0.22978', '0.20331', '-0.36308', '-0.022572', '0.042341', '0.43218', '-0.016921', '0.13097', '1.0983', '0.29603', '-0.29021', '0.020473', '-0.31833', '-0.58473', '-0.098', '0.049079', '-0.52673', '0.20277', '-0.38167', '-0.42331', '-0.33024', '-0.6353', '0.11201', '-0.0072223', '-0.098756', '0.084024', '0.38257', '-0.17998', '-0.10995', '-0.17451', '0.57884', '0.051173', '0.33876', '0.17066', '-0.0044339', '-0.48094', '0.45427', '0.21647', '-0.33465', '-0.30874', '-0.84948', '0.23015', '0.26173', '0.48439', '-0.37949', '0.16163', '0.42102', '-0.20904', '-0.028854', '-0.25135', '0.6825', '0.50248', '-0.045756', '0.41991', '-0.22363', '0.43526', '-0.27397', '-0.17557', '-0.21913', '-0.10409', '0.64972']\n",
      "[!] l = ['0.13211', '0.19999', '0.37907', '-1.0064', '-0.40911', '-0.51834', '-0.0023625', '0.72729', '0.32459', '-0.92157', '0.90164', '0.44957', '0.38033', '1.1382', '0.29983', '0.12735', '0.25229', '-0.70663', '-0.23786', '0.43878', '0.044275', '0.1876', '0.45449', '0.3476', '-0.51336', '-0.24544', '-0.73065', '0.38381', '-0.6095', '-0.45189', '0.37978', '0.2842', '0.80136', '0.35596', '0.15797', '0.38702', '0.46921', '-0.20393', '0.25601', '0.54377', '0.46229', '-0.69493', '0.07983', '-0.50592', '-0.3271', '-0.42406', '-0.83159', '-0.050007', '-0.83818', '0.49213', '0.27459', '-0.50084', '0.51376', '-0.58616', '0.18178', '-1.0722', '-0.40106', '-0.15986', '0.26965', '-0.13816', '-0.36793', '-0.18336', '1.1066', '-0.32457', '-0.55654', '-0.47314', '0.61528', '0.068438', '0.084194', '0.033582', '-0.29639', '0.15035', '-0.050321', '0.14397', '-0.27038', '-0.2947', '-1.001', '0.70635', '-0.15076', '-0.043514', '-0.38566', '-0.31853', '-0.50682', '-0.28274', '-0.1367', '-0.36531', '0.55686', '-0.93258', '-0.7801', '-0.43035', '-0.12523', '0.25327', '-0.37912', '-0.50763', '0.85128', '0.80422', '0.092283', '-0.48235', '0.32777', '0.24035', '0.22018', '0.1182', '0.70058', '-0.00084526', '-0.41511', '-0.62978', '-0.097118', '-0.71455', '-0.39009', '0.1846', '-0.13906', '0.035382', '0.53272', '0.10279', '0.77777', '0.37078', '-0.49637', '0.52343', '-0.04347', '0.26746', '-0.77774', '-0.37641', '-0.33329', '0.34514', '-0.29231', '-0.089529', '-0.14439', '-0.058624', '0.46157', '0.51257', '0.022505', '-0.70146', '0.45609', '-0.0826', '-0.85215', '0.77573', '0.33152', '-0.33469', '-0.049881', '0.059259', '0.51844', '-0.1267', '-0.47618', '0.45214', '-0.46762', '0.44944', '0.46255', '-0.029265', '-0.38288', '-0.44515', '-0.25096', '0.79598', '-0.31184', '0.56242', '0.25586', '0.5386', '-0.43364', '-0.32716', '0.011484', '-0.20216', '0.33827', '0.28704', '-0.14276', '0.84932', '0.23969', '0.84647', '0.30411', '0.17099', '-0.072405', '1.2317', '-0.31855', '0.47201', '1.0762', '0.38598', '0.26505', '-0.64974', '0.17403', '-0.3518', '-0.52021', '0.10343', '0.83718', '-0.46345', '0.40451', '0.074564', '0.59008', '0.44032', '-0.28199', '-0.19057', '0.068128', '0.023678', '-0.11378', '0.20098', '0.75415', '-0.74819', '0.20964', '0.0030014', '0.30832', '0.54829', '-0.1342', '0.5293', '0.13906', '0.87743', '0.24384', '-0.19371', '-0.14142', '0.18452', '0.54626', '0.17129', '-0.25267', '0.50648', '-0.12714', '0.59112', '-0.0013936', '0.24264', '-0.48477', '-0.20515', '-0.36366', '-0.53311', '0.66627', '0.42985', '0.42223', '-0.76472', '0.6559', '0.29811', '-0.72223', '-0.52763', '0.071882', '-0.39642', '-0.99183', '-0.52499', '0.21683', '0.8103', '-0.61911', '-0.31345', '0.22773', '0.46359', '-0.29799', '-0.80257', '-0.37183', '-0.17519', '-0.035301', '-0.21734', '0.18175', '0.32128', '-0.21378', '0.12324', '0.18513', '0.32688', '0.28035', '-0.57293', '-0.11153', '-0.31491', '0.024416', '0.1472', '-0.32103', '-0.25759', '1.0529', '-0.31997', '-0.14839', '0.35013', '-0.27626', '-0.35195', '0.82421', '-0.28224', '-0.17659', '0.28742', '0.29171', '-0.27983', '-0.030487', '-0.15465', '-0.21473', '-0.1967', '-0.71649', '0.15181', '0.28155', '-0.10214', '0.23059', '0.11032', '-0.25677', '-0.21932', '-0.93667', '0.3276', '-0.3111', '-0.27596', '0.11256', '0.10858', '-0.067365', '0.25548', '-0.040267', '0.54465', '0.25009', '-0.1655', '-0.21356', '-0.062042', '0.084086', '0.17978', '0.13674', '0.22974', '0.56769', '-1.2427']\n",
      "[!] l = ['contact', 'name@domain.com', '0.016426', '0.13728', '0.18781', '0.75784', '0.44012', '0.096794', '0.060987', '0.31293', '-0.15884', '-1.2367', '0.43769', '0.10465', '0.048858', '-0.23182', '0.71125', '0.022376', '0.63524', '-1.4974', '0.12243', '-0.07386', '-0.021514', '-0.37652', '0.17503', '-0.011225', '-0.12668', '-0.0090601', '0.38418', '0.11132', '0.15851', '-0.47498', '0.33619', '-0.48833', '0.23423', '0.13258', '0.29362', '0.13526', '-0.05115', '-0.0055236', '0.27734', '-0.23565', '0.19571', '-0.29095', '0.062419', '-0.47502', '-0.71402', '-0.36384', '0.53562', '0.40136', '0.30963', '0.16238', '-0.11662', '-0.16201', '0.30672', '0.21663', '0.086839', '-0.38895', '-0.19644', '-0.52311', '-0.33153', '0.27012', '-0.89654', '-0.15193', '0.12447', '-0.19112', '-0.494', '-0.011873', '-0.41412', '0.52585', '0.27316', '-0.047525', '-0.1178', '-0.3371', '0.61151', '-0.012169', '0.36935', '0.32679', '-0.098269', '0.038729', '0.003551', '-0.51871', '-0.48189', '-0.079238', '-0.34291', '-0.44045', '-0.24479', '0.05593', '0.83227', '-0.55939', '-0.29242', '-0.19718', '0.17693', '-0.12205', '0.55837', '-0.28505', '-0.64676', '0.57716', '-1.4398', '0.066288', '-0.086048', '0.381', '-0.25805', '-0.11941', '-0.25664', '-0.057845', '1.0033', '-1.0863', '0.14343', '0.17181', '-0.81313', '0.19286', '0.12922', '-0.20835', '0.98495', '0.58797', '0.33635', '-0.68359', '0.5062', '0.10678', '0.57212', '0.59786', '0.053268', '0.028426', '0.42805', '0.23711', '0.66162', '0.2584', '0.32478', '-0.041477', '0.35121', '0.19489', '0.0080291', '-0.36795', '-0.38805', '0.17224', '-0.36105', '0.36373', '0.011708', '-0.056135', '0.26371', '-0.59047', '1.1774', '0.36488', '-0.66474', '-0.5875', '0.38414', '-0.055643', '0.39133', '-0.34066', '-0.47462', '0.58672', '0.46918', '0.6621', '0.085039', '-0.26208', '-0.27596', '0.51976', '0.78402', '-0.21788', '0.41615', '0.057734', '0.48454', '0.032466', '-0.57003', '-0.24011', '0.64941', '0.1786', '0.16456', '0.22892', '-0.34876', '-0.21609', '-0.40112', '0.69337', '0.85653', '-0.14865', '0.55925', '0.28205', '-0.48622', '0.041444', '-0.52311', '0.89091', '-0.14692', '0.59723', '0.45691', '-0.34411', '0.71453', '-0.028327', '0.066396', '-0.28523', '-0.066252', '0.52379', '0.26523', '-0.07658', '0.41631', '-0.56412', '-0.11156', '0.55618', '0.58285', '0.19863', '0.28857', '0.26299', '0.9523', '0.15152', '-0.021361', '0.024232', '0.29266', '0.47276', '-0.061013', '-0.34556', '-0.065448', '0.51832', '-0.01764', '-0.065723', '0.035893', '-0.57871', '-0.36286', '0.53646', '-0.30941', '0.28428', '0.035597', '0.74796', '0.71145', '-0.51704', '-0.059401', '0.29316', '-0.33866', '-0.47136', '-0.26915', '-0.48318', '-0.73448', '-0.82325', '0.18229', '-0.19008', '-0.17385', '0.34227', '-0.016458', '0.15097', '-0.77959', '-0.72615', '0.38242', '-0.25839', '0.067803', '-0.22942', '-0.70517', '-0.31424', '0.57725', '-0.18576', '0.3782', '0.72404', '0.56874', '-0.16028', '0.13383', '-0.54238', '-0.47211', '0.30398', '-0.20181', '-0.35449', '0.40175', '-0.58787', '0.2751', '-0.0001217', '0.1691', '0.33633', '-0.1821', '-0.33645', '0.3404', '0.60535', '-0.17278', '0.31449', '0.11914', '-0.22371', '0.043944', '0.45005', '0.89327', '-0.18597', '-0.77009', '0.79949', '0.378', '0.026014', '-0.39198', '-0.80185', '0.24173', '0.22765', '0.75733', '-0.37899', '0.30622', '0.56785', '0.43714', '0.18369', '-0.027663', '0.099504', '0.33348', '0.088492', '0.080002', '-0.75372', '0.26302', '-0.14392', '-0.37882', '0.31386', '-0.084045', '0.70962']\n",
      "[!] l = ['-0.38024', '0.61431', '0.81146', '-0.76394', '-0.19657', '0.11078', '-0.48388', '0.20633', '0.29338', '-1.1915', '0.031847', '-0.4115', '0.36805', '0.49588', '0.13105', '-0.35901', '0.66379', '-1.1519', '0.067329', '0.044183', '0.47607', '-0.01831', '-0.094304', '0.72384', '-1.0576', '-0.32641', '-0.60887', '-0.23653', '0.53253', '0.0071736', '0.14743', '-0.39498', '0.22907', '0.40914', '-0.29576', '0.37041', '0.41462', '0.32433', '0.013848', '0.1493', '0.10169', '0.26288', '-0.089235', '-0.10305', '-0.076485', '0.14427', '0.13496', '-0.20195', '-0.45063', '0.58541', '0.43103', '-0.57936', '0.22184', '-0.19762', '-0.032769', '-1.1831', '-0.30654', '-0.46583', '-0.0965', '-0.37734', '-0.27668', '0.13681', '1.1758', '0.090845', '-1.2651', '0.27605', '0.3203', '0.27979', '0.002744', '-0.12', '0.17253', '-0.44621', '-0.044658', '-0.35172', '0.12478', '-0.24757', '-0.51956', '0.49587', '-0.21825', '-0.33698', '-0.40646', '-0.45802', '0.033569', '0.29989', '0.17406', '0.12293', '0.99174', '-0.84194', '0.26894', '-0.10951', '-0.14404', '-0.50987', '0.0015717', '-0.29964', '0.18028', '0.58709', '0.617', '-0.10019', '-0.03308', '-0.080183', '0.49027', '0.13914', '0.78463', '0.40423', '0.0052089', '-0.42332', '0.53297', '-0.62549', '-0.49874', '-0.40964', '0.17438', '-0.11739', '0.45287', '0.073891', '0.76254', '0.47945', '-0.73757', '0.38872', '0.03891', '0.049184', '-0.8729', '-0.60075', '-0.30035', '0.37198', '-0.69635', '0.030414', '0.39002', '-0.060856', '0.020439', '0.073746', '0.46589', '-0.81778', '0.030988', '0.25552', '-0.92167', '0.45939', '-0.22004', '-0.63398', '-0.06483', '-0.15865', '0.25266', '-0.27388', '-0.78348', '0.7644', '-0.76763', '0.17775', '-0.41178', '0.33204', '-0.52741', '0.23785', '-0.20551', '0.33535', '-0.33909', '1.1142', '0.2585', '0.82173', '-0.1593', '-0.7597', '-0.086216', '-0.40343', '0.20762', '0.31962', '0.37359', '0.34092', '0.50245', '0.10872', '0.39478', '0.44402', '-0.53152', '0.83209', '0.09888', '0.098639', '1.1425', '0.066606', '0.44238', '-0.50067', '-0.65219', '-0.24283', '-0.23538', '-0.21492', '0.891', '-0.0011944', '0.50252', '0.022683', '0.13542', '0.41166', '0.53802', '-0.21229', '0.25587', '-0.53597', '0.68588', '0.17193', '0.49448', '-0.72113', '0.36913', '-0.040694', '0.85263', '0.36094', '-0.33728', '0.13549', '0.83881', '0.55877', '0.1394', '0.7023', '0.21372', '0.24292', '0.89523', '0.14688', '-0.54288', '0.081865', '0.29708', '0.32373', '-0.19121', '0.46238', '-0.066726', '-0.27294', '-0.3098', '0.1321', '0.84423', '0.061012', '0.02419', '-0.84027', '0.39133', '0.23882', '-0.94984', '-0.53428', '-0.097859', '-0.1175', '-1.2519', '-0.26203', '-0.0053243', '0.53873', '-0.50904', '-0.056877', '-0.032332', '0.64583', '-0.028669', '-0.25967', '-0.48922', '0.68671', '0.40161', '-0.19764', '0.74176', '0.020575', '-0.4533', '0.20684', '0.25419', '0.21105', '-0.006952', '-0.20994', '-0.43429', '0.020079', '0.43515', '0.63793', '-0.15446', '-0.42096', '0.19379', '-0.16843', '0.25462', '-0.056472', '-0.55995', '-0.76004', '0.10799', '0.0022283', '-0.83875', '0.24067', '0.17338', '-0.46126', '-0.25405', '0.11485', '-0.55184', '-0.11525', '-0.69579', '0.10919', '0.31153', '-0.0074518', '0.26508', '-0.17048', '-0.28488', '-0.23757', '-0.74062', '0.3429', '-0.20848', '0.078322', '0.038162', '-0.0091354', '-0.56469', '0.37543', '-0.43433', '-0.2131', '0.50579', '0.16218', '-0.22897', '0.11223', '0.10043', '-0.22417', '0.18641', '-0.51603', '0.79525', '-0.5028']\n",
      "[!] l = ['-0.0033421', '0.4899', '1.119', '-1.1039', '-0.43012', '-0.10575', '-0.41147', '0.41198', '0.4217', '-1.1474', '0.75974', '-0.32708', '0.19114', '0.86659', '0.0277', '-0.16528', '0.15653', '-0.99331', '0.42927', '-0.272', '0.31049', '0.34058', '-0.12565', '0.82157', '-1.02', '-0.24144', '-0.42876', '0.33708', '0.46665', '-0.21487', '-0.080799', '0.096942', '0.27106', '0.51323', '0.25295', '-0.095867', '0.57536', '0.72603', '0.038852', '0.41351', '0.90414', '-0.05979', '0.01906', '-0.020534', '-0.22322', '-0.57924', '0.2106', '-0.20256', '-0.36304', '0.51056', '0.4685', '0.067866', '0.77369', '0.08512', '0.26736', '-1.1224', '-0.47353', '-0.075091', '0.090267', '0.42668', '-0.27217', '0.00082357', '1.1958', '-0.50279', '-0.94605', '0.35149', '0.94766', '0.38067', '0.049846', '0.29573', '0.20623', '-0.30876', '-0.11723', '-0.43945', '0.28167', '-0.15395', '-0.62523', '0.45378', '-0.29078', '-0.18305', '-0.02323', '-0.57743', '0.33795', '-0.04868', '0.10634', '-0.19458', '1.0771', '-0.59244', '0.17142', '-0.26726', '-0.067978', '0.1064', '-0.036547', '-0.58865', '0.69669', '0.75856', '0.41468', '-0.084012', '0.16371', '0.13527', '0.0034677', '0.18337', '0.78953', '0.61498', '0.074059', '-0.71297', '0.33224', '-0.61879', '-0.3748', '-0.20558', '-0.2898', '0.29551', '-0.050135', '-0.041241', '0.7411', '0.22055', '-0.22782', '0.52417', '0.028373', '0.22305', '-0.74568', '-0.66688', '-0.03676', '0.50137', '-0.60308', '0.20253', '0.57295', '0.065622', '0.055917', '-0.17912', '0.038046', '-0.64214', '0.01545', '0.17925', '-0.52084', '0.388', '0.3032', '-0.54131', '0.046137', '-0.71535', '0.62135', '-0.32544', '-0.90522', '0.60211', '-0.92253', '0.21765', '-0.37901', '0.28622', '-0.64706', '0.098388', '0.029217', '0.64722', '-0.39558', '0.98142', '0.61932', '0.77084', '-0.16722', '-0.48187', '0.061517', '-0.49846', '0.47514', '0.10498', '0.087725', '0.40003', '0.31031', '0.61882', '0.11719', '0.17369', '-0.25088', '0.73207', '0.20869', '0.68062', '0.75868', '0.47936', '0.47285', '-0.10094', '-0.33325', '0.11272', '-0.57483', '-0.033875', '0.8601', '-0.60479', '0.25969', '0.44063', '0.37427', '0.18225', '0.39239', '-0.15905', '0.53333', '-0.49573', '0.047233', '0.24629', '0.72525', '-0.65348', '0.46025', '-0.42107', '0.70839', '0.42328', '-0.081451', '-0.020437', '0.67316', '0.56422', '0.52575', '0.76497', '-0.33639', '0.33732', '0.58747', '0.36276', '-0.27091', '0.66182', '-0.10401', '0.58685', '-0.0069205', '0.40072', '-0.5339', '0.12853', '-0.36908', '0.24769', '0.44548', '0.20518', '0.2624', '-0.72808', '0.23854', '0.44334', '-0.79402', '-0.68234', '0.31038', '-0.41873', '-1.3965', '-0.6576', '0.53212', '0.36431', '-0.44823', '-0.4594', '-0.32047', '0.81983', '0.023507', '-0.45371', '-0.56643', '0.73356', '0.27924', '-0.13309', '0.25513', '0.16403', '-0.5493', '0.032856', '0.72186', '0.055202', '-0.31541', '0.086215', '-0.39683', '0.017186', '0.047105', '0.19885', '0.08015', '-0.066396', '0.45437', '-0.14573', '0.47351', '0.14152', '-0.48003', '-0.5452', '0.5743', '-0.20541', '-0.17628', '0.18899', '-0.21987', '-0.12792', '-0.19717', '0.34962', '-0.22362', '-0.12494', '-1.0295', '0.73254', '0.28512', '-0.24672', '-0.00076546', '0.13942', '-0.67728', '0.2788', '-1.0902', '0.59274', '-0.44622', '-0.11861', '0.37277', '-0.41357', '-0.7176', '0.6104', '0.069611', '0.20687', '0.23299', '-0.38887', '-0.51778', '-0.29611', '-0.18504', '-0.19019', '0.028504', '0.17982', '0.34429', '-0.67529']\n",
      "[!] l = ['Email', 'name@domain.com', '0.37344', '0.024573', '-0.12583', '0.36009', '0.25605', '0.07326', '0.3292', '-0.0037022', '-0.10145', '-1.8061', '0.17271', '0.40612', '0.14999', '0.077813', '0.87133', '0.37621', '0.2232', '-1.5266', '0.38978', '0.072969', '0.4398', '0.25206', '0.22285', '0.35195', '-0.15422', '-0.13137', '-0.033746', '-0.16894', '0.14939', '-0.85071', '0.8303', '-0.22077', '0.32756', '-0.46347', '0.15704', '0.15775', '-0.075318', '-0.024818', '-0.045062', '0.076934', '0.048395', '-0.72626', '0.46597', '-0.26966', '-0.58763', '0.23509', '0.41876', '0.38672', '0.38338', '0.32791', '0.1036', '0.31148', '0.36017', '-0.17754', '-0.12346', '-0.28095', '-0.12792', '-0.4066', '0.14674', '0.66217', '-0.48934', '0.33352', '-0.08094', '-0.081471', '-0.56518', '0.60675', '-0.071591', '0.55095', '0.37616', '-0.11914', '-0.22842', '-0.72311', '-0.30394', '0.06935', '0.11861', '0.18844', '0.40996', '-0.30491', '-0.073846', '-0.24488', '-0.85284', '-0.0016392', '0.058739', '-0.11254', '0.21054', '-0.12957', '0.60001', '-0.57975', '0.098064', '0.25146', '0.38879', '-0.59721', '0.74942', '0.13982', '-0.64333', '0.8068', '-0.8529', '-0.097729', '0.27562', '0.024347', '0.32667', '0.03216', '0.28938', '0.042973', '0.66781', '-0.62474', '0.050046', '0.11425', '-0.1425', '0.096074', '0.45494', '-0.089113', '0.52999', '0.30064', '0.51882', '0.062207', '0.4262', '0.30348', '0.58217', '0.4461', '-0.083093', '0.23491', '0.32839', '-0.20729', '-0.18414', '0.25996', '0.75131', '0.1707', '-0.22728', '-0.02473', '0.44001', '-0.67008', '-0.1713', '0.045325', '-0.19009', '-0.053806', '0.49594', '-0.15684', '-0.25893', '-0.29879', '0.68854', '0.052915', '-0.55306', '-0.12448', '0.16334', '0.39501', '0.45141', '-0.051286', '-0.23803', '0.31448', '0.098186', '0.5036', '0.22789', '-0.22292', '-0.30905', '-0.27327', '0.4907', '-0.31093', '0.034955', '0.53066', '0.8932', '0.19769', '0.02168', '-0.044989', '0.80471', '0.13248', '-0.004733', '0.53816', '0.0021613', '-0.34276', '-0.17542', '0.6035', '0.51525', '-0.2402', '0.31781', '0.028639', '-0.4713', '-0.029991', '-0.58394', '0.31659', '0.075969', '0.37688', '0.47865', '-0.62708', '0.47169', '0.74507', '0.15765', '-0.17955', '0.095543', '0.43644', '0.056638', '-0.46781', '0.3761', '0.21661', '-0.18378', '0.34377', '0.40629', '0.47489', '1.0477', '0.11735', '0.22259', '0.38282', '0.098808', '-0.088356', '0.089153', '0.099398', '-0.090611', '-0.54011', '0.15283', '0.06929', '-0.27999', '-0.076277', '0.18012', '-0.53787', '-0.34725', '0.71109', '-0.28219', '0.20893', '0.099495', '0.59917', '0.48383', '-0.13997', '0.5227', '-0.19693', '-0.24871', '-0.63357', '-0.09565', '-0.69101', '-0.25347', '-0.85556', '-0.15462', '0.0071559', '0.038633', '0.67575', '-0.20821', '1.0249', '-0.50038', '0.012226', '0.57338', '-0.32834', '0.78573', '-0.087241', '-0.7386', '-0.24992', '0.43287', '0.24101', '0.73668', '0.64582', '0.26793', '-0.2846', '-0.34104', '-0.0062504', '-0.64824', '0.58085', '-0.20472', '-0.41832', '-0.023617', '-0.44203', '-0.017908', '0.14106', '-0.33705', '-0.65206', '0.45318', '-0.53319', '0.57341', '0.5229', '0.024291', '-0.59827', '-0.026417', '0.10453', '0.12568', '0.80541', '0.77114', '0.069694', '-0.62583', '0.3634', '0.11365', '0.076932', '0.21719', '-0.50624', '0.1619', '0.66214', '0.12204', '-0.20929', '-0.030636', '0.64906', '0.1983', '0.12721', '-0.67107', '0.6206', '0.71175', '-0.45709', '-0.35856', '-0.51486', '0.20406', '0.16845', '-0.24387', '0.024908', '0.094458', '0.18698']\n",
      "[!] l = ['on', 'name@domain.com', '0.037295', '-0.15381', '-0.045189', '1.0566', '0.42898', '0.24093', '0.34305', '-0.090393', '-0.79877', '-1.2107', '0.31958', '0.46744', '-0.20072', '-0.61936', '0.69963', '0.70189', '0.58516', '-1.6244', '0.45742', '-0.10967', '-0.068128', '0.11185', '0.1758', '0.36282', '0.207', '0.085671', '-0.024428', '0.064891', '0.27972', '-0.62379', '0.11502', '-0.48324', '0.32067', '0.064549', '0.13098', '0.42373', '-0.2884', '0.062554', '0.28625', '-0.40777', '-0.15946', '-0.3914', '-0.18405', '-0.69952', '-0.41679', '-0.27461', '0.5992', '0.031153', '0.57437', '0.19658', '0.3127', '-0.078999', '0.03098', '-0.53815', '0.091315', '-0.38143', '-0.75908', '-0.12581', '-0.20777', '0.24508', '-0.51285', '0.020266', '0.16943', '-0.21203', '-0.14944', '0.46839', '-0.40152', '0.24182', '0.059784', '-0.10328', '-0.23935', '-0.15119', '-0.052004', '-0.078992', '-0.35178', '-0.21634', '0.59896', '-0.47535', '-0.11839', '-0.21983', '-0.61098', '-0.31832', '-0.55874', '0.052747', '0.039198', '-0.56885', '1.0475', '-0.22668', '-0.53216', '-0.12656', '0.29789', '-0.12112', '0.59625', '0.076639', '-1.0092', '0.66201', '-0.51496', '-0.076104', '0.082936', '-0.0094538', '0.63184', '0.20894', '0.15745', '0.074381', '0.31779', '-0.68213', '-0.18466', '-0.074668', '-0.48459', '0.06025', '0.43421', '-0.26723', '0.7676', '0.5288', '0.27123', '-0.82053', '0.26875', '0.25783', '0.4725', '0.94076', '-0.27055', '0.11796', '0.16388', '0.16271', '0.71576', '-0.84291', '0.27383', '0.12842', '-0.13808', '-0.57815', '-0.16535', '-0.45732', '-0.59394', '-0.063703', '0.1311', '0.33161', '0.27681', '-0.049656', '0.035738', '-0.29478', '1.111', '0.086364', '-0.63429', '-0.28759', '0.16863', '0.088104', '0.33693', '-0.24049', '-0.40589', '0.63117', '0.2691', '-0.0059601', '0.077498', '0.0048893', '-0.42893', '0.19667', '0.18104', '-0.033527', '0.41909', '0.56974', '0.22464', '0.44984', '-0.3836', '-0.056989', '0.70036', '-0.062551', '-0.076638', '0.41616', '-0.43017', '0.051679', '-0.45284', '0.59514', '0.74749', '-0.49283', '0.30089', '0.020652', '-0.28359', '-0.15402', '-0.60682', '-0.044729', '0.27258', '0.17982', '0.69056', '0.26935', '0.88267', '-0.25968', '0.018912', '-0.06569', '0.2459', '0.57953', '0.38363', '-0.38261', '0.89573', '-0.030606', '-0.19293', '1.0351', '0.47315', '-0.029376', '0.29365', '0.53393', '0.23375', '0.46295', '-0.3838', '0.33758', '0.40473', '0.5511', '0.0051471', '0.098574', '-0.0016179', '0.055963', '-0.26139', '0.075229', '-0.16292', '0.24037', '-0.07256', '0.22245', '-0.0040634', '0.12477', '0.16262', '0.61958', '0.10218', '-0.30888', '0.41935', '0.23097', '-0.78383', '-0.42864', '-0.43192', '-0.52814', '0.13438', '-0.83851', '-0.41554', '-0.24818', '-0.20883', '0.75631', '-0.25623', '0.32551', '-0.6399', '0.26467', '0.18173', '-0.32961', '0.028943', '0.44702', '-0.31151', '0.34936', '0.61233', '-0.34992', '0.68184', '0.64509', '-0.098248', '-0.45894', '0.20604', '-0.334', '-0.40975', '0.17371', '-0.15081', '-0.93562', '0.091008', '-0.41959', '0.026537', '-0.37542', '-0.39248', '-0.01727', '0.1491', '-0.42839', '1.2822', '0.86974', '0.4999', '-0.09103', '-0.44413', '0.15755', '0.34085', '0.51527', '0.41727', '0.2675', '-0.37372', '0.35689', '0.241', '-0.65587', '-0.11725', '-0.6031', '0.1791', '0.32321', '0.27526', '-0.18872', '0.12421', '0.077509', '0.6748', '-0.053329', '0.071209', '0.47614', '0.35372', '-0.26264', '0.067594', '-0.063179', '0.2563', '0.059929', '0.30376', '0.16772', '-0.067342', '0.30749']\n",
      "[!] l = ['0.14608', '0.31513', '0.50599', '-0.69518', '0.074028', '0.31401', '-0.20477', '0.19799', '0.22865', '-1.3954', '0.70655', '-0.06139', '0.72414', '0.96076', '0.017984', '-0.12881', '0.27998', '-1.0307', '0.55916', '0.4062', '0.27879', '-0.024454', '0.02543', '0.1957', '-0.9298', '-0.61389', '-0.83103', '-0.25625', '0.23622', '-0.57222', '-0.57596', '0.032783', '0.10992', '0.325', '0.077335', '-0.34205', '0.16476', '0.40327', '-0.011577', '0.9868', '0.95818', '-0.11623', '0.18998', '-0.28907', '-0.46201', '-0.21299', '-0.84849', '-0.33279', '-0.65109', '0.73348', '0.45931', '-0.41959', '0.51033', '-0.39727', '0.078889', '-0.91549', '-0.28378', '-0.49955', '-0.21038', '0.1287', '-0.62786', '-0.14829', '0.90545', '-0.2414', '-1.1763', '0.49755', '0.89153', '-0.061712', '0.34048', '-0.096586', '0.063058', '-0.50248', '0.27061', '-0.33679', '0.082883', '-0.020241', '-0.39487', '0.44583', '-0.16025', '-0.61305', '-0.019254', '-0.24391', '0.0019327', '0.25278', '-0.29323', '0.39912', '0.67106', '-0.55149', '-0.26651', '-0.42533', '-0.21396', '-0.42004', '-0.52614', '-0.6558', '0.30047', '0.89769', '0.53117', '-0.40601', '0.58967', '0.37333', '-0.063091', '-0.1584', '0.50199', '0.31437', '-0.43803', '-0.7203', '0.042496', '-0.12057', '-0.48574', '0.086115', '0.11777', '0.33071', '0.14979', '0.25438', '0.35045', '0.18188', '-0.77576', '0.37023', '-0.39806', '0.12319', '-0.51776', '-0.32303', '-0.32091', '0.19713', '-0.6725', '-0.067435', '0.070692', '0.13111', '-0.505', '0.11611', '0.15056', '-0.53498', '0.21043', '0.43067', '-0.66039', '0.97804', '-0.050203', '-0.4236', '0.38073', '-0.43606', '0.73208', '-0.26034', '-0.9972', '0.4288', '-0.80624', '0.16057', '-0.06048', '0.16808', '-0.94981', '0.060725', '0.16102', '0.47579', '-0.37428', '0.81535', '0.5251', '0.87488', '-0.61416', '0.12488', '0.13245', '-0.24288', '0.15699', '0.26259', '0.30163', '0.34802', '0.20402', '-0.3572', '-0.081697', '0.36203', '-0.087141', '0.46466', '0.018039', '0.34221', '0.96039', '0.55764', '-0.043104', '-0.46632', '-0.59186', '-0.17909', '-1.0391', '0.20849', '0.81734', '-0.43642', '0.45815', '0.11918', '0.59875', '0.84783', '0.74982', '0.38388', '-0.35014', '-0.2763', '0.29431', '0.50431', '0.47363', '-0.7376', '0.31598', '0.1696', '0.39782', '0.2522', '-0.16825', '0.048404', '0.097691', '0.62802', '0.48682', '0.18444', '-0.22826', '0.38361', '0.64838', '-0.037527', '0.15086', '0.28871', '0.17809', '0.64836', '-0.10215', '0.55592', '0.020203', '-0.12796', '0.0079407', '0.58531', '0.943', '-0.09715', '0.75007', '-0.30968', '0.29013', '-0.60052', '-0.46536', '-0.10924', '0.056554', '-0.19418', '-1.4786', '-0.89112', '0.56536', '0.74686', '-0.55416', '0.05116', '-0.20433', '1.0917', '0.069341', '-0.19233', '-0.59615', '0.89142', '0.58347', '-0.24956', '0.44355', '-0.40964', '-0.24574', '0.31139', '0.45903', '0.5556', '-0.31532', '0.31386', '-0.29276', '0.32388', '0.65163', '0.02787', '-0.55754', '-0.22982', '0.93623', '0.53509', '-0.14722', '-0.082144', '0.067207', '-0.62224', '0.28815', '0.18393', '-0.69426', '0.15564', '-0.25739', '-0.65644', '-0.054358', '0.21328', '-0.54851', '0.19117', '-0.54022', '0.16877', '-0.62094', '-0.13798', '-0.22448', '-0.014951', '-0.86948', '-0.17697', '-1.1901', '0.2733', '-0.48534', '0.19589', '0.28524', '0.12873', '-0.23325', '0.33817', '-0.36903', '0.24945', '0.38681', '-0.33507', '-0.048823', '-0.43982', '0.28896', '-0.17989', '0.56749', '-0.34025', '-0.067917', '-0.4408']\n",
      "[!] l = ['-0.36288', '-0.075749', '0.35433', '-1.6601', '-0.55839', '-0.40633', '-0.6132', '0.81386', '0.21159', '-1.3559', '0.75789', '0.28286', '0.30005', '1.3721', '-0.28048', '-0.41695', '0.44829', '-0.94929', '0.4668', '0.6213', '0.42376', '0.29959', '0.1673', '0.34641', '-0.44189', '-0.2786', '-0.0041916', '0.21559', '-0.19199', '-0.4685', '0.36494', '-0.44012', '0.22775', '0.27703', '-0.40449', '0.49769', '-0.18591', '0.086103', '0.31697', '-0.21846', '0.22434', '-0.13028', '-0.17373', '0.070797', '0.048253', '-0.73376', '-0.5351', '-0.57453', '-0.42547', '0.11231', '0.39573', '0.16654', '0.61061', '-0.28846', '0.30833', '-1.0041', '-0.34371', '-0.16771', '-0.39661', '-0.10575', '-0.66953', '0.19924', '0.7076', '-0.80152', '-1.0281', '0.23338', '0.55362', '0.74601', '-0.11912', '-0.20651', '0.77696', '-0.47107', '0.20611', '-0.60277', '0.17355', '-0.090661', '-1.2547', '0.32475', '-0.21933', '-0.29006', '0.40624', '0.033296', '0.9458', '-0.23071', '-0.20162', '0.17527', '0.44177', '-0.37446', '0.030624', '-0.074708', '0.33978', '0.31737', '0.13944', '-0.29465', '0.34802', '0.4', '0.23525', '0.026686', '0.4695', '-0.30263', '-0.15727', '0.31735', '0.53136', '0.90208', '-1.0483', '-0.62299', '0.41699', '-0.48235', '-0.032685', '0.17621', '0.28609', '-0.13563', '0.21965', '0.46168', '0.052413', '0.93666', '-0.15523', '0.31027', '-0.28653', '0.21021', '-0.99398', '-0.24016', '0.19947', '0.22327', '-0.58639', '-0.060659', '0.14744', '-0.10668', '-0.34351', '0.64211', '-0.30517', '-0.40452', '-0.24029', '0.26375', '-0.5943', '1.5113', '0.45027', '-0.21335', '-0.62576', '-0.66909', '0.7613', '-0.1666', '-0.85068', '0.63433', '-0.69053', '0.13544', '0.3615', '-0.40823', '-1.2319', '0.18069', '0.68336', '1.0462', '-0.54514', '0.6034', '0.37601', '0.79703', '-0.48777', '0.32344', '0.4133', '-0.67219', '0.018535', '-0.088362', '0.55708', '0.28247', '0.34933', '0.52927', '-0.023126', '0.37935', '-0.0066405', '1.128', '-0.16175', '0.66708', '0.92183', '0.069945', '-0.084846', '0.13033', '-0.5409', '0.14396', '-0.70834', '0.41947', '0.48077', '-0.63632', '0.22444', '0.55343', '0.49651', '0.69049', '0.51414', '-0.40924', '-0.35479', '-0.65747', '-0.090105', '0.33537', '-0.044043', '-1.0271', '0.46986', '-0.49679', '0.74352', '0.38098', '-0.34359', '-0.15263', '0.81561', '0.95234', '0.22588', '-0.032547', '-0.40272', '0.24138', '1.1152', '0.082477', '-0.3166', '0.38439', '-0.029631', '0.5532', '-0.10046', '0.98196', '-0.2563', '-0.31218', '-0.070906', '0.30719', '0.5705', '-0.40962', '0.50679', '-0.82092', '0.73728', '-1.0318', '-0.68167', '0.10705', '-0.062218', '-0.60404', '-0.84784', '-1.0608', '-0.19488', '0.092624', '-0.061167', '0.021886', '-0.32356', '1.2509', '0.054767', '-0.21726', '-0.35196', '0.86249', '0.09716', '0.095925', '0.3206', '-0.58682', '-0.66871', '0.14927', '0.62588', '1.3604', '-0.52541', '0.098919', '-0.6949', '-0.04327', '0.18446', '-0.18489', '-0.2383', '-0.7651', '0.65188', '0.30342', '-0.20888', '0.5503', '-0.17208', '-0.94628', '0.50616', '0.014689', '-1.0449', '0.21472', '-0.60579', '-0.2994', '-0.5931', '0.18085', '-0.20646', '-0.37992', '-0.64169', '-0.35102', '0.36473', '-0.008052', '0.19836', '-0.29026', '0.12074', '-0.31957', '-0.66487', '0.22165', '-0.70765', '-0.46892', '0.4574', '0.27551', '-0.33431', '0.57212', '-0.29854', '0.55793', '0.6987', '0.44605', '-0.60738', '-0.38327', '0.31548', '0.35143', '0.27462', '-0.064983', '0.7844', '-0.70439']\n",
      "[!] l = ['At', 'Killerseats.com', '-0.13854', '-0.01706', '-0.13651', '0.1237', '0.15633', '-0.16556', '0.29374', '-0.064174', '0.3209', '-0.091516', '-0.017139', '-0.061828', '-0.17736', '-0.037355', '0.0015256', '-0.13485', '-0.1714', '-0.040029', '0.12924', '0.06956', '0.02036', '-0.011202', '0.0039595', '-0.1076', '-0.014068', '0.091797', '0.0064314', '-0.14604', '0.0039716', '-0.045866', '-0.16544', '-0.13641', '0.099001', '0.10089', '0.018211', '0.045112', '0.18615', '-0.018217', '-0.085986', '-0.13309', '-0.0047582', '-0.076491', '0.23942', '0.061977', '-0.11535', '-0.02209', '0.1325', '-0.0012518', '0.10991', '-0.18539', '-0.12344', '-0.06419', '0.0020279', '0.08325', '0.23703', '-0.20077', '0.0068709', '-0.24464', '-0.019755', '-0.11095', '0.0046046', '-0.20673', '-0.049618', '-0.067157', '0.02305', '0.09159', '-0.11349', '0.030432', '-0.034791', '0.1984', '-0.056183', '-0.10847', '0.0062699', '0.047591', '0.0012842', '0.039199', '-0.086015', '0.12928', '0.27708', '-0.11308', '-0.030623', '0.10101', '-0.26294', '0.021336', '-0.030286', '0.094648', '-0.26473', '0.20893', '0.011739', '-0.27749', '-0.26562', '0.015384', '-0.066983', '0.091436', '-0.01803', '-0.10952', '0.033702', '0.19421', '0.018353', '0.067715', '-0.14521', '0.076887', '0.20074', '0.049053', '0.031508', '0.18307', '0.12594', '-0.012344', '-0.00075399', '0.20056', '0.18769', '-0.13718', '0.23926', '0.15341', '0.068396', '0.10398', '0.12115', '0.049698', '-0.0432', '0.11788', '0.063636', '-0.051266', '0.00095202', '-0.063047', '-0.11646', '-0.0499', '-0.010252', '0.037608', '0.027558', '-0.047844', '0.18169', '-0.026459', '-0.08777', '0.0095411', '0.17127', '-0.10256', '-0.17665', '-0.0016558', '-0.29702', '-0.14523', '-0.060911', '-0.12564', '0.27857', '0.016822', '0.051903', '-0.21652', '-0.15691', '0.033816', '-0.045045', '-0.16395', '0.12095', '-0.036018', '-0.12004', '0.16239', '0.17236', '0.0043345', '-0.09299', '0.12625', '-0.16393', '-0.16347', '0.13903', '-0.0024316', '-0.059626', '0.094548', '-0.16164', '-0.025202', '0.09998', '-0.2755', '-0.028732', '-0.073529', '-0.25114', '-0.057089', '-0.14148', '0.011597', '-0.25583', '-0.040875', '-0.0020845', '0.24255', '0.094112', '0.28931', '0.20464', '0.15538', '-0.27549', '0.046415', '-0.018823', '0.036006', '-0.1589', '-0.1442', '-0.11284', '0.19729', '-0.11681', '-0.0027812', '0.11', '-0.069434', '0.16456', '-0.19392', '0.03563', '-4.8304e-06', '0.21579', '0.096814', '0.17293', '-0.30057', '0.087337', '-0.088238', '-0.13899', '0.16369', '-0.061456', '0.10775', '0.23821', '0.08518', '0.076787', '-0.091381', '-0.12392', '-0.12457', '0.20588', '-0.21204', '0.064002', '0.22108', '-0.34458', '-0.029845', '-0.086207', '-0.070035', '0.015213', '0.0044742', '0.054369', '0.16562', '-0.071822', '-0.0196', '-0.18231', '0.0070107', '0.10313', '-0.057425', '0.09921', '-0.17666', '0.30342', '-0.09457', '0.087363', '-0.10153', '-0.15785', '0.074526', '-0.055874', '-0.24675', '-0.059254', '0.093743', '0.10697', '0.11725', '0.010904', '-0.070051', '-0.094899', '0.12954', '-0.037657', '-0.11976', '-0.25947', '0.10308', '0.12485', '-0.06518', '-0.018488', '-0.045376', '0.12019', '-0.020251', '-0.046336', '0.056461', '-0.041232', '-0.0052311', '0.02632', '0.0015934', '0.11822', '0.20279', '0.011894', '0.32837', '-0.025418', '0.050482', '-0.37133', '-0.08665', '-0.0078504', '-0.031485', '-0.22887', '0.046758', '0.10404', '-0.086826', '-0.31606', '0.14347', '0.081695', '0.0028145', '0.13973', '0.10326', '0.064327', '0.06976', '-0.03511', '0.038678', '-0.11595', '0.26615', '-0.22468', '0.26911', '0.019649', '-0.13941', '0.024992', '-0.2906', '0.32547', '0.12586']\n",
      "[!] l = ['by', 'name@domain.com', '0.6882', '-0.36436', '0.62079', '1.1482', '-0.055475', '-0.37936', '0.0064471', '-0.33046', '-0.43406', '-1.3468', '0.70312', '-0.41314', '-0.65868', '0.64324', '0.13018', '0.65846', '0.86269', '-0.93108', '0.3476', '0.73912', '-0.51405', '-0.15113', '0.27331', '0.51396', '-0.74688', '0.87989', '-0.11887', '0.3641', '0.37838', '0.36177', '-0.45182', '0.16173', '-0.36353', '-0.55643', '-1.1186', '0.70117', '-0.48075', '0.074095', '0.43022', '0.4625', '0.011133', '0.030287', '-0.73342', '-0.772', '0.31058', '0.022106', '-0.16845', '-0.70695', '-0.16243', '-0.15454', '-0.12034', '0.018702', '0.51626', '-0.17255', '0.37335', '-0.059377', '0.013126', '-0.30727', '0.1581', '0.74527', '-0.7927', '-0.34603', '-0.01438', '-1.055', '-0.95074', '-0.81794', '0.27925', '-0.35405', '-0.26783', '-0.30391', '0.16093', '-0.064806', '0.69283', '-1.1955', '0.18414', '-0.71183', '0.062622', '-0.62435', '-0.16458', '-0.74362', '-0.19251', '-0.1841', '0.99035', '-0.20552', '-0.46621', '0.98506', '1.4113', '0.024391', '-0.14285', '0.40063', '0.10516', '0.065123', '-0.4613', '0.27429', '0.022191', '0.55307', '0.18442', '-0.22378', '-0.50433', '0.046039', '0.12306', '-0.11203', '-0.30851', '-0.13275', '-0.36831', '-0.63785', '-0.99149', '-0.55833', '0.17128', '0.27324', '-0.37803', '0.4641', '0.39427', '0.048909', '0.46002', '0.072136', '0.090133', '-0.77511', '0.31636', '1.0484', '0.65455', '-0.54555', '-0.16023', '0.51455', '0.46977', '-0.70197', '-0.032668', '-0.23516', '-0.053423', '0.6293', '-0.35302', '-0.44905', '-0.44235', '-0.14551', '-0.60106', '-0.68264', '0.71618', '0.44723', '0.17647', '0.60878', '1.0692', '-1.0488', '-0.48178', '0.10825', '-0.27708', '0.50754', '0.69316', '-0.40861', '0.98707', '0.045813', '0.6237', '-0.041425', '0.23719', '0.50664', '-0.14663', '0.55561', '0.60422', '0.089302', '-0.18124', '0.51483', '0.51676', '-0.25888', '0.64182', '0.22324', '0.54462', '0.39816', '-0.97695', '0.17496', '-0.35361', '-0.77967', '0.23945', '0.21711', '0.26793', '-0.029782', '0.75712', '0.083235', '0.38329', '0.14695', '-0.42997', '-0.12402', '0.078012', '0.69183', '0.36966', '0.75105', '0.91858', '-0.11291', '0.53013', '0.35894', '0.65682', '-0.30278', '0.074167', '0.064695', '0.08892', '-0.04165', '-0.5437', '1.2794', '0.21133', '0.47272', '0.24959', '-0.77101', '-0.20456', '0.54224', '0.44168', '0.11797', '-0.28053', '-0.30537', '0.39505', '-0.46693', '0.54209', '0.58962', '0.17908', '0.15278', '0.51678', '1.0729', '-0.21624', '-0.63379', '0.6209', '-0.029061', '0.45802', '-0.70106', '-0.096258', '-0.13379', '0.99638', '0.62168', '0.045304', '-0.85691', '0.15816', '-0.77156', '0.45802', '-0.48487', '0.10298', '0.12015', '-0.61097', '-0.51157', '-0.49761', '0.34814', '0.20141', '0.33286', '1.2239', '-0.42395', '-0.33754', '-0.51882', '-0.66346', '-0.32048', '0.25638', '0.06427', '0.22788', '-0.32311', '0.11933', '-0.47903', '0.049264', '-0.35126', '-0.79376', '-0.073158', '0.25249', '-0.92067', '-0.2175', '0.4002', '-0.17607', '-0.49748', '0.028808', '-0.66234', '-0.15404', '-0.51768', '0.023225', '0.46048', '0.19184', '0.17417', '-0.32702', '0.0023848', '0.36328', '0.40963', '0.2658', '-0.016626', '-0.85864', '-0.77511', '0.71664', '-0.8535', '-0.53959', '-0.29513', '-0.11449', '0.25627', '0.3023', '-0.32311', '0.58707', '0.61075', '-0.68101', '-0.26994', '-0.50763', '0.23818', '1.1097', '-0.5269', '-0.036155', '-0.20817', '0.2446', '-1.0573', '-0.11534', '0.08409', '-0.47809', '-0.32664']\n",
      "[!] l = ['in', 'mylot.com', '-0.18148', '0.47096', '0.32916', '0.044196', '-0.93045', '-0.16299', '0.31996', '0.39017', '0.013753', '-1.0117', '1.4497', '0.14411', '0.09409', '-0.034215', '-0.11889', '-0.24614', '0.88631', '-0.62527', '0.1653', '0.17695', '-0.092857', '-0.82453', '-0.3729', '-0.32292', '-0.048221', '-0.42264', '0.11826', '0.17933', '-0.058891', '0.16458', '0.09932', '0.4913', '0.051474', '-0.059584', '-0.086531', '0.060116', '-0.24148', '0.34348', '0.011429', '0.046588', '0.33665', '-0.63822', '0.036459', '0.65724', '0.17596', '-0.33444', '-0.17779', '-0.047878', '0.29427', '0.10278', '0.31098', '0.62907', '-0.098816', '-0.5123', '-0.43283', '0.47748', '-0.13249', '-1.0708', '0.081945', '-0.04147', '-0.23569', '0.3655', '-0.30838', '-0.76359', '-0.36836', '-0.22545', '-1.1477', '-0.12543', '0.087085', '0.10207', '-0.24626', '-0.41621', '0.20584', '-0.034737', '-0.79378', '-0.32414', '0.32755', '0.66629', '0.058202', '0.13797', '0.39361', '-0.41695', '-0.40613', '-0.076039', '-0.11123', '-0.17525', '-0.0088467', '-0.044072', '-0.69197', '-0.74632', '0.39905', '-0.047611', '0.19114', '0.83765', '0.42194', '0.4051', '0.18833', '0.76741', '-0.50475', '-0.38181', '0.054343', '-0.22186', '-0.18301', '-0.1434', '-0.66303', '0.54454', '-0.19581', '-0.37495', '0.37518', '-0.14736', '0.12769', '0.066166', '-0.66023', '-0.13895', '0.47418', '0.44716', '0.14277', '-1.1752', '0.17875', '-0.26377', '-0.71254', '-0.056002', '1.1058', '0.48956', '-0.0034874', '-1.2341', '-0.25508', '0.44549', '-0.47797', '0.38046', '0.062388', '-0.30549', '0.018524', '-0.73576', '0.11699', '-0.48136', '0.68783', '0.34749', '0.33697', '0.25123', '1.6869', '0.22458', '-0.25622', '0.29453', '-0.67176', '0.77372', '0.17065', '-0.90475', '0.043384', '-0.1352', '-0.035265', '-0.70517', '-0.45938', '-0.068112', '-0.51148', '0.47336', '-0.10527', '-0.4393', '-0.38211', '-0.27846', '0.19689', '0.075033', '0.32149', '-0.24502', '0.054691', '0.11946', '-0.26407', '-0.0074057', '-0.66552', '-0.20797', '-0.043512', '0.53229', '-0.28674', '0.74577', '0.27161', '-0.18583', '0.020504', '0.032247', '0.093833', '0.16584', '0.0064513', '-0.11134', '-0.6277', '0.66321', '1.0995', '0.17221', '0.39757', '0.22174', '0.37047', '-0.11836', '0.046102', '-0.21112', '0.41396', '-0.0033789', '-0.41364', '0.41285', '0.06004', '0.067526', '-0.21855', '-0.48179', '0.16688', '0.59051', '0.21349', '0.41751', '0.065228', '-0.83219', '0.28468', '0.027128', '0.56385', '0.5687', '0.12164', '0.26231', '0.10388', '-0.051454', '0.16501', '-0.92392', '-0.10778', '-0.31637', '0.28046', '0.080897', '-0.020649', '0.56557', '0.56447', '0.48398', '-0.85527', '-0.17136', '0.038894', '-0.28895', '-0.19452', '-0.17616', '0.39584', '0.43994', '0.36613', '0.012852', '-0.13704', '0.11059', '-0.39179', '-0.03542', '0.084348', '-0.41242', '-0.59482', '-0.51298', '0.104', '-0.083639', '-0.84864', '0.34791', '-0.22998', '0.41067', '0.20366', '-0.56011', '-1.0281', '0.66185', '0.11875', '1.1039', '0.15888', '-0.18237', '-0.50039', '-0.058217', '-0.60943', '0.35013', '-0.62093', '0.098393', '0.50862', '-0.10342', '-0.14692', '0.014277', '0.63924', '0.14633', '-0.2731', '0.069363', '0.78212', '0.17801', '-0.81651', '0.087242', '-0.83718', '0.037194', '0.29325', '-0.062079', '0.038745', '0.12474', '-0.40302', '0.11265', '-0.82604', '-0.12445', '0.016351', '-0.41329', '-0.58576', '-0.44029', '-0.0035195', '0.26308', '0.071372', '0.43961', '-0.34098', '-0.27518', '0.20454', '0.32802', '-0.39246', '0.11756', '0.1804', '-0.17405']\n",
      "[!] l = ['0.5478', '0.18474', '0.14057', '-0.92452', '-0.90097', '-0.11618', '-0.53059', '0.34625', '0.15681', '-1.6009', '0.87718', '-0.43912', '0.62262', '1.0086', '-0.24369', '-0.073876', '0.15422', '-1.4411', '0.04154', '0.21189', '-0.078096', '0.39895', '0.33953', '0.74124', '-0.882', '-0.26633', '-0.85343', '0.30064', '-0.020434', '-0.61369', '-0.19055', '-0.057147', '0.12374', '0.20491', '-0.22628', '-0.0091328', '0.66361', '0.16047', '0.25018', '0.5086', '0.33618', '-0.56053', '-0.15328', '0.26308', '-0.11548', '-0.38679', '-0.044865', '-0.007149', '-0.89748', '0.58535', '0.6608', '0.024609', '0.57439', '-0.58476', '-0.30736', '-0.71418', '-0.32937', '-0.3314', '-0.18063', '0.47736', '-0.37436', '0.074195', '1.2006', '-0.7102', '-0.96096', '-0.26169', '0.69275', '-0.44601', '0.12978', '-0.29132', '0.25006', '-0.14374', '0.062975', '-0.0049365', '0.098459', '-0.04074', '-0.74706', '0.50604', '-0.14141', '-0.051607', '-0.55256', '-0.83174', '0.12062', '-0.20317', '-0.41949', '-0.1259', '1.0226', '-0.18972', '-0.54802', '-0.41357', '-0.086502', '0.17879', '-0.20251', '-0.34271', '0.14262', '0.75682', '0.070895', '-0.61308', '0.0021172', '-0.21004', '-0.1065', '0.69804', '0.27875', '0.51017', '-0.25194', '-0.76736', '0.42043', '0.35976', '-0.75106', '-0.78491', '0.56864', '0.40162', '0.47725', '0.52216', '0.45299', '0.64315', '-0.12639', '0.19486', '-0.1274', '0.45286', '-0.59884', '-0.7082', '-0.2396', '-0.1469', '-0.13079', '0.34273', '0.014092', '-0.42714', '-0.42578', '0.17877', '0.33789', '-0.96627', '0.3436', '0.41198', '-0.58338', '0.18418', '0.22099', '-0.34998', '0.013181', '-0.18736', '0.92477', '0.042716', '-0.84905', '0.32272', '-0.69996', '0.21243', '0.15588', '0.17886', '-0.53359', '-0.03536', '0.31629', '0.45827', '-0.70023', '1.0647', '0.42755', '0.58494', '-0.44634', '-1.1485', '0.075585', '-0.16845', '-0.69743', '0.33217', '0.45417', '-0.071693', '-0.036545', '-0.20981', '0.55431', '0.69681', '0.27748', '0.59429', '-0.47597', '0.82669', '0.48641', '0.027003', '-0.1806', '-0.45916', '0.033882', '0.27143', '-0.71112', '-0.24105', '1.1416', '-0.43917', '0.094856', '0.25288', '0.42936', '0.7144', '0.21149', '-0.28522', '-0.56226', '-0.222', '0.26691', '1.0352', '0.71026', '-0.71584', '0.25802', '-0.23319', '0.45549', '0.26215', '-0.31332', '0.0194', '0.64079', '0.6694', '0.26666', '-0.0021461', '-0.26552', '0.56062', '-0.049035', '0.5463', '-0.19113', '0.17019', '0.30571', '0.76945', '-0.11537', '0.78514', '-0.39848', '-0.58326', '-1.0197', '0.38151', '0.6642', '0.17558', '0.357', '-0.84856', '0.40566', '0.12277', '-0.24621', '-0.13929', '-0.18337', '-0.306', '-0.85803', '-0.77093', '-0.055736', '0.61975', '-1.023', '-0.50313', '-0.0014855', '1.047', '0.49733', '-0.69285', '-0.23979', '0.52959', '-0.12767', '0.21181', '0.16831', '-0.29236', '-0.33376', '-0.27753', '0.39377', '1.0664', '0.28654', '-0.5776', '0.13813', '0.55482', '0.12714', '0.45065', '0.15148', '0.0057206', '0.6618', '0.51676', '0.42685', '0.038925', '-0.025408', '-0.38989', '0.88643', '0.30902', '-0.21188', '0.31677', '0.12778', '-0.42002', '-0.41836', '0.48682', '-0.74989', '0.15952', '-0.41727', '0.088895', '0.13082', '0.28449', '0.030655', '0.1761', '-0.57418', '-0.58955', '-0.68712', '0.30097', '-0.29138', '0.37903', '0.22141', '0.040963', '-0.33546', '0.33845', '-0.39784', '0.72826', '0.097868', '0.20954', '-0.092298', '-0.46449', '0.50592', '-0.041343', '0.21309', '-0.10714', '0.2881', '-0.52064']\n",
      "[!] l = ['emailing', 'name@domain.com', '0.39173', '-0.39132', '-0.4266', '0.82429', '0.42919', '0.17601', '0.16663', '-0.011601', '-0.53551', '-1.255', '0.35066', '0.06361', '-0.70235', '-0.31163', '0.25379', '0.2463', '0.21129', '-1.5845', '0.19661', '-0.01602', '-0.54956', '-0.58445', '0.03272', '0.90671', '0.33191', '-0.52747', '0.32609', '0.53668', '-0.18302', '-0.49699', '0.069751', '-0.32102', '0.034384', '0.30422', '-0.37326', '0.73485', '-0.36973', '0.89839', '0.30623', '0.18463', '-0.23602', '-0.17', '0.34307', '0.026764', '-0.1685', '0.4373', '0.43021', '-0.17562', '0.15805', '0.19768', '0.064745', '0.048354', '-0.17168', '-0.24719', '-0.2572', '-0.59113', '0.1086', '-0.20647', '0.19999', '0.25039', '-0.50867', '0.013532', '-0.1393', '0.20503', '-0.94526', '0.76842', '-0.25137', '0.062295', '-0.0779', '-0.43982', '0.17724', '-0.5318', '0.15458', '-0.26637', '-0.34106', '-0.26619', '0.39768', '-0.0027818', '0.17052', '-0.29347', '-1.3207', '-0.43398', '-0.22671', '0.026272', '0.12023', '0.12684', '0.92681', '-0.62325', '-0.086858', '-0.17383', '0.38999', '0.31858', '-0.32013', '0.32169', '-1.4042', '0.774', '-1.0086', '0.076515', '0.22546', '-0.34501', '0.64108', '-0.62987', '-0.33897', '0.69475', '0.50582', '-0.79349', '-0.41876', '0.66703', '-0.081664', '-0.079977', '-0.099099', '-0.41525', '0.9304', '-0.13361', '0.17219', '-0.32933', '-0.29329', '-0.1672', '0.12365', '0.75392', '0.49888', '0.51591', '0.5768', '0.19417', '0.49986', '-0.60501', '0.5596', '0.16308', '0.23527', '0.34854', '-0.4135', '-0.4168', '-0.34117', '0.41811', '-0.22984', '0.045453', '1.021', '0.15702', '-0.16201', '0.34939', '0.74524', '0.64977', '-1.0165', '-0.24076', '0.024844', '0.44116', '0.79557', '-0.72798', '-0.046033', '0.28843', '0.41386', '0.38713', '-0.29446', '-0.1907', '-0.67133', '0.57932', '-0.1499', '-0.049878', '-0.2862', '1.0487', '0.38315', '0.5175', '0.17042', '-1.03', '0.6565', '0.075288', '0.21532', '0.2055', '-0.10369', '-0.24518', '-0.79469', '0.54382', '0.56461', '0.12539', '0.23951', '-0.17072', '0.05467', '-0.31934', '-0.699', '0.40972', '0.00091743', '0.32084', '0.050919', '-0.19141', '0.90119', '0.39184', '0.97954', '0.036333', '0.66042', '0.94628', '0.74539', '-0.36498', '1.0807', '-0.050241', '0.16434', '-0.014925', '0.9391', '0.51309', '0.4418', '0.42688', '0.37485', '0.021399', '0.3364', '0.061034', '0.87373', '0.20534', '0.51232', '0.44053', '0.35507', '0.17071', '-0.46624', '0.21338', '-0.28831', '0.14749', '0.50913', '0.52774', '0.3225', '0.18207', '0.25947', '0.53903', '0.78083', '-0.47849', '-0.37548', '-0.2368', '-0.26527', '-0.31599', '-0.72522', '-0.2193', '0.14468', '-0.51742', '-0.29822', '0.52975', '0.19253', '0.02847', '-0.12754', '0.31759', '0.073123', '0.23208', '-0.32065', '-0.64801', '-0.49646', '0.34046', '-0.16553', '-0.60268', '0.44127', '-0.53358', '0.53215', '0.20003', '0.36597', '-0.064675', '0.06453', '-0.18848', '-0.58001', '-0.13561', '0.12283', '-0.53895', '0.12008', '-0.52819', '-0.15895', '0.23558', '-0.32919', '0.15223', '-0.15', '-0.8083', '0.25604', '0.81459', '0.18576', '0.048995', '0.48536', '0.012105', '0.43842', '0.26448', '0.52854', '0.16257', '-0.20932', '0.089357', '-0.014704', '-0.15414', '0.13277', '-0.43273', '-0.43306', '0.0024893', '0.15383', '-0.27752', '0.67561', '0.68944', '1.0019', '0.52244', '0.14167', '0.51255', '0.84324', '0.19646', '-0.055988', '-0.65196', '-0.023645', '0.26897', '-0.13204', '0.34183', '-1.1074', '-0.21701']\n",
      "[!] l = ['Contact', 'name@domain.com', '0.14933', '-0.28605', '0.3444', '0.29015', '-0.22999', '0.1271', '0.35722', '0.35118', '0.28224', '-1.3679', '0.44651', '-0.017463', '-0.12998', '0.39657', '0.71315', '0.11213', '0.70139', '-1.5873', '0.7609', '0.072771', '0.66317', '-0.33039', '0.31256', '-0.020889', '-0.47135', '0.3383', '0.53767', '0.057899', '-0.023796', '-0.31841', '0.23784', '-0.28834', '0.17782', '-0.5244', '0.29458', '0.44297', '0.53541', '-0.10591', '1.0599', '0.18864', '0.55033', '-0.50063', '0.022688', '-0.17678', '-0.70947', '-0.8091', '0.27532', '-0.048975', '0.32952', '-0.26311', '0.54794', '0.50121', '0.61235', '0.039506', '-0.39968', '-0.84155', '-0.52122', '-0.6594', '-0.43807', '0.54237', '-0.79301', '0.034652', '-0.12363', '-0.35739', '-0.46396', '0.92434', '0.29489', '0.26151', '-0.25829', '-0.55722', '-0.14739', '-0.21945', '0.46516', '-0.44058', '0.41805', '-0.011851', '0.5441', '-0.21327', '0.043397', '-0.10785', '-1.3427', '-0.37442', '-0.40596', '0.044713', '0.071265', '0.22554', '1.0166', '-0.18678', '-0.080248', '-0.070555', '0.38612', '-0.034025', '0.33291', '0.16814', '-0.33787', '0.48429', '-0.9275', '-0.37455', '0.28879', '0.011307', '-0.034424', '-0.074561', '0.33677', '0.086636', '0.58377', '-0.97006', '0.21602', '-0.10354', '-0.18782', '-0.18874', '-0.0075329', '0.5045', '0.41263', '0.63932', '0.46135', '0.043514', '0.037813', '0.48608', '0.45065', '0.094473', '-0.51254', '-0.2553', '0.60168', '-0.22751', '0.14411', '-0.015375', '0.26335', '0.12941', '0.55472', '-0.35128', '0.14395', '-0.37141', '-0.47784', '0.54191', '-0.30975', '-0.12484', '-0.10024', '-0.080352', '-0.79665', '-0.65575', '0.99039', '0.092417', '-1.0277', '0.10834', '-0.24512', '-0.16528', '0.034586', '0.35529', '-0.27859', '0.28722', '0.30695', '0.31876', '-0.4585', '0.35226', '-0.03028', '0.047087', '1.488', '-0.11154', '0.40292', '0.092156', '0.89874', '0.30825', '0.068088', '0.072786', '0.77015', '0.43306', '-0.089741', '0.34737', '-0.60596', '-0.10934', '0.45442', '0.38238', '0.90917', '0.40277', '0.037961', '-0.71513', '-0.28856', '-0.21618', '0.098854', '0.65154', '0.36816', '0.12634', '-0.07398', '-0.81584', '0.79696', '0.65227', '0.62773', '-0.19997', '0.58956', '1.2545', '0.16172', '-0.26292', '0.48653', '0.12693', '-0.20123', '0.12498', '0.43937', '0.64485', '0.5004', '-0.11575', '-0.010742', '-0.094577', '0.52967', '0.14039', '0.30212', '0.58504', '-0.074609', '-0.35289', '0.032048', '0.23581', '-0.47016', '-0.041431', '0.15496', '-0.25254', '-0.18551', '0.34496', '-0.016185', '0.1799', '0.76838', '0.5616', '-0.0043105', '-0.083277', '0.86002', '-0.2186', '-0.78777', '-0.31371', '-0.408', '-0.30427', '-0.48196', '-0.69419', '0.00093017', '0.20153', '0.46771', '0.055064', '0.16938', '0.028213', '-1.1183', '0.22422', '0.32095', '-0.21798', '0.78578', '-0.4124', '-0.90433', '-0.23226', '0.80892', '0.36045', '0.34569', '0.7428', '0.57411', '0.013348', '-0.24344', '-0.5165', '-0.12468', '0.21977', '-0.46848', '-0.44024', '0.57263', '-0.39965', '0.00536', '0.15398', '-0.57058', '-0.12496', '0.17647', '-0.36044', '-0.077987', '0.2501', '-0.18364', '-0.106', '-0.079021', '-0.11581', '-0.026927', '0.7054', '0.39769', '0.30528', '-0.12727', '0.49794', '0.52739', '-0.037058', '-0.61461', '-0.74724', '0.57124', '0.0069423', '-0.022656', '-0.86076', '0.33951', '0.8744', '-0.00072818', '0.31948', '-0.47734', '0.29885', '0.72535', '-0.79345', '-0.031487', '-0.58767', '0.3152', '0.0075835', '-0.27275', '-0.017409', '0.43731', '-0.05677']\n",
      "[!] l = ['0.59759', '-0.64012', '0.32797', '-0.40237', '-0.96606', '0.059478', '-0.21576', '0.16216', '-0.32797', '-1.6323', '0.68522', '-0.073292', '0.21555', '1.3283', '-0.031215', '-0.53429', '0.49431', '-1.1694', '0.24104', '-0.33426', '0.58275', '0.38341', '0.55435', '0.48166', '-0.52648', '0.064375', '-0.8332', '0.36364', '0.41019', '-0.72814', '0.37718', '0.12334', '0.65675', '0.24793', '-0.18758', '0.14272', '0.62822', '0.29398', '-0.38322', '0.1577', '0.61929', '0.13675', '0.58332', '0.19871', '0.25006', '-0.81523', '-0.45037', '0.079187', '-0.35683', '1.014', '0.56923', '0.011635', '0.73754', '-0.16272', '0.16946', '-0.85107', '0.28715', '-0.040366', '-0.47167', '0.64212', '-0.17451', '-0.03211', '1.2196', '-0.77149', '-0.83482', '-0.26648', '0.52057', '0.027308', '-0.30045', '0.052085', '0.5338', '-0.34966', '-0.15528', '-1.1164', '-0.30006', '0.4234', '-0.81019', '0.50919', '-0.46411', '0.043904', '-0.50195', '0.081817', '0.51933', '0.12137', '-0.21922', '0.010013', '0.76642', '0.39335', '-0.16865', '-0.45341', '-0.76879', '-0.15407', '-0.68731', '-0.12682', '0.03759', '0.78558', '-0.10268', '-0.98331', '0.35893', '-0.066734', '-0.43872', '0.5072', '0.61228', '0.99948', '-0.59586', '-0.83491', '0.13185', '-0.5546', '-1.0491', '0.010842', '0.14695', '0.10353', '0.15564', '0.062978', '0.36645', '0.097292', '-0.34948', '0.44568', '-0.11599', '0.64676', '-0.97452', '0.1114', '-0.092003', '0.31164', '-0.70877', '-0.33556', '-0.47921', '0.49446', '-0.55879', '0.15302', '-0.27326', '0.012649', '0.20556', '-0.19405', '0.26573', '-0.2673', '0.63431', '-0.36549', '0.32918', '-0.79218', '1.0017', '-0.12909', '-0.62981', '0.73703', '-1.0183', '-0.16428', '0.50129', '0.31386', '-0.75925', '-0.18461', '-0.31729', '0.74761', '-0.74921', '0.66782', '0.60203', '0.68145', '-0.25125', '-0.038851', '0.059788', '-0.36312', '0.20819', '0.32061', '1.1507', '-0.11093', '0.64951', '0.23964', '-0.3711', '0.54309', '-0.61297', '0.59027', '0.04426', '0.32154', '0.45133', '0.2978', '0.45305', '-0.17236', '-0.98148', '0.19651', '-0.54731', '-0.42562', '0.38961', '-0.56711', '0.79143', '0.14931', '0.70015', '0.55543', '0.02439', '0.11753', '-0.083093', '-0.14134', '0.53873', '0.64715', '0.70151', '-1.2352', '0.51635', '0.08466', '0.99562', '0.16819', '-0.45894', '-0.43009', '-0.041268', '0.85343', '0.59703', '0.03857', '-0.75895', '0.38059', '0.011928', '0.15895', '-0.49433', '0.31188', '0.55795', '0.489', '0.28439', '0.5521', '-0.29506', '-0.24759', '-0.38424', '0.53704', '0.73393', '-0.10952', '0.33085', '-0.30816', '0.53731', '-0.22865', '-0.63114', '-0.61012', '0.61583', '-0.45295', '-1.226', '-0.90847', '0.4904', '0.089766', '-0.3157', '-0.27985', '-0.65357', '0.92423', '0.71765', '-0.41399', '-1.0852', '0.37901', '-0.10546', '0.52884', '0.65648', '-0.64929', '-0.86047', '0.49595', '0.73634', '1.0105', '-0.12911', '0.157', '0.067726', '0.15838', '0.18021', '0.97215', '0.1298', '-0.068941', '0.38541', '0.42544', '0.17778', '0.39474', '-0.16531', '-0.5659', '0.1885', '0.45535', '-0.10229', '0.077971', '-0.044195', '-0.26739', '-0.48962', '-0.27334', '-0.13136', '-0.22749', '0.074986', '0.44992', '-0.16366', '-0.036424', '0.45594', '0.29928', '-0.11101', '-0.54998', '-0.68498', '0.6914', '-0.026335', '0.44588', '0.17025', '0.64951', '0.045954', '1.3128', '-0.91445', '0.29611', '0.25968', '-0.331', '-0.60079', '-0.49182', '0.74319', '-0.22999', '-0.10779', '-0.45078', '-0.34475', '-0.56342']\n",
      "[!] l = ['at', 'name@domain.com', '0.44321', '-0.40005', '-0.20065', '1.1209', '0.34041', '0.086082', '-0.067128', '0.0022702', '-0.94649', '-1.4669', '0.61248', '0.34827', '-0.20983', '-0.61434', '0.41102', '0.57759', '0.69071', '-1.9301', '0.75265', '-0.13238', '0.22003', '0.28856', '0.35234', '0.45989', '-0.21944', '0.1931', '-0.11664', '0.14996', '0.70354', '-0.039238', '0.55298', '-0.53503', '-0.3221', '-0.28595', '-0.1246', '0.054544', '-0.45937', '0.1447', '0.8203', '-0.33182', '0.10864', '-0.56552', '0.39898', '-0.65012', '-0.20285', '0.11557', '0.35711', '-0.23958', '-0.30281', '0.51593', '0.71883', '-0.30403', '0.59458', '-0.3217', '-0.23967', '-0.2576', '-0.50224', '-0.36055', '-0.71763', '0.4981', '-0.69945', '-0.0072578', '0.37327', '-0.029839', '-0.42705', '0.93128', '-0.046928', '0.045162', '-0.44879', '0.16579', '-0.26272', '-0.35286', '0.17395', '-0.24436', '-0.1439', '-0.39857', '0.25342', '-0.44737', '0.37618', '-0.80252', '-0.87776', '-0.19282', '-0.48746', '0.065159', '-0.24349', '-0.77669', '0.81629', '-0.043888', '-0.68276', '-0.15709', '-0.46533', '-0.066009', '0.063028', '0.090332', '-0.81297', '0.88979', '-0.6391', '0.17351', '0.3328', '-0.30808', '0.46158', '-0.11289', '-0.0261', '-0.089243', '0.37318', '-0.73511', '0.19798', '-0.060219', '-0.12113', '-0.2146', '0.62061', '0.34296', '0.89595', '0.22495', '0.16079', '-0.84005', '-0.6749', '0.4053', '0.17549', '0.70242', '-0.22907', '0.068957', '0.092359', '0.034122', '0.24296', '-0.47643', '0.27613', '0.04291', '-0.055271', '0.35486', '0.32692', '-0.66761', '-0.77199', '-0.075454', '-0.36238', '0.052461', '0.34678', '0.070744', '-0.5845', '0.14793', '0.81643', '0.45906', '-0.80253', '0.18628', '-0.039264', '0.21197', '0.45927', '-0.274', '-0.81792', '0.73455', '0.30696', '0.10415', '0.035539', '0.012445', '0.088135', '0.67792', '0.45663', '0.2726', '-0.50234', '0.62198', '0.38648', '0.01011', '-0.32851', '-0.068547', '0.85187', '0.14533', '0.28169', '0.757', '-0.61313', '0.46784', '-0.53806', '0.22123', '0.59195', '-0.52707', '0.342', '-0.32091', '-0.0031903', '-0.063289', '-0.59452', '0.22196', '-0.2044', '0.26864', '0.14327', '0.077514', '0.75915', '0.17929', '0.27785', '-0.24118', '0.57407', '0.94331', '0.61097', '-0.36571', '0.37771', '0.30822', '-0.42249', '1.1221', '0.67737', '0.29251', '0.42105', '0.1204', '-0.19767', '0.70793', '-0.19204', '0.27399', '0.21617', '0.58442', '0.020547', '-0.004382', '0.2435', '-0.30135', '-0.32904', '0.2376', '-0.42115', '-0.2809', '-0.12963', '0.23212', '0.28883', '0.047856', '0.019705', '0.57196', '0.56619', '0.012819', '0.24007', '0.22556', '-0.47738', '-0.15644', '-0.64105', '-0.57604', '-0.36252', '-0.20212', '-0.3813', '0.27112', '0.37976', '0.20358', '-0.28793', '0.36938', '-0.42238', '0.14111', '-0.19996', '-0.34411', '0.10342', '-0.49926', '-0.32485', '0.42403', '0.35958', '-0.35653', '0.27493', '0.43134', '0.45197', '-0.17992', '0.36502', '-0.1422', '-0.20199', '-0.35023', '0.16243', '-1.1251', '0.044546', '-0.59278', '0.03059', '-0.44425', '-0.82482', '-0.21953', '-0.39308', '-0.84476', '0.20993', '0.99719', '0.1983', '-0.28091', '-0.36525', '0.52296', '0.65851', '0.46585', '0.52257', '0.14744', '-0.74425', '-0.12268', '0.81749', '-0.2461', '0.018109', '-1.0154', '0.29857', '0.32427', '0.57664', '-0.41371', '-0.099047', '0.24261', '0.12247', '0.31508', '-0.076621', '0.6407', '0.25333', '-0.2548', '0.022574', '-0.037949', '0.0059522', '0.068205', '-0.11948', '-0.44629', '-0.45647', '-0.15084']\n",
      "[!] l = ['•', 'name@domain.com', '-0.13288', '-0.31383', '-0.032356', '0.52036', '-0.26985', '0.43339', '0.32587', '-0.51581', '-0.9806', '-1.5879', '0.73555', '0.35252', '-0.73296', '0.6446', '0.37158', '-0.26666', '0.21589', '-0.91457', '0.18992', '0.81333', '0.8067', '0.59507', '-0.5518', '0.48099', '0.12726', '0.03783', '0.24796', '0.50997', '0.075809', '-0.75845', '0.32875', '-0.29168', '0.057426', '-0.16102', '-0.68347', '0.86165', '0.2599', '-0.56354', '-0.040419', '-0.28022', '-0.20049', '0.0064199', '-0.45262', '-0.10725', '0.31171', '0.35856', '0.032082', '1.203', '-0.3693', '-0.027401', '1.1', '-0.20264', '0.18898', '-0.45642', '0.048134', '0.5478', '-0.14036', '0.073562', '1.0508', '-0.12821', '0.33468', '0.2014', '0.028187', '-0.37055', '0.48804', '0.042724', '0.030665', '-0.025691', '0.10987', '-0.31731', '-1.3087', '-0.24197', '-0.16837', '-0.69608', '0.086061', '-0.17898', '0.46507', '0.33104', '0.55842', '-0.60105', '-0.063918', '-0.24297', '0.48818', '0.14094', '-0.59654', '0.79936', '0.58047', '0.24874', '-0.98018', '0.24882', '0.25235', '-0.72269', '-0.2084', '0.12267', '0.011462', '0.16495', '-0.33902', '-0.52693', '-0.42821', '0.42544', '1.0436', '0.31029', '-0.021538', '-1.0413', '-0.80963', '-0.098143', '-0.1954', '0.36169', '-1.1466', '-0.44647', '0.35587', '0.76049', '0.50332', '-0.52995', '0.84432', '0.77632', '-0.18343', '-0.89036', '0.14031', '-0.14793', '-0.55824', '0.16072', '-0.15132', '0.35349', '-0.0015194', '-0.1344', '0.75585', '1.4173', '0.34559', '0.84169', '0.23758', '-0.55682', '-0.4233', '0.34256', '0.21639', '0.23196', '-0.058196', '-0.34022', '0.30834', '0.085223', '1.0005', '-0.75103', '-0.19934', '1.1943', '-0.75858', '0.47305', '0.2842', '-0.23272', '-0.31152', '0.016731', '0.26253', '-0.11474', '-0.97903', '-0.0092514', '0.068853', '0.98873', '-0.24826', '0.081491', '0.48154', '0.83283', '0.70266', '0.42554', '0.57001', '0.98106', '0.18615', '0.48742', '0.57736', '0.55659', '-0.68907', '0.33097', '-1.2181', '-0.01721', '-0.80811', '-0.41657', '-0.03589', '0.019744', '0.38763', '0.75493', '0.9686', '-0.81071', '0.28657', '-0.41282', '0.46566', '-0.3717', '1.0972', '0.36665', '0.90244', '-0.82436', '0.56426', '-0.35644', '0.36488', '-0.47052', '0.98485', '-0.68457', '-0.84468', '-0.34603', '0.71688', '0.63703', '-0.49426', '0.11989', '0.018898', '1.1553', '0.026494', '-0.38328', '-0.83349', '-0.99136', '-0.092262', '0.39492', '0.3145', '-0.55455', '0.12677', '0.18484', '0.070504', '-0.16828', '-0.13925', '0.21572', '0.29662', '-0.24226', '-0.19738', '0.36044', '-0.57074', '0.2877', '0.539', '-0.62249', '-0.61904', '-0.030457', '0.52519', '-0.29111', '-0.037765', '-0.016212', '0.29459', '0.21433', '-0.39144', '-0.76351', '-0.33047', '0.32052', '-0.066346', '0.29668', '0.33397', '-1.3425', '-0.16393', '-0.15417', '-0.1092', '-0.48709', '-0.28508', '0.83707', '-0.44983', '0.99084', '0.70429', '-0.30875', '-1.1595', '-0.25862', '0.099996', '-0.20872', '-0.31094', '-1.0475', '-0.36245', '0.39012', '-0.3529', '0.70455', '-0.33983', '-0.74342', '-0.12024', '-0.8151', '-0.40635', '-0.52105', '0.61012', '-0.25333', '-0.9383', '-0.048584', '-0.14662', '0.48444', '0.84921', '1.1434', '-0.34354', '0.61107', '-1.0268', '0.41495', '0.29176', '-1.3315', '-0.10941', '0.41862', '-0.14078', '-0.41347', '0.71217', '0.5683', '-0.72551', '0.29458', '-0.88888', '0.2785', '0.8716', '0.42549', '-1.2827', '0.60796', '0.10976', '-0.033875', '0.056314', '0.86177', '0.080508', '0.17186']\n",
      "[!] l = ['at', 'Amazon.com', '-0.5275', '-0.73685', '0.10968', '0.22214', '-0.30063', '-0.63201', '-0.053204', '-0.16241', '-0.33811', '-1.4072', '1.1837', '0.45468', '-0.84209', '-0.011505', '0.896', '0.03319', '0.26241', '-1.2644', '0.57994', '-0.23699', '-0.49705', '0.16729', '-0.18469', '0.12163', '0.23774', '-0.0997', '-0.3207', '0.56191', '-0.017743', '-0.32547', '0.014204', '-0.57266', '0.42053', '-0.020948', '-0.29288', '-0.086615', '0.46141', '0.28811', '0.30401', '-0.10123', '-0.90849', '0.36862', '0.40785', '-0.80729', '0.23232', '-0.14189', '0.3161', '0.049262', '-0.33411', '-0.25296', '-0.019395', '-0.24234', '-0.59846', '0.76997', '0.87265', '-0.24425', '0.51755', '-0.20878', '-0.56027', '-0.21439', '-0.42029', '0.13491', '-0.028368', '-0.56969', '-0.1266', '0.38287', '-0.16767', '-0.58677', '-0.32237', '0.15846', '0.59556', '-1.0552', '0.79441', '0.17987', '-0.62691', '0.38177', '-0.066748', '0.16996', '-0.10173', '0.066299', '-0.55561', '-0.38713', '0.33772', '0.33874', '0.053886', '-0.10488', '0.56415', '-0.13275', '0.1262', '0.29306', '-0.31095', '-0.17854', '0.35285', '0.00013075', '0.51898', '1.2236', '-0.2643', '0.037547', '0.51511', '0.22394', '-0.25357', '-1.0723', '0.49631', '-0.27033', '0.11704', '-0.67869', '-0.22536', '0.13763', '0.1407', '0.44025', '0.043805', '-0.11425', '-0.17916', '-0.10361', '0.14973', '0.81742', '-0.0013219', '-0.14155', '0.19542', '0.62475', '-0.58424', '0.31022', '0.66416', '-0.33286', '-0.16716', '0.072762', '-0.49764', '-0.10817', '0.22071', '0.14323', '-0.23053', '-1.1992', '-0.28862', '0.21024', '-0.62443', '0.076061', '0.18231', '0.087195', '-0.34459', '0.02946', '0.67898', '-0.30063', '-0.174', '-0.89263', '-0.042857', '-0.30505', '0.66966', '-0.013918', '0.29442', '0.059769', '0.24655', '-0.29502', '-0.024101', '-0.43562', '-0.042243', '0.48316', '-0.24061', '-0.55936', '-0.25671', '-0.16496', '0.41056', '-0.011025', '0.3995', '-0.10955', '0.57864', '-0.095326', '-0.14995', '0.10487', '-0.50071', '-0.022227', '-0.3401', '-0.12177', '0.21538', '0.44572', '0.73496', '-0.1897', '0.0044095', '-0.84324', '-1.1227', '-0.13945', '-0.18635', '0.047539', '-0.02576', '0.94764', '0.22948', '-0.15774', '-0.25206', '-0.38703', '0.6866', '0.20996', '-0.15875', '0.0058876', '0.5742', '-0.67418', '-0.16063', '-0.075739', '0.42837', '0.20913', '0.16235', '-0.65112', '0.33446', '0.1024', '0.2308', '0.26889', '-0.53514', '0.51401', '-0.077549', '0.23051', '0.27226', '-0.096375', '0.0030776', '0.58233', '-0.46236', '-0.25057', '-0.074138', '-0.32048', '0.62223', '0.54458', '0.6217', '1.1464', '0.50221', '-0.90171', '0.31349', '-1.1352', '-0.19198', '-0.31481', '-0.12779', '-0.29484', '-0.22873', '-0.31661', '0.53138', '0.24974', '0.070768', '-0.65404', '-0.37577', '0.61155', '0.49496', '-0.052352', '0.53938', '0.005889', '0.21959', '0.10324', '-1.1018', '-0.14499', '0.25521', '0.24265', '0.31371', '0.33447', '-0.37324', '-0.52313', '0.20971', '-0.13097', '-0.16648', '-0.049781', '0.2748', '-0.92401', '0.75744', '0.48051', '-0.44793', '-0.42594', '0.34599', '-0.53192', '-0.13063', '0.096985', '-0.042562', '0.52247', '0.74385', '-0.68267', '-0.19708', '-0.0070701', '-0.02074', '-0.58448', '-0.065581', '1.3816', '-0.9249', '0.033565', '-0.16956', '0.83749', '-0.40574', '-0.50804', '-1.0029', '-0.14006', '-0.1387', '-0.58868', '-0.2428', '-0.022855', '-0.34926', '0.053938', '-0.60598', '-0.031057', '0.7892', '-0.75688', '0.12333', '-0.094554', '0.042073', '-0.30965', '0.24605', '0.12449', '-0.48643', '0.23043']\n",
      "[!] l = ['is', 'name@domain.com', '-0.1197', '0.10706', '-0.10519', '-0.12412', '0.4096', '-0.0287', '0.34704', '0.3549', '-0.24818', '-1.2408', '0.080013', '0.72535', '0.86863', '-0.044256', '0.91981', '0.7825', '1.1884', '-1.7801', '0.95549', '0.18023', '-0.47942', '0.6672', '0.06455', '-0.89756', '0.50649', '0.45807', '-0.4137', '0.29551', '-0.72757', '0.35583', '0.21081', '0.69184', '0.17948', '-0.38435', '-0.82156', '0.34316', '-0.2292', '-0.1174', '0.93835', '-0.24956', '-0.21212', '-1.2493', '0.34025', '-0.60752', '0.15362', '-0.38889', '0.27859', '-1.0553', '-0.10272', '0.71688', '0.46476', '-0.24386', '0.60731', '-0.14626', '0.49906', '-0.031597', '-1.3152', '-0.37701', '-0.15905', '0.81663', '-0.28852', '-0.028202', '0.2644', '0.36998', '-0.54607', '-0.26863', '-0.10398', '-0.16196', '-0.71419', '0.61126', '-0.81195', '-0.15081', '0.49439', '0.31647', '0.74124', '-0.37209', '0.68943', '0.19744', '0.1258', '-0.22018', '-0.96408', '-0.46702', '-0.13894', '0.30874', '0.50665', '0.17943', '0.71153', '-0.00026381', '-0.47585', '-0.86687', '-0.63838', '0.3674', '-0.027812', '-0.50548', '-0.0050586', '1.1909', '-0.2514', '0.422', '0.0074521', '0.78066', '0.13396', '0.0063949', '0.54671', '0.21331', '-0.063979', '-0.71203', '-0.34413', '-0.10592', '-1.664', '0.059908', '0.035519', '0.74303', '-0.71156', '-0.083368', '0.046354', '-0.27944', '-0.22258', '-0.24948', '0.76799', '0.012705', '-0.55539', '0.064894', '0.14826', '0.41538', '0.27372', '-0.059421', '-0.076653', '0.06399', '0.083734', '0.20121', '0.17862', '-0.30495', '0.2237', '0.51722', '-0.091633', '0.17208', '0.12375', '0.67001', '-0.24705', '-0.022074', '0.60573', '-0.093634', '0.18453', '0.31808', '0.088848', '0.40591', '0.13941', '-1.114', '-0.73248', '1.0375', '-0.36915', '1.116', '0.5766', '0.16222', '0.34327', '0.46721', '0.30673', '-0.413', '0.041562', '0.090004', '0.93973', '0.18829', '0.395', '0.10011', '1.1547', '-0.057363', '0.40281', '0.77703', '-0.4892', '-0.73961', '0.019956', '0.66545', '0.28972', '-0.023978', '0.45869', '0.43236', '0.64363', '0.18573', '-0.78063', '0.050466', '-0.091639', '0.40486', '0.12737', '0.83952', '1.0197', '0.13825', '0.6778', '-0.45252', '-0.43476', '0.69962', '0.43987', '0.044541', '1.1151', '0.29316', '0.55698', '0.29285', '0.51472', '0.12164', '-0.17263', '0.68153', '-0.15713', '-0.0082957', '0.052173', '0.22946', '-0.40682', '-0.057423', '0.34515', '-0.2588', '-0.71888', '0.27499', '0.56278', '0.59652', '-0.82314', '0.53207', '-0.56464', '0.35201', '-0.81956', '-0.45959', '-0.11181', '0.002767', '0.20839', '-0.11993', '-0.16981', '0.34309', '-0.53543', '0.12176', '0.15777', '-0.22412', '0.10459', '-0.55587', '0.36915', '0.014192', '-0.61869', '-0.34567', '-0.60151', '-0.1234', '-0.59543', '-0.62935', '-0.35452', '0.29317', '0.092327', '-0.044436', '-0.48798', '0.9649', '0.45625', '-0.0059057', '-0.43416', '-0.71245', '0.22928', '-0.1943', '-0.058306', '0.016294', '-0.67552', '0.49185', '0.72063', '-0.58111', '0.43908', '-0.052694', '-0.27258', '-0.14604', '-0.70854', '-0.26019', '-0.077697', '-1.2106', '0.23728', '0.42626', '-1.0457', '0.17442', '-0.62781', '0.80538', '0.079948', '0.20073', '0.2751', '0.292', '-0.07767', '0.12779', '0.37107', '-0.0064965', '0.037248', '-0.39539', '0.050184', '0.20656', '0.38481', '-0.87068', '0.56231', '-0.33836', '0.14319', '0.25696', '0.36331', '-0.38799', '0.58948', '-0.33161', '-0.071877', '0.40965', '0.13535', '-0.030141', '1.0225', '0.082464', '-0.95272', '0.0059679']\n",
      "(30524, 300) 7590\n",
      "[!] l = ['2000000', '300']\n",
      "[!] l = ['-0.1719', '0.1353', '-0.0702', '-0.0061', '0.0124', '-0.1084', '0.1182', '0.0007', '0.1400', '0.3481', '-0.2844', '0.4066', '0.1285', '0.3661', '-0.2915', '0.1992', '0.2382', '0.4510', '0.1252', '-0.1517', '0.1235', '-0.2814', '0.0071', '0.0101', '0.3132', '-0.1255', '-0.2274', '0.0547', '-0.1048', '0.3544', '0.0547', '-0.1371', '-0.0308', '0.2008', '-0.2784', '0.1385', '-0.2677', '-0.2009', '-0.4081', '0.0190', '-0.3697', '0.0359', '0.2690', '-0.3849', '-0.1344', '0.7400', '0.1596', '-0.1473', '-0.2437', '-0.2382', '0.2497', '0.3910', '0.2399', '0.1478', '-0.3188', '0.0255', '0.1963', '0.3341', '0.1248', '-0.3353', '-0.0136', '0.1313', '-0.4232', '0.1095', '0.2349', '-0.4334', '0.1511', '0.1147', '-0.1562', '-0.0087', '-0.4865', '-0.1269', '0.2745', '-0.1389', '0.0228', '0.2093', '-0.6707', '0.1760', '-0.1793', '0.2222', '0.3460', '-0.0819', '-0.0911', '0.4244', '-0.1281', '0.2235', '0.1600', '-0.0080', '0.0847', '0.0604', '0.0869', '0.1827', '-0.1394', '0.0966', '-0.0540', '-0.2140', '0.0945', '0.0700', '0.2677', '-0.0385', '-0.1862', '-0.1243', '0.2504', '-0.1343', '0.0292', '0.5339', '0.5051', '0.2195', '0.2637', '0.1161', '-0.0664', '-0.2540', '-0.0418', '-0.2007', '-0.5083', '-0.1808', '0.0838', '-0.0287', '-0.0859', '-0.0223', '-0.3362', '0.1027', '0.0136', '-0.3003', '-0.4304', '0.1853', '-0.1057', '-0.2565', '0.3902', '-0.3047', '-0.3069', '-0.2962', '0.1447', '-0.0288', '-0.1311', '-0.2125', '-0.1852', '-0.5233', '0.0148', '-0.0480', '-0.1166', '0.2514', '-0.0298', '-0.0881', '0.4078', '0.0418', '0.4199', '-0.1092', '-0.3399', '0.2739', '0.0580', '-0.0006', '0.6980', '0.2025', '0.2810', '-0.0691', '-0.1774', '0.3469', '0.0494', '-0.4978', '0.5677', '-0.1139', '0.0582', '-0.0396', '0.3258', '0.3306', '-0.3478', '0.0978', '-0.2901', '-0.5825', '0.1458', '-0.0699', '-0.2144', '-0.2849', '0.4078', '0.0802', '0.0608', '-0.0590', '0.3806', '0.3222', '-0.2041', '-0.0321', '0.1712', '0.2576', '0.5507', '-0.2113', '0.1023', '0.3621', '-0.0473', '0.3260', '0.0286', '0.2153', '0.1485', '-0.2751', '-0.0431', '0.2436', '-0.2552', '-0.1042', '-0.0702', '-0.1285', '-0.0557', '-0.3145', '0.3336', '0.1741', '-0.0510', '-0.1379', '-0.1743', '-0.1541', '0.0492', '0.0775', '0.1592', '0.4474', '0.3099', '-0.1358', '-0.3815', '-0.2954', '0.5116', '0.2457', '-0.3090', '-0.2697', '0.1034', '-0.2474', '0.3088', '0.1473', '0.0045', '-0.2367', '0.0797', '-0.2729', '0.0339', '-0.1408', '0.0117', '0.0619', '-0.0238', '-0.2428', '0.3474', '-0.1013', '-0.0181', '0.1732', '0.4031', '-0.1430', '-0.3106', '-0.2186', '0.2317', '0.2827', '0.1637', '0.2585', '-0.7041', '0.4018', '-0.0447', '0.0592', '-0.5611', '-0.1979', '0.3260', '0.0425', '-0.2991', '-0.8151', '-0.3542', '0.0539', '0.1446', '-0.8573', '0.1219', '-0.0628', '-0.0219', '0.4989', '0.2603', '-0.0526', '-0.1152', '-0.0531', '-0.0534', '0.5803', '-0.1244', '-0.0423', '-0.0949', '-0.0346', '-0.0832', '-0.2728', '0.0550', '-0.1697', '0.0787', '0.2406', '0.1613', '-0.6509', '0.2913', '-0.1584', '-0.1393', '0.3071', '-0.3445', '0.0881', '-0.1149', '-0.1062', '0.3906', '-0.3820', '0.4580', '-0.2770', '-0.1978', '-0.2113', '-0.1867', '0.0087', '-0.0968', '-0.1249']\n",
      "[!] l = ['0.2023', '0.2547', '0.2360', '-0.0980', '0.0057', '0.7237', '-0.0495', '-0.0644', '0.0754', '0.3751', '-0.1167', '0.5211', '-0.1637', '0.3809', '0.1243', '0.3141', '0.0084', '0.2649', '-0.0847', '-0.5148', '0.0638', '-0.0218', '0.0086', '0.5922', '-0.2671', '-0.1646', '-0.3918', '-0.6026', '0.3479', '0.3717', '-0.1667', '-0.2700', '-0.1357', '0.1731', '-0.2286', '-0.3138', '-0.0103', '0.0527', '-0.3687', '0.0112', '0.3249', '0.1572', '-0.0587', '-0.1183', '0.0702', '-0.0093', '-0.1183', '-0.3399', '-0.2849', '-0.3852', '0.1505', '0.3940', '0.3699', '0.1003', '0.7097', '0.7259', '-0.0724', '-0.0707', '0.1292', '-0.3996', '0.1341', '0.5045', '-0.1698', '-0.2544', '0.2251', '-0.0017', '0.3526', '0.3328', '-0.2364', '-0.3315', '-0.0603', '0.0930', '0.1699', '0.0211', '0.2008', '0.0685', '-0.5494', '0.3394', '-0.3623', '-0.0821', '0.4128', '0.0520', '0.1576', '-0.0766', '-0.0994', '-0.0723', '-0.2339', '0.1590', '0.5547', '0.0490', '-0.1142', '0.3948', '0.1430', '-0.4227', '0.1644', '-0.0715', '0.0927', '-0.0760', '0.1675', '0.3599', '-0.2436', '-0.2767', '0.1212', '-0.3653', '0.1734', '0.3209', '0.0217', '0.2873', '0.2032', '-0.1197', '-0.2826', '-0.0811', '-0.2230', '0.0401', '-0.2902', '-0.1430', '0.0084', '0.1849', '0.0146', '0.3055', '-0.2915', '0.1156', '-0.3181', '-0.2521', '-0.0452', '-0.2769', '0.1169', '-0.1409', '0.3093', '-0.5767', '0.0388', '-0.4850', '-0.3316', '0.0031', '-0.0808', '-0.0055', '0.2949', '-0.4228', '0.0348', '-0.2100', '0.3311', '-0.3207', '0.0998', '0.0643', '-0.0182', '0.2381', '0.1863', '-0.0672', '-0.3160', '0.2933', '0.0123', '0.5600', '0.1260', '-0.0000', '0.0421', '0.0813', '-0.2514', '-0.3611', '0.1889', '-0.0151', '0.5521', '-0.4126', '0.1371', '-0.2694', '0.0924', '-0.0864', '-0.1675', '0.0627', '-0.2427', '-0.0753', '0.6647', '-0.2587', '-0.0800', '-0.1346', '0.4510', '-0.0972', '-0.0411', '0.0786', '0.3986', '0.2210', '-0.0349', '0.0382', '-0.0632', '-0.0939', '0.1544', '0.1928', '0.0200', '0.2541', '0.2164', '-0.1832', '0.1970', '0.1452', '-0.0554', '-0.5973', '0.2297', '0.0879', '0.1012', '-0.0988', '-0.1062', '0.2604', '-0.1816', '-0.2495', '0.2133', '0.0152', '0.7617', '0.0735', '-0.1472', '-0.2172', '0.3883', '0.1352', '-0.1718', '0.4585', '-0.1877', '-0.1471', '-0.2459', '-0.4096', '0.6962', '0.2451', '0.1270', '-0.2364', '0.2189', '0.0700', '0.2289', '-0.1944', '0.4503', '-0.0756', '0.2423', '0.1738', '-0.1958', '-0.1540', '-0.0803', '0.1399', '-0.2565', '-0.2021', '0.0972', '-0.0500', '0.2378', '0.0111', '0.1183', '0.1648', '0.0820', '-0.2641', '0.2641', '0.3131', '0.4181', '0.1943', '-0.3228', '0.3453', '-0.0184', '-0.1149', '-0.6676', '-0.0966', '0.0023', '-0.2861', '-0.4386', '-0.4881', '0.1766', '-0.1140', '-0.0715', '-0.0129', '-0.1575', '-0.5017', '0.3772', '0.3417', '0.2593', '-0.3074', '-0.0276', '0.2412', '-0.2458', '0.6663', '0.2414', '0.1962', '0.5164', '0.0778', '0.0218', '-0.7076', '-0.2004', '0.3330', '0.2768', '0.2252', '0.0770', '-0.7459', '0.1708', '-0.2123', '0.0193', '0.3864', '0.1240', '-0.0064', '-0.0656', '-0.1997', '-0.1648', '-0.3714', '0.2487', '0.0425', '-0.2300', '-0.0921', '0.0028', '0.1076', '-0.1556', '0.2220']\n",
      "[!] l = ['0.1959', '0.4681', '-0.0898', '0.1395', '0.0368', '-0.0521', '-0.2757', '-0.1348', '0.3389', '-0.2790', '-0.3436', '0.4771', '-0.6354', '0.4563', '-0.2390', '0.0531', '-0.3057', '0.2387', '0.3753', '0.1547', '0.2020', '-0.2606', '-0.2416', '0.0852', '-0.1519', '-0.0930', '-0.0313', '-0.5858', '-0.0957', '0.6781', '0.2491', '-0.1908', '-0.2673', '-0.3006', '-0.0296', '0.3552', '0.0842', '0.0784', '-0.2394', '-0.0561', '-0.1431', '0.3574', '0.0525', '-0.0902', '0.0761', '0.2041', '-0.0203', '-0.0266', '-0.2326', '-0.4698', '0.1878', '1.1512', '0.5173', '0.1078', '-0.0455', '-0.0040', '-0.4089', '-0.4962', '0.2302', '-0.1774', '0.5426', '0.5017', '0.1334', '-0.3219', '0.1741', '-0.2620', '0.2522', '0.4148', '-0.4559', '-0.4506', '0.4103', '0.2734', '0.3035', '0.2161', '0.1617', '0.3178', '-0.6294', '-0.0412', '-0.5294', '-0.0422', '0.3857', '-0.2334', '0.0534', '0.2881', '-0.2133', '-0.1559', '-0.1652', '-0.1086', '0.3309', '0.1979', '0.2644', '0.3964', '0.1807', '-0.1877', '0.2426', '-0.5061', '-0.1385', '0.1566', '-0.0005', '0.3585', '0.0407', '-0.1224', '0.4508', '-0.2485', '0.5176', '-0.2724', '-0.2231', '-0.4197', '-0.0592', '-0.4352', '-0.3172', '0.0381', '0.0635', '-0.0155', '-0.5750', '0.0247', '-0.3787', '-0.4257', '-0.0136', '0.4659', '-0.5113', '0.0892', '-0.2407', '0.1822', '-0.1814', '-0.1794', '-0.1982', '-0.2240', '0.2677', '0.1907', '0.2730', '0.1968', '-0.0026', '0.1164', '-0.1207', '0.1480', '0.3139', '-0.7141', '-0.2650', '-0.3755', '-0.0664', '0.2361', '-0.3757', '-0.2650', '0.0553', '0.2049', '0.1113', '0.3435', '-0.2122', '0.3529', '-0.1450', '0.2576', '-0.0435', '0.3409', '-0.0886', '0.0713', '0.0972', '0.3073', '0.0052', '-0.0350', '0.5553', '-0.3395', '0.3005', '-0.2564', '0.1282', '-0.1233', '0.0864', '0.0532', '-0.0912', '0.4158', '0.1340', '-0.5189', '-0.1494', '-0.1143', '0.1949', '0.0281', '-0.1054', '0.0235', '-0.2407', '0.1504', '0.0359', '-0.1324', '0.2873', '-0.1654', '0.2846', '-0.0239', '0.0956', '0.4427', '0.0351', '-0.3290', '0.1229', '0.0971', '-0.4705', '-0.1089', '-0.1225', '0.0336', '0.1450', '-0.0585', '-0.0285', '0.3675', '0.0188', '-0.0999', '-0.3789', '0.3999', '0.4178', '-0.3243', '0.4023', '0.1650', '0.1163', '0.0128', '-0.2123', '0.5136', '0.1220', '-0.3908', '0.0712', '-0.3973', '-0.2234', '0.1276', '-0.0501', '-0.4909', '0.1688', '0.4300', '0.1382', '-0.3020', '0.6986', '-0.4683', '-0.2221', '0.0219', '-0.3579', '-0.0814', '0.1740', '0.2505', '-0.3986', '-0.1979', '-0.0426', '-0.2733', '0.1265', '0.3682', '-0.1720', '0.0434', '0.3203', '-0.4085', '0.2832', '0.3511', '0.7432', '0.0560', '-0.1180', '0.2796', '0.1884', '0.0474', '-0.4213', '-0.2813', '0.2715', '-0.0940', '-0.5208', '-0.3986', '0.1935', '-0.0809', '0.2068', '-0.4435', '-0.0135', '0.3210', '-0.2169', '0.1150', '0.1524', '0.1753', '-0.3609', '0.2366', '-0.6096', '0.0936', '0.0574', '0.4491', '-0.1573', '-0.1743', '-0.0282', '-0.3661', '-0.1571', '0.4385', '0.4696', '0.0426', '-0.0821', '-0.6876', '0.5222', '0.2479', '-0.0025', '0.7056', '0.2150', '-0.0411', '-0.1315', '-0.1075', '-0.1793', '-0.0054', '0.2754', '0.4054', '-0.4161', '-0.2829', '0.2808', '0.1951', '-0.2531', '-0.1846']\n",
      "[!] l = ['0.2543', '0.4483', '-0.1112', '0.1583', '-0.1247', '-0.2175', '-0.3596', '-0.1341', '0.4602', '-0.4099', '-0.2639', '0.3844', '-0.5983', '-0.0997', '-0.8078', '0.0695', '-0.3853', '0.1381', '0.4510', '0.0286', '0.1457', '0.0936', '0.0441', '0.3831', '-0.2349', '-0.2709', '0.1554', '0.0176', '-0.0110', '0.5524', '0.1872', '-0.4065', '-0.1820', '-0.1140', '0.0225', '0.3604', '-0.1606', '0.1053', '-0.3655', '-0.1757', '0.0528', '0.4496', '-0.0517', '0.0238', '-0.0305', '0.3656', '-0.1293', '0.1275', '-0.0416', '-0.6108', '-0.1277', '0.7571', '0.4596', '0.1214', '0.5902', '0.1276', '-0.1857', '-0.5554', '0.0913', '-0.2583', '0.3688', '0.4592', '0.4020', '-0.1763', '-0.0716', '-0.3019', '0.1424', '-0.1608', '-0.3212', '-0.3660', '0.3222', '-0.2480', '0.3696', '0.2063', '0.1371', '0.1760', '-0.5538', '0.1184', '-0.5363', '-0.3821', '0.6066', '-0.3727', '-0.2802', '0.1237', '0.0389', '0.0098', '-0.6080', '-0.0957', '0.3721', '0.1112', '0.4214', '0.3918', '-0.1187', '0.2297', '0.2643', '-0.4890', '0.1166', '0.1523', '0.0352', '0.2742', '-0.1821', '0.4932', '0.1607', '-0.2528', '0.3795', '0.0290', '0.1743', '-0.2689', '0.4597', '-0.5003', '-0.1325', '0.3412', '0.1550', '0.1067', '-0.3350', '0.0158', '-0.3353', '0.1131', '0.2471', '0.2073', '-0.5906', '0.2186', '-0.0853', '0.1699', '-0.2490', '-0.1062', '-0.3073', '-0.0977', '0.3708', '0.2420', '0.2019', '-0.0114', '-0.0602', '0.0886', '-0.0764', '0.0130', '0.0545', '-0.7139', '-0.3839', '-0.5169', '0.1057', '-0.0182', '-0.1789', '-0.3279', '-0.2430', '0.3445', '0.3019', '0.5106', '-0.5064', '0.2240', '-0.1257', '0.1748', '0.1003', '0.0871', '-0.0298', '0.4011', '0.1520', '0.0157', '-0.1172', '-0.0124', '0.6172', '-0.4076', '0.0824', '-0.1742', '0.0782', '0.0467', '0.2877', '0.2103', '-0.1771', '0.2171', '0.2344', '-0.3215', '-0.0185', '-0.1359', '0.3107', '-0.0089', '0.5495', '0.1813', '-0.0693', '0.2414', '-0.0325', '-0.0981', '-0.3347', '-0.0851', '-0.2445', '0.3516', '0.0616', '0.1856', '0.2978', '-0.2999', '0.2762', '0.0891', '-0.3121', '-0.0016', '-0.0253', '-0.0740', '0.3197', '0.0370', '0.2501', '-0.3010', '-0.2624', '0.3668', '-0.4848', '0.2060', '0.5598', '-0.3341', '0.1654', '-0.0125', '0.5674', '0.1630', '-0.1030', '0.4477', '0.2941', '-0.4378', '-0.7234', '-0.2041', '-0.3623', '0.3428', '-0.3080', '-0.3439', '0.0879', '-0.0163', '-0.2777', '0.0882', '0.4097', '-0.1283', '0.1701', '0.2879', '-0.3589', '-0.3298', '0.0185', '0.0754', '-0.3995', '-0.0457', '-0.1269', '-0.3556', '0.1683', '0.1575', '-0.0890', '0.1167', '0.2390', '-0.4903', '0.3559', '0.4177', '0.7981', '0.0661', '-0.1963', '-0.1375', '0.3497', '0.3611', '-0.5641', '-0.0724', '0.1582', '0.2362', '-0.3535', '0.3363', '0.0124', '0.0567', '0.0507', '-0.5038', '-0.2451', '0.3121', '-0.0645', '-0.2322', '0.4465', '-0.0280', '-0.1663', '0.0839', '-0.5238', '0.0933', '-0.0708', '0.6057', '0.0018', '-0.0278', '0.2326', '-0.2422', '-0.0547', '0.0863', '0.6219', '0.1606', '-0.4335', '-0.7901', '0.3604', '0.1695', '-0.1965', '0.2233', '0.0606', '0.3000', '-0.2922', '-0.1028', '-0.0856', '0.0393', '0.2901', '0.3864', '-0.4113', '-0.2557', '0.3777', '0.2275', '-0.3067', '-0.0277']\n",
      "[!] l = ['0.1722', '0.3303', '0.0884', '0.0690', '0.0593', '0.5049', '-0.2000', '0.0021', '0.3161', '0.0010', '-0.4286', '0.4398', '-0.4477', '0.0983', '0.0685', '0.2936', '0.1942', '0.2229', '-0.0491', '-0.5384', '0.1734', '0.2547', '-0.2717', '0.3069', '-0.1095', '-0.3110', '-0.1451', '-0.6982', '0.4101', '0.5030', '0.0048', '-0.1489', '-0.0575', '0.2157', '-0.1477', '-0.3315', '0.1559', '-0.0384', '-0.0684', '0.0236', '0.1706', '0.3390', '-0.0307', '-0.1611', '0.0599', '-0.1668', '-0.2103', '-0.3675', '-0.0295', '-0.4120', '-0.0897', '0.7601', '0.5428', '0.3032', '0.6005', '0.3735', '-0.2680', '-0.4238', '0.1618', '-0.1901', '0.3673', '0.3177', '-0.0340', '-0.2821', '0.4473', '0.0277', '0.3332', '0.5492', '-0.4362', '-0.2329', '0.2712', '0.0617', '0.3965', '0.0006', '0.0490', '0.2178', '-0.4852', '0.1634', '-0.3227', '-0.0804', '0.4806', '-0.1958', '0.1519', '-0.1290', '-0.0442', '-0.1155', '0.0435', '-0.1328', '0.5647', '-0.1128', '-0.3044', '0.3361', '0.4117', '-0.3320', '0.3407', '-0.3251', '0.0654', '0.0724', '0.1787', '0.1685', '0.0493', '-0.3367', '0.3526', '-0.4441', '0.3207', '0.1973', '0.0047', '0.2405', '0.0567', '-0.0501', '-0.4651', '-0.1605', '-0.1785', '0.1951', '-0.6842', '0.1473', '-0.1218', '0.3631', '-0.0024', '-0.1230', '-0.5965', '-0.0025', '-0.2918', '0.0672', '-0.0049', '-0.3759', '0.0899', '-0.3661', '0.6573', '-0.7171', '-0.0056', '-0.4204', '-0.3296', '0.2833', '-0.1019', '-0.3192', '0.5515', '-0.3120', '0.1166', '-0.5688', '0.2559', '-0.0860', '0.0813', '-0.4409', '0.0119', '0.2686', '0.2703', '-0.1094', '-0.3401', '0.6120', '-0.0265', '0.2811', '0.1365', '-0.1566', '0.1500', '-0.1006', '0.1437', '-0.0197', '0.0863', '-0.3833', '0.6791', '-0.2905', '0.2919', '-0.1771', '0.2636', '0.0242', '-0.1351', '0.3510', '-0.3331', '-0.0053', '0.5959', '-0.4195', '-0.0777', '-0.2553', '0.2824', '0.0683', '-0.2300', '-0.0731', '0.2417', '0.0933', '-0.1759', '0.1305', '-0.1250', '0.0639', '0.0967', '0.1881', '-0.0129', '0.7816', '0.2350', '-0.4890', '0.3040', '0.2831', '-0.1739', '-0.5662', '0.1252', '0.1484', '0.0770', '0.1883', '-0.0873', '0.1709', '-0.1746', '-0.2340', '0.1783', '0.2168', '0.5059', '-0.2562', '0.0417', '0.0385', '0.3323', '0.2614', '-0.2141', '0.3374', '0.1573', '-0.1632', '-0.0699', '-0.5005', '0.5387', '-0.0725', '0.2164', '-0.5496', '-0.1748', '-0.1781', '0.2483', '-0.1980', '0.3574', '-0.0702', '0.2235', '0.2291', '0.0441', '-0.2024', '-0.0927', '0.1400', '-0.5392', '-0.0340', '0.0473', '-0.5277', '0.2719', '0.1793', '0.1272', '0.1670', '-0.0073', '-0.2507', '0.4077', '0.1473', '0.9193', '0.3309', '-0.2463', '0.0223', '0.0850', '-0.1112', '-0.3647', '-0.3446', '-0.0110', '-0.3917', '-0.5217', '-0.2569', '0.0195', '-0.0940', '-0.1632', '-0.2262', '-0.1475', '-0.3885', '0.5179', '-0.0090', '0.0721', '-0.1322', '-0.1739', '0.4088', '-0.3001', '0.3900', '0.2971', '0.3261', '0.3451', '-0.0452', '0.2547', '-0.7378', '-0.1534', '0.5058', '0.5123', '0.4525', '0.1056', '-1.0024', '0.2197', '-0.3044', '-0.0866', '0.2512', '0.0451', '0.0375', '-0.1781', '-0.2460', '-0.3126', '-0.2026', '0.4267', '0.1973', '-0.3909', '-0.2041', '-0.2645', '0.0797', '-0.2059', '-0.1893']\n",
      "(30524, 300) 7218\n"
     ]
    }
   ],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype=np.float16)\n",
    "\n",
    "\n",
    "def load_embed(path, dim=300, word_index=None):\n",
    "    embedding_index = {}\n",
    "    with open(path, mode=\"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            l = l.strip().split()\n",
    "            word, arr = l[0], l[1:]\n",
    "            if len(arr) != dim:\n",
    "                print(\"[!] l = {}\".format(l))\n",
    "                continue\n",
    "            if word_index and word not in word_index:\n",
    "                continue\n",
    "            word, arr = get_coefs(word, arr)\n",
    "            embedding_index[word] = arr\n",
    "    return embedding_index\n",
    "\n",
    "\n",
    "def build_matrix(path, word_index=None, max_features=None, dim=300):\n",
    "    embedding_index = load_embed(path, dim=dim, word_index=word_index)\n",
    "    max_features = len(word_index) + 1 if max_features is None else max_features \n",
    "    embedding_matrix = np.zeros((max_features + 1, dim))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i <= max_features:\n",
    "            try:\n",
    "                embedding_matrix[i] = embedding_index[word]\n",
    "            except KeyError:\n",
    "#                 print(word)\n",
    "                unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def load_word_embed(word_embed_glove=\"../data/glove.840B.300d.txt\", \n",
    "                    word_embed_crawl=\"../data/crawl-300d-2M.vec\",\n",
    "               save_filename=\"./word_embedding_matrix\",\n",
    "               word_index=None):\n",
    "\n",
    "    #(30524, 300) 7590\n",
    "    #(30524, 300) 7218\n",
    " \n",
    "    if os.path.exists(save_filename + \".npy\"):\n",
    "        word_embedding_matrix = np.load(save_filename + \".npy\").astype(\"float32\")\n",
    "    else:\n",
    "        word_embedding_matrix, tx_unk = build_matrix(word_embed_glove, word_index=word_index, dim=300)\n",
    "\n",
    "        print(word_embedding_matrix.shape, len(tx_unk))\n",
    "        \n",
    "        word_embedding_matrix_v2, tx_unk = build_matrix(word_embed_crawl, word_index=word_index, dim=300)\n",
    "\n",
    "        print(word_embedding_matrix_v2.shape, len(tx_unk))\n",
    "        \n",
    "        word_embedding_matrix = np.concatenate([word_embedding_matrix, word_embedding_matrix_v2], axis=1)\n",
    "        \n",
    "        gc.collect()\n",
    "        np.save(save_filename, word_embedding_matrix)\n",
    "    return word_embedding_matrix\n",
    "\n",
    "\n",
    "word_embedding_matrix = load_word_embed(word_index=word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(cfg, summary=False, word_embedding_matrix=None):\n",
    "    \n",
    "    def _get_model(base_dir, cfg_=None):\n",
    "        config_file = os.path.join(base_dir, 'bert_config.json')\n",
    "        checkpoint_file = os.path.join(base_dir, 'bert_model.ckpt')\n",
    "        if not os.path.exists(config_file):\n",
    "            config_file = os.path.join(base_dir, 'bert_config_large.json')\n",
    "            checkpoint_file = os.path.join(base_dir, 'roberta_l24_large_model')\n",
    "        print(config_file, checkpoint_file)\n",
    "        model = load_trained_model_from_checkpoint(config_file, \n",
    "                                           checkpoint_file, \n",
    "                                           training=False, \n",
    "                                           trainable=cfg_[\"bert_trainable\"], \n",
    "                                           output_layer_num=cfg[\"cls_num\"],\n",
    "                                           seq_len=cfg_['maxlen'])\n",
    "        return model\n",
    "    \n",
    "    def get_opt(num_example, warmup_proportion=0.1, lr=2e-5, min_lr=None):\n",
    "        if cfg[\"opt\"].lower() == \"nadam\":\n",
    "            opt = Nadam(lr=lr)\n",
    "        else:\n",
    "            total_steps, warmup_steps = calc_train_steps(\n",
    "                num_example=num_example,\n",
    "                batch_size=B_SIZE,\n",
    "                epochs=MAX_EPOCH,\n",
    "                warmup_proportion=warmup_proportion,\n",
    "            )\n",
    "\n",
    "            opt = AdamWarmup(total_steps, warmup_steps, lr=lr, min_lr=min_lr)\n",
    "\n",
    "        return opt\n",
    "\n",
    "    model1 = _get_model(cfg[\"base_dir\"], cfg)\n",
    "    model1 = Model(inputs=model1.inputs[: 2], outputs=model1.layers[-7].output)\n",
    "\n",
    "    if word_embedding_matrix is not None:\n",
    "        embed_layer = Embedding(input_dim=word_embedding_matrix.shape[0], \n",
    "                                output_dim=word_embedding_matrix.shape[1],\n",
    "                                weights=[word_embedding_matrix],\n",
    "                                trainable=cfg[\"trainable\"],\n",
    "                                name=\"embed_layer\"\n",
    "                         )\n",
    "        \n",
    "    inp_token1 = Input(shape=(None, ), dtype=np.int32, name=\"query_token_input\")\n",
    "    inp_segm1 = Input(shape=(None, ), dtype=np.float32, name=\"query_segm_input\")\n",
    "    inp_image = Input(shape=(None, 2048), dtype=np.float32, name=\"image_input\")\n",
    "    inp_image_mask = Input(shape=(None, ), dtype=np.float32, name=\"image_mask_input\")\n",
    "    inp_pos = Input(shape=(None, 5), dtype=np.float32, name=\"image_pos_input\")        \n",
    "    inp_image_char = Input(shape=(None, cfg[\"max_char\"]), dtype=np.int32, name='image_char_input')\n",
    "    \n",
    "    \n",
    "    mask = Lambda(lambda x: K.cast(K.not_equal(x, cfg[\"x_pad\"]), 'float32'), name=\"token_mask\")(inp_token1)\n",
    "    word_embed = embed_layer(inp_token1)\n",
    "    word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "    word_embed = Bidirectional(LSTM(cfg[\"unit1_1\"], return_sequences=True), merge_mode=\"sum\")(word_embed)\n",
    "    word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "\n",
    "    sequence_output = model1([inp_token1, inp_segm1])\n",
    "    sequence_output = Concatenate(axis=-1)([sequence_output, word_embed])\n",
    "    text_pool = Lambda(lambda x: x[:, 0, :])(sequence_output)\n",
    "\n",
    "    character_embedding_layer = TimeDistributed(Sequential([\n",
    "        embed_layer,\n",
    "        # 임베딩\n",
    "        Conv1D(filters=128, kernel_size=3, name=\"char_embed_conv1d\"),\n",
    "        GlobalMaxPooling1D()\n",
    "    ]), name='CharEmbedding')\n",
    "    character_embedding_layer.build(input_shape=(None, None, cfg[\"max_char\"]))\n",
    "    image_char_embed  = character_embedding_layer(inp_image_char)    \n",
    "    image_embed = Concatenate(axis=-1)([image_char_embed, inp_image])    \n",
    "    image_embed = Dense(512, activation='relu', name='image_embed')(image_embed)\n",
    "    image_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([image_embed, inp_image_mask])\n",
    "    pos_embed = Dense(512, activation='relu', name='pos_embed')(inp_pos)\n",
    "    pos_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([pos_embed, inp_image_mask])\n",
    "    embed = Add()([image_embed , pos_embed]) # batch, maxlen(10), 1024+128\n",
    "    \n",
    "    image_embed = Bidirectional(LSTM(1152, return_sequences=True), merge_mode=\"sum\")(embed)\n",
    "    image_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([image_embed, inp_image_mask])\n",
    "    \n",
    "    image_pool = Lambda(lambda x: x[:, 0, :])(image_embed)\n",
    "    \n",
    "    pool = Concatenate(axis=-1)([image_pool, text_pool])\n",
    "    pool = Dense(2048, activation=\"relu\")(pool)\n",
    "    pool = Dense(512, activation=\"relu\")(pool)\n",
    "    pool = Dense(128, activation=\"relu\")(pool)\n",
    "    \n",
    "    output = Dense(2, activation='softmax', name='output')(pool)\n",
    "\n",
    "    opt = get_opt(num_example=cfg[\"num_example\"], lr=cfg[\"lr\"], min_lr=cfg['min_lr'])\n",
    "    model = Model(inputs=[inp_token1, inp_segm1, \n",
    "                          inp_image, inp_image_mask,\n",
    "                          inp_pos, inp_image_char], outputs=[output])#\n",
    "    \n",
    "    model.compile(optimizer=opt, loss={\n",
    "                'output': 'sparse_categorical_crossentropy'\n",
    "            }, metrics=['accuracy'])\n",
    "    if summary:\n",
    "        model.summary()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "#이미지&쿼리 concat\n",
    "\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            print(len(tokens_a))\n",
    "            tokens_b.pop()\n",
    "\n",
    "\n",
    "def token2id_X(X, x_dict, maxlen=None):\n",
    "    x = tokenizer.tokenize(X)\n",
    "    if maxlen:\n",
    "        x = x[: 1] + list(x)[1: maxlen - 1] + x[-1: ]     \n",
    "    seg = [0 for _ in x]\n",
    "    token = list(x)\n",
    "    x = [x_dict[e] if e in x_dict else x_dict[\"[UNK]\"] for e in token]\n",
    "    assert len(x) == len(seg)\n",
    "    return x, seg\n",
    "\n",
    "\n",
    "def seq_padding(X, maxlen=None, padding_value=None, debug=False):\n",
    "    L = [len(x) for x in X]\n",
    "    if maxlen is None:\n",
    "        maxlen = max(L)\n",
    "\n",
    "    pad_X = np.array([\n",
    "        np.concatenate([x, [padding_value] * (maxlen - len(x))]) if len(x) < maxlen else x[: maxlen] for x in X\n",
    "    ])\n",
    "    if debug:\n",
    "        print(\"[!] before pading {}\\n\".format(X))\n",
    "        print(\"[!] after pading {}\\n\".format(pad_X))\n",
    "    return pad_X\n",
    "    \n",
    "\n",
    "def MyChoice(Myset):\n",
    "    result = []\n",
    "    for i in Myset:\n",
    "        temp_set = set()\n",
    "        temp_set.add(i)\n",
    "        cho = choice(list(Myset - temp_set))\n",
    "        result.append(cho)\n",
    "    return result\n",
    "\n",
    "\n",
    "class data_generator:\n",
    "    \n",
    "    def __init__(self, data, batch_size=B_SIZE, shuffle=SHUFFLE):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = len(self.data) // self.batch_size\n",
    "        self.shuffle = shuffle\n",
    "        if len(self.data) % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            idxs = list(range(len(self.data)))\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(idxs)\n",
    "            T1, T2, Image1, Pos1, label_word_list, image1_mask, image1_char = [], [], [], [], [], [], []\n",
    "            S1, S2, Image2, Pos2, image2_mask, image2_char = [], [], [], [], [], [] \n",
    "            Id_set = set()\n",
    "\n",
    "            for i in idxs:\n",
    "                d = self.data.iloc[i]\n",
    "                text = d['words']\n",
    "                label_words = d['label_words']\n",
    "                \n",
    "                t1, t2 = token2id_X(text, x_dict=word_index, maxlen=cfg[\"maxlen\"])\n",
    "                image = np.array(d['features'], dtype=\"float32\")\n",
    "                image = image[: cfg[\"max_box\"]]\n",
    "                img_mask = [1 for _ in image[: cfg[\"max_box\"]]]\n",
    "                \n",
    "                pos = np.array(d['pos'], dtype=\"float32\")\n",
    "                pos = pos[: cfg[\"max_box\"]]\n",
    "                \n",
    "                image_char = [token2id_X(ent, x_dict=word_index)[0] for ent in label_words.split(IMAGE_LABEM_CONCAT_TOKEN)]\n",
    "                image_char = image_char[: cfg[\"max_box\"]]\n",
    "                # print(\"image_char\", len(image_char))\n",
    "                image_char = pad_sequences(image_char, \n",
    "                                           maxlen=cfg[\"max_char\"], \n",
    "                                           dtype='int32',\n",
    "                                           padding='post',\n",
    "                                           truncating='post',\n",
    "                                           value=cfg[\"x_pad\"])\n",
    "                \n",
    "                assert image.shape[0] == pos.shape[0]\n",
    "                assert image.shape[0] == cfg[\"max_box\"] or image.shape[0] == len(label_words.split(IMAGE_LABEM_CONCAT_TOKEN))\n",
    "                assert image_char.shape == (image.shape[0], cfg[\"max_char\"])\n",
    "\n",
    "                T1.append(t1)\n",
    "                T2.append(t2)\n",
    "                Image1.append(image)\n",
    "                image1_mask.append(img_mask)  \n",
    "                Pos1.append(pos)\n",
    "                image1_char.append(image_char)\n",
    "                Id_set.add(i)\n",
    "\n",
    "                if len(T1) == self.batch_size//2 or i == idxs[-1]:\n",
    "                    Id_new = MyChoice(Id_set)\n",
    "                    for i, id_ in enumerate(Id_new):\n",
    "                        d_new = self.data.iloc[id_]\n",
    "                        text = d_new['words']\n",
    "                        t1, t2 = token2id_X(text, x_dict=word_index, maxlen=cfg[\"maxlen\"])\n",
    "                        S1.append(t1)\n",
    "                        S2.append(t2)\n",
    "                        \n",
    "                        image = Image1[i]\n",
    "                        img_mask = image1_mask[i]\n",
    "                        pos = Pos1[i]\n",
    "                        image_char = image1_char[i]\n",
    "                        \n",
    "                        Image2.append(image)\n",
    "                        Pos2.append(pos)\n",
    "                        image2_mask.append(img_mask)\n",
    "                        image2_char.append(image_char)\n",
    "                    \n",
    "                    Y = [1] * len(T1) + [0] * len(S1)\n",
    "                   \n",
    "                    T1 = seq_padding(T1 + S1, padding_value=cfg[\"x_pad\"]) \n",
    "                    T2 = seq_padding(T2 + S2, padding_value=cfg[\"x_pad\"])\n",
    "                    \n",
    "                    Image1 = seq_padding(Image1 + Image2, \n",
    "                                         padding_value=np.zeros(shape=(2048, ))\n",
    "                                        )\n",
    "                                                         \n",
    "                    Pos1 = seq_padding(Pos1 + Pos2,\n",
    "                                       padding_value=np.zeros(shape=(5, ))\n",
    "                                      )\n",
    "                    image1_mask = seq_padding(image1_mask + image2_mask,\n",
    "                                             padding_value=0)\n",
    "                    \n",
    "                    image1_char = seq_padding(image1_char + image2_char,\n",
    "                                             padding_value=np.zeros(shape=(cfg[\"max_char\"])), debug=False)\n",
    "                    \n",
    "                    Y = np.array(Y).reshape((len(T1), -1))\n",
    "                    \n",
    "                    idx = np.arange(len(T1))\n",
    "                    np.random.shuffle(idx)\n",
    "        \n",
    "                    T1 = T1[idx]\n",
    "                    T2 = T2[idx]\n",
    "                    Image1 = Image1[idx]\n",
    "                    image1_mask = image1_mask[idx]\n",
    "                    Pos1 = Pos1[idx]\n",
    "                    image1_char = image1_char[idx]\n",
    "                    Y = Y[idx]\n",
    "                    \n",
    "                    yield [T1, T2, Image1, image1_mask, Pos1, image1_char], Y\n",
    "                    T1, T2, Image1, Pos1, label_word_list, image1_mask, image1_char = [], [], [], [], [], [], []\n",
    "                    S1, S2, Image2, Pos2, image2_mask, image2_char = [], [], [], [], [], []\n",
    "                    Id_set = set()\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (128, 11) (128, 11) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 7593.  7593.  2703. ...  2703.  7593.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [54190. 54190. 54190. ...     0.     0.     0.]\n",
      " ...\n",
      " [33096. 33096. 21740. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 6210. 29895.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...  2703.  2703.  6210.]\n",
      " [21740. 21740. 21740. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [21740. 33096.  4959. ...     0.     0.     0.]\n",
      " [ 2703.  6210.     0. ...     0.     0.     0.]]\n",
      "x (128, 14) (128, 14) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 4959. 21740. 54190. ...     0.     0.     0.]\n",
      " [ 4959.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.  6210.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 4959. 21740. 54190. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [15348.  2703.  2703. ... 54190.  2703.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 6210.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  4959.  4959. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [33096.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 14) (128, 14) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [15348.  4959.  2703. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " ...\n",
      " [21740.     0.     0. ...     0.     0.     0.]\n",
      " [21740.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 6210.  6210.     0. ...     0.     0.     0.]\n",
      " [63124.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 9887.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703. 37880.  2703. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.  4959. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [33096. 21740. 33096. ...  6210.  6210.  4959.]\n",
      " ...\n",
      " [37880.     0.     0. ...     0.     0.     0.]\n",
      " [54190.  4959. 21740. ...     0.     0.     0.]\n",
      " [37880.  2703. 37880. ...     0.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 4959. 21740.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 6210.  6210.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " ...\n",
      " [11102. 33096. 21740. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]]\n",
      "x (128, 14) (128, 14) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 6210.  6210.  6210. ...     0.     0.     0.]\n",
      " [11102. 11102.  2703. ... 11102.  2703. 11102.]\n",
      " [33096.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 6210.  6210.  6210. ...     0.     0.     0.]\n",
      " [11102.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]]\n",
      "x (128, 14) (128, 14) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[63124. 63124.  2703. ... 63124. 63124. 63124.]\n",
      " [21740.     0.     0. ...     0.     0.     0.]\n",
      " [ 6210.  6210.  6210. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [33096.  4959.  6210. ...     0.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 6210. 54190.  6210. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " ...\n",
      " [33096.     0.     0. ...     0.     0.     0.]\n",
      " [21740.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703. 63124. 63124. ...     0.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 7593.     0.     0. ...     0.     0.     0.]\n",
      " [ 6210.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [37880.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703. 33096.  2703. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703. 11102.  2703. ...  2703.  2703. 11102.]\n",
      " [21740.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 15) (128, 15) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 4959.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703. 11102. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[29895. 29895.     0. ...     0.     0.     0.]\n",
      " [37880. 37880. 37880. ...     0.     0.     0.]\n",
      " [37880. 37880.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [33096.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703. 11102. 11102. ...  2703.  2703.  2703.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 7593.  2703.  2703. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 4959. 21740. 21740. ...  6210.  6210.     0.]]\n",
      "x (128, 15) (128, 15) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[21740. 21740.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [54190.  2703.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [11102.  2703. 11102. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 6210. 21740.  6210. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703. 11102.  2703. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [33096. 21740.  4959. ...     0.     0.     0.]\n",
      " [ 6210.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [54190.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...  2703.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703. 29895.  2703. ...     0.     0.     0.]\n",
      " [54190.  4959. 21740. ...     0.     0.     0.]\n",
      " [37880.  4959.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [54190.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 6210.  6210.  6210. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [21740.  6210. 33096. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [21740.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 6210.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703. 11102. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[33096. 21740.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 7593.  6210.  2703. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 7593.  6210.  2703. ...     0.     0.     0.]\n",
      " [33096. 21740. 21740. ...     0.     0.     0.]\n",
      " [11102. 11102.  2703. ...     0.     0.     0.]]\n",
      "x (128, 14) (128, 14) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [54190. 21740.  4959. ... 54190.  4959.  4959.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [37880. 37880.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [54190. 21740. 21740. ... 33096. 54190. 54190.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [11102. 11102.     0. ...     0.     0.     0.]\n",
      " [37880. 37880.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]]\n",
      "x (128, 11) (128, 11) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703. 54190.  2703. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.  4959. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 11) (128, 11) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[2703. 2703.    0. ...    0.    0.    0.]\n",
      " [2703.    0.    0. ...    0.    0.    0.]\n",
      " [2703. 2703. 2703. ...    0.    0.    0.]\n",
      " ...\n",
      " [2703.    0.    0. ...    0.    0.    0.]\n",
      " [2703.    0.    0. ...    0.    0.    0.]\n",
      " [2703.    0.    0. ...    0.    0.    0.]]\n",
      "x (128, 10) (128, 10) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.  2703. ... 11102. 11102.  2703.]\n",
      " [11102.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  6210.  6210. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703. 11102.  2703. ...     0.     0.     0.]\n",
      " [29895.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.  6210. ...     0.     0.     0.]\n",
      " [54190. 21740.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 14) (128, 14) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 6210.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 6210. 21740. 33096. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703. 63124. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  7593. ...  2703.  7593.  2703.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " ...\n",
      " [33096.     0.     0. ...     0.     0.     0.]\n",
      " [37880.  2703. 37880. ...  2703.  2703.  2703.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]]\n",
      "x (128, 14) (128, 14) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 7593.  2703.  2703. ...  7593.  2703.  7593.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [15348.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 6210. 33096.  6210. ...     0.     0.     0.]\n",
      " [ 2703. 54190.     0. ...     0.     0.     0.]\n",
      " [ 7593.  7593.  2703. ...  2703.  2703.  2703.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  6210. 15348. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 4959.  2703. 21740. ...  2703. 54190. 54190.]\n",
      " ...\n",
      " [ 2703.  2703. 29895. ...     0.     0.     0.]\n",
      " [ 2703.  7593.  7593. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [21740. 33096. 33096. ...     0.     0.     0.]\n",
      " [37880.  2703.  2703. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 6210.  6210.  6210. ...     0.     0.     0.]\n",
      " [ 6210.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703. 33096. 33096. ...  6210.  2703.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[54190.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...  2703.  2703.  2703.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 4959. 15348.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  6210. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 6210.  6210. 54190. ...     0.     0.     0.]\n",
      " [15348. 29895. 54190. ...     0.     0.     0.]\n",
      " ...\n",
      " [29895.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[11102. 11102.     0. ...     0.     0.     0.]\n",
      " [ 2703. 29895. 37880. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 4959.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [21740.  4959.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [29895.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 4959. 21740. 33096. ...     0.     0.     0.]\n",
      " [33096.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [33096. 33096. 33096. ...     0.     0.     0.]\n",
      " [11102.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 14) (128, 14) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [37880. 37880. 37880. ...  2703. 37880.  2703.]\n",
      " ...\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [54190.  4959. 21740. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.  2703. ...  2703.  2703.  2703.]\n",
      " [63124.  2703.  2703. ...     0.     0.     0.]\n",
      " [54190. 54190. 54190. ... 54190. 54190. 54190.]\n",
      " ...\n",
      " [ 4959. 15348.     0. ...     0.     0.     0.]\n",
      " [21740.     0.     0. ...     0.     0.     0.]\n",
      " [33096.  6210.  6210. ...     0.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703. 11102.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703. 11102.  2703. ...  2703. 11102.     0.]\n",
      " [21740.  4959. 54190. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703. 63124.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 6210.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]]\n",
      "x (128, 14) (128, 14) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[15348.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  4959. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 4959. 15348.     0. ...     0.     0.     0.]\n",
      " [ 2703.  6210.  6210. ... 21740.  4959. 33096.]\n",
      " [54190.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 14) (128, 14) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[21740. 21740. 21740. ...     0.     0.     0.]\n",
      " [ 4959. 21740.     0. ...     0.     0.     0.]\n",
      " [11102.  2703. 11102. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 4959. 21740. 21740. ...     0.     0.     0.]\n",
      " [33096.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  4959.  6210. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 4959. 21740. 33096. ...     0.     0.     0.]\n",
      " [63124.  2703.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [21740. 33096.  4959. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...  2703.  2703.  2703.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 14) (128, 14) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703. 10430.     0. ...     0.     0.     0.]\n",
      " [37880. 37880. 37880. ...     0.     0.     0.]\n",
      " [63124. 63124. 63124. ...  2703.  2703. 63124.]\n",
      " ...\n",
      " [ 2703.  2703.  2703. ...  2703. 11102.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 11) (128, 11) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[11102. 11102. 11102. ...  2703. 11102. 11102.]\n",
      " [ 2703. 11102.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [21740.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...  2703.  2703.     0.]]\n",
      "x (128, 11) (128, 11) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 9887.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [21740. 21740.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [54190.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[15348. 33096. 54190. ...     0.     0.     0.]\n",
      " [63124.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...  7593.  2703.     0.]\n",
      " ...\n",
      " [15348. 33096. 54190. ...     0.     0.     0.]\n",
      " [54190.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703. 21740.     0. ...     0.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[11102. 11102. 11102. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...  2703.  2703.  2703.]\n",
      " [ 2703. 63124.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.  2703. ...  2703.  2703.  2703.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 15) (128, 15) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703. 63124.  2703. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [21740. 54190.     0. ...     0.     0.     0.]]\n",
      "x (128, 11) (128, 11) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 6210.  6210.  6210. ...     0.     0.     0.]\n",
      " [11102. 11102. 11102. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.  2703. 21740. ...     0.     0.     0.]\n",
      " [54190. 21740.  4959. ...  6210. 21740.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [21740.  2703.  2703. ... 21740.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 4959.  4959.  4959. ...     0.     0.     0.]\n",
      " [ 6210.     0.     0. ...     0.     0.     0.]\n",
      " [21740. 21740. 21740. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 6210. 33096.  6210. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.  2703. ... 63124.  2703.     0.]\n",
      " [11102. 11102.     0. ...     0.     0.     0.]\n",
      " [ 6210.  6210.  6210. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[37880.  2703. 37880. ...     0.     0.     0.]\n",
      " [21740. 33096.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [33096.  4959.  6210. ...     0.     0.     0.]\n",
      " [ 6210.  6210.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]]\n",
      "x (128, 14) (128, 14) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [21740.     0.     0. ...     0.     0.     0.]\n",
      " [29895. 21740.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 4959. 54190. 21740. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.  7593.  2703. ...     0.     0.     0.]]\n",
      "x (128, 11) (128, 11) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.  7593.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703. 33096.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 4959. 21740.  2703. ...     0.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 6210.  6210.  6210. ...     0.     0.     0.]\n",
      " [ 2703. 63124.     0. ...     0.     0.     0.]\n",
      " [29895. 54190. 15348. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703. 11102.  2703. ...     0.     0.     0.]\n",
      " [21740.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  6210.  6210. ...  6210.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  7593.  2703. ...  2703.  2703.  2703.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703. 33096. 33096. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703. 37880. ...  2703.  2703.  2703.]\n",
      " [21740.     0.     0. ...     0.     0.     0.]\n",
      " [54190. 54190. 54190. ...     0.     0.     0.]]\n",
      "x (128, 14) (128, 14) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.  2703. ...  4959.     0.     0.]\n",
      " [63124.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [54190. 21740. 54190. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]]\n",
      "x (128, 14) (128, 14) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[7593.    0.    0. ...    0.    0.    0.]\n",
      " [2703. 7593. 7593. ...    0.    0.    0.]\n",
      " [2703. 4959.    0. ...    0.    0.    0.]\n",
      " ...\n",
      " [2703. 2703.    0. ...    0.    0.    0.]\n",
      " [7593. 2703. 2703. ...    0.    0.    0.]\n",
      " [2703.    0.    0. ...    0.    0.    0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.  6210.  6210. ...     0.     0.     0.]\n",
      " ...\n",
      " [21740. 21740.     0. ...     0.     0.     0.]\n",
      " [ 7593. 21740. 29895. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]]\n",
      "x (128, 14) (128, 14) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[63124. 63124.  2703. ... 63124.  2703. 63124.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703. 63124. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 6210.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]]\n",
      "x (128, 11) (128, 11) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [54190.  2703.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703. 29895. 11102. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 11) (128, 11) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703. 29895. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...  2703.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.  2703. ...  2703.  2703.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [21740. 54190.  4959. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[37880. 37880. 37880. ... 37880.  2703.     0.]\n",
      " [54190. 33096. 54190. ...     0.     0.     0.]\n",
      " [ 6210.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [21740.  2703.  4959. ...     0.     0.     0.]\n",
      " [21740. 33096.  6210. ...     0.     0.     0.]\n",
      " [21740. 33096.  6210. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 6210.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [21740. 21740.  4959. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  6210.  6210. ...  6210.  6210.  6210.]\n",
      " [54190.  2703. 63124. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ... 10430. 10430.     0.]]\n",
      "x (128, 11) (128, 11) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.  2703. ...  9887.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [21740. 21740. 21740. ...     0.     0.     0.]\n",
      " ...\n",
      " [33096. 54190.  4959. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [29895. 21740. 21740. ... 54190.  6210.  6210.]\n",
      " [11102. 11102. 11102. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 14) (128, 14) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[21740. 21740.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 4959.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [33096.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 11) (128, 11) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[21740.  6210.  6210. ... 33096.  6210.     0.]\n",
      " [ 6210.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [37880.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[54190. 54190. 63124. ...     0.     0.     0.]\n",
      " [ 7593.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 6210.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703. 54190.  4959. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [21740.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 6210.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [11102.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 6210.  6210.  7593. ... 15348.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[21740. 11102. 11102. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703. 63124. 63124. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[21740.  4959.     0. ...     0.     0.     0.]\n",
      " [21740.     0.     0. ...     0.     0.     0.]\n",
      " [11102. 11102. 11102. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [21740. 33096. 33096. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703. 11102.  4959. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 10) (128, 10) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[21740. 21740.  6210. ...     0.     0.     0.]\n",
      " [37880.  2703.  2703. ...  2703.  2703.     0.]\n",
      " [ 6210. 33096. 33096. ...  6210.  6210.  6210.]\n",
      " ...\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [37880.  2703.  2703. ...     0.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 8, 2048) (128, 8) (128, 8, 5) (128, 8, 8) (128, 1)\n",
      "[[11102.  6210. 33096. ...     0.     0.     0.]\n",
      " [63124. 54190. 54190. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " ...\n",
      " [37880.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [21740.  2703.  4959. ...     0.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.  2703. ...  2703.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 6210.  6210.  6210. ...  6210.  6210.  6210.]\n",
      " ...\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 7593.  2703. 63124. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [11102.  2703.  6210. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.  2703. ...  2703.  2703.     0.]\n",
      " [11102. 11102. 11102. ... 11102. 11102.     0.]\n",
      " [ 6210.  6210.     0. ...     0.     0.     0.]]\n",
      "x (128, 11) (128, 11) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 6210.  6210.     0. ...     0.     0.     0.]\n",
      " [ 7593.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703. 11102. 11102. ... 11102. 11102.     0.]\n",
      " ...\n",
      " [ 2703.  9887.  9887. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 9887.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 6210.     0.     0. ...     0.     0.     0.]\n",
      " [15348.     0.     0. ...     0.     0.     0.]\n",
      " [21740.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [54190.  4959. 54190. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[2703.    0.    0. ...    0.    0.    0.]\n",
      " [2703. 2703.    0. ...    0.    0.    0.]\n",
      " [2703. 2703.    0. ...    0.    0.    0.]\n",
      " ...\n",
      " [2703. 2703. 7593. ...    0.    0.    0.]\n",
      " [6210.    0.    0. ...    0.    0.    0.]\n",
      " [2703.    0.    0. ...    0.    0.    0.]]\n",
      "x (128, 15) (128, 15) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[54190. 54190.     0. ...     0.     0.     0.]\n",
      " [ 2703. 21740.  2703. ...     0.     0.     0.]\n",
      " [ 7593.  2703.  7593. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 6210.  2703.  6210. ...     0.     0.     0.]\n",
      " [21740.  4959.     0. ...     0.     0.     0.]\n",
      " [ 9887.  9887.  2703. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703. 21740. 54190. ...  2703.     0.     0.]\n",
      " [ 4959. 21740. 29895. ... 54190.  6210.  6210.]\n",
      " [21740.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [54190. 15348. 21740. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...  2703.  2703. 63124.]]\n",
      "x (128, 14) (128, 14) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 4959. 21740.  2703. ...     0.     0.     0.]\n",
      " [29895. 21740.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [37880. 37880. 37880. ...     0.     0.     0.]\n",
      " [54190. 21740. 54190. ...     0.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703. 29895.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 6210.  6210.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]]\n",
      "x (128, 11) (128, 11) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703. 37880.  2703. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]]\n",
      "x (128, 11) (128, 11) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 4959.  4959. 21740. ...  4959. 54190.  4959.]\n",
      " [ 2703. 11102.     0. ...     0.     0.     0.]\n",
      " [ 9887.     0.     0. ...     0.     0.     0.]]\n",
      "x (128, 13) (128, 13) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 6210.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703. 54190.  2703. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " ...\n",
      " [11102.  2703.  2703. ...     0.     0.     0.]\n",
      " [11102.  2703.  2703. ...     0.     0.     0.]\n",
      " [37880.  2703.     0. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 2703. 11102.  2703. ...     0.     0.     0.]\n",
      " [37880. 37880.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [33096. 29895. 33096. ...     0.     0.     0.]\n",
      " [54190.     0.     0. ...     0.     0.     0.]\n",
      " [21740. 29895.  2703. ...     0.     0.     0.]]\n",
      "x (128, 10) (128, 10) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [11102.  2703. 11102. ...     0.     0.     0.]\n",
      " [54190. 54190. 54190. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [21740. 21740.     0. ...     0.     0.     0.]\n",
      " [ 6210. 21740.  2703. ...     0.     0.     0.]\n",
      " ...\n",
      " [ 2703.  2703.     0. ...     0.     0.     0.]\n",
      " [ 6210.  6210.     0. ...     0.     0.     0.]\n",
      " [ 2703. 29895.     0. ...     0.     0.     0.]]\n",
      "x (128, 11) (128, 11) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [63124. 63124. 63124. ... 63124. 63124. 63124.]\n",
      " ...\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [63124.  2703. 63124. ...     0.     0.     0.]]\n",
      "x (128, 12) (128, 12) (128, 10, 2048) (128, 10) (128, 10, 5) (128, 10, 8) (128, 1)\n",
      "[[ 2703.  2703.  2703. ...     0.     0.     0.]\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 2703.  2703. 29895. ...  2703.  2703.  2703.]\n",
      " ...\n",
      " [ 2703.     0.     0. ...     0.     0.     0.]\n",
      " [ 6210.     0.     0. ...     0.     0.     0.]\n",
      " [11102.     0.     0. ...     0.     0.     0.]]\n"
     ]
    }
   ],
   "source": [
    "train_D = data_generator(train_data)\n",
    "_i  = 0\n",
    "for d in train_D:\n",
    "    _i += 1\n",
    "    if  _i > 100:\n",
    "        break\n",
    "    print('x',d[0][0].shape, d[0][1].shape,d[0][2].shape, d[0][3].shape, d[0][4].shape, d[0][5].shape, d[1].shape)\n",
    "    print(d[0][5].sum(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluate(Callback):\n",
    "    def __init__(self, filename=None):\n",
    "        self.score = []\n",
    "        self.best = 0.\n",
    "        self.filename = filename\n",
    "       \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch ==  0:\n",
    "            print(\"[!] test load&save model\")\n",
    "            f = self.filename + \".h5\"\n",
    "            custom_objects = get_custom_objects()\n",
    "            self.model.save(f, include_optimizer=False, overwrite=True)\n",
    "            if \"bert\" in cfg[\"verbose\"]:\n",
    "                model_ = load_model(f, custom_objects=custom_objects)  \n",
    "            else:\n",
    "                model_ = load_model(f) \n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "#         if epoch + 1 < 5:\n",
    "#             return\n",
    "        score = self.evaluate(self.model)\n",
    "        self.score.append((epoch, score))\n",
    "        \n",
    "        if epoch + 1 in SAVE_EPOCHS:\n",
    "            self.model.save(self.filename + \"_{}.h5\".format(epoch + 1), include_optimizer=False)             \n",
    "        if score > self.best:\n",
    "            self.model.save(self.filename + \".h5\", include_optimizer=False)\n",
    "            \n",
    "        if score > self.best:\n",
    "            self.best = score\n",
    "            print(\"[!] epoch = {}, new best score = {}\".format(epoch + 1,  score))\n",
    "        print('[!] epoch = {}, score = {}, best score: {}\\n'.format(epoch + 1, score, self.best))\n",
    "\n",
    "    def evaluate(self, model):\n",
    "        result = defaultdict(list)\n",
    "        for i in trange(len(val_data)):\n",
    "            d = val_data.iloc[i]\n",
    "            qid = d['query_id']\n",
    "            pid = d['product_id']\n",
    "            text = d['query']\n",
    "            label_words = d['label_words']\n",
    "            t1, t2 = token2id_X(text, x_dict=word_index, maxlen=cfg[\"maxlen\"])\n",
    "            \n",
    "            image = np.array(d['feature_convert'], dtype=\"float32\")\n",
    "            image = image[: cfg[\"max_box\"]]\n",
    "            img_mask = [1 for _ in image[: cfg[\"max_box\"]]]                   \n",
    "            pos = np.array(d['pos'], dtype=\"float32\")\n",
    "            pos = pos[: cfg[\"max_box\"]]\n",
    "            \n",
    "            image_char = [token2id_X(ent, x_dict=word_index)[0] for ent in label_words.split(IMAGE_LABEM_CONCAT_TOKEN)]\n",
    "            image_char = image_char[: cfg[\"max_box\"]]\n",
    "            image_char = pad_sequences(image_char, \n",
    "                                       maxlen=cfg[\"max_char\"], \n",
    "                                       dtype='int32',\n",
    "                                       padding='post',\n",
    "                                       truncating='post',\n",
    "                                       value=cfg[\"x_pad\"]) \n",
    "            output = model.predict([[t1], [t2], [image], [img_mask], [pos], [image_char]])\n",
    "            result[qid].append((pid, output[0][1]))\n",
    "            \n",
    "        query_id,product1,product2,product3,product4,product5 = [],[],[],[],[],[]\n",
    "        for key in result.keys():\n",
    "            rlist = result[key]\n",
    "            rlist.sort(key=lambda x: x[1], reverse=True)\n",
    "            query_id.append(key)\n",
    "            product1.append(rlist[0][0])\n",
    "            product2.append(rlist[1][0])\n",
    "            product3.append(rlist[2][0])\n",
    "            product4.append(rlist[3][0])\n",
    "            product5.append(rlist[4][0])\n",
    "        sub = pd.DataFrame({'query-id':query_id,\n",
    "                            'product1':product1,\n",
    "                            'product2':product2,\n",
    "                            'product3':product3,\n",
    "                            'product4':product4,\n",
    "                            'product5':product5,\n",
    "\n",
    "        })\n",
    "        sub.to_csv('../result/val_submission.csv',index=0)\n",
    "        \n",
    "        reference = json.load(open(VAL_ANS_PATH))\n",
    "        \n",
    "        k = 5\n",
    "        predictions = read_submission('../result/val_submission.csv', reference, k)\n",
    "\n",
    "        score_sum = 0.\n",
    "        for qid in reference.keys():\n",
    "            ground_truth_ids = set([str(pid) for pid in reference[qid]])\n",
    "            ref_vec = [1.0] * len(ground_truth_ids)\n",
    "            pred_vec = [1.0 if pid in ground_truth_ids else 0.0 for pid in predictions[qid]]\n",
    "            score_sum += get_ndcg(pred_vec, ref_vec, k)\n",
    "        score = score_sum / len(reference)\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[!] fold_id = -1 starting\n",
      "{'verbose': '[image-bert-concat-query]-wwm_uncased_L12-768_v3_quart', 'base_dir': '../data/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/', 'maxlen': 20, 'max_box': 10, 'max_char': 8, 'lr': 0.0001, 'min_lr': 6e-08, 'opt': 'nadam', 'loss_w': 20.0, 'trainable': True, 'bert_trainable': True, 'mix_mode': '', 'unit1_1': 128, 'accum_step': 1, 'cls_num': 4, 'raw_filename': '{}_{}oof{}', 'x_pad': 0, 'filename': '[image-bert-concat-query]-wwm_uncased_L12-768_v3_quart_20oof-1', 'num_example': 10000}\n",
      "../data/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/bert_config.json ../data/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/bert_model.ckpt\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image_char_input (InputLayer)   (None, None, 8)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "CharEmbedding (TimeDistributed) (None, None, 128)    18544928    image_char_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "image_input (InputLayer)        (None, None, 2048)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 2176)   0           CharEmbedding[0][0]              \n",
      "                                                                 image_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "image_pos_input (InputLayer)    (None, None, 5)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "query_token_input (InputLayer)  (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "image_embed (Dense)             (None, None, 512)    1114624     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "image_mask_input (InputLayer)   (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pos_embed (Dense)               (None, None, 512)    3072        image_pos_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "embed_layer (Embedding)         (None, None, 600)    18314400    query_token_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "token_mask (Lambda)             (None, None)         0           query_token_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, None, 512)    0           image_embed[0][0]                \n",
      "                                                                 image_mask_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, None, 512)    0           pos_embed[0][0]                  \n",
      "                                                                 image_mask_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 600)    0           embed_layer[0][0]                \n",
      "                                                                 token_mask[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, None, 512)    0           lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "query_segm_input (InputLayer)   (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 128)    747520      lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 1152)   15353856    add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 multiple             103788288   query_token_input[0][0]          \n",
      "                                                                 query_segm_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 128)    0           bidirectional_1[0][0]            \n",
      "                                                                 token_mask[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, None, 1152)   0           bidirectional_2[0][0]            \n",
      "                                                                 image_mask_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 896)    0           model_3[1][0]                    \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 1152)         0           lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 896)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 2048)         0           lambda_7[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2048)         4196352     concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          1049088     dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          65664       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            258         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 144,863,650\n",
      "Trainable params: 144,863,650\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "No OpKernel was registered to support Op 'CudnnRNN' used by node bidirectional_1/CudnnRNN (defined at C:\\Users\\funny\\anaconda3\\lib\\site-packages\\keras\\layers\\cudnn_recurrent.py:517) with these attrs: [seed=2021, dropout=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\", seed2=0, is_training=true]\nRegistered devices: [CPU]\nRegistered kernels:\n  <no registered kernels>\n\n\t [[bidirectional_1/CudnnRNN]]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node bidirectional_1/CudnnRNN:\n bidirectional_1/transpose (defined at C:\\Users\\funny\\anaconda3\\lib\\site-packages\\keras\\layers\\cudnn_recurrent.py:484)\t\n bidirectional_1/ExpandDims_2 (defined at C:\\Users\\funny\\anaconda3\\lib\\site-packages\\keras\\layers\\cudnn_recurrent.py:488)\t\n bidirectional_1/ExpandDims_1 (defined at C:\\Users\\funny\\anaconda3\\lib\\site-packages\\keras\\layers\\cudnn_recurrent.py:487)\t\n bidirectional_1/concat (defined at C:\\Users\\funny\\anaconda3\\lib\\site-packages\\keras\\layers\\cudnn_recurrent.py:60)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1355\u001b[0m   \u001b[0m_DEAD_HANDLES_THRESHOLD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_register_dead_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m       \u001b[0mnode_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1339\u001b[1;33m       \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1340\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m         \u001b[0mfeeds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mholder\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_handle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m         \u001b[0mfetches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeleter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeeds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: No OpKernel was registered to support Op 'CudnnRNN' used by {{node bidirectional_1/CudnnRNN}}with these attrs: [seed=2021, dropout=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\", seed2=0, is_training=true]\nRegistered devices: [CPU]\nRegistered kernels:\n  <no registered kernels>\n\n\t [[bidirectional_1/CudnnRNN]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-ee02b6a7cea1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m                           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_EPOCH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                           \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                           \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m                           )\n\u001b[0;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\\n[!] fold_id = {} finish\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfold_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    331\u001b[0m                     \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdates\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmetrics_updates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m                     \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train_function'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m                     **self._function_kwargs)\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, updates, **kwargs)\u001b[0m\n\u001b[0;32m   3004\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3005\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_tf_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3006\u001b[1;33m         \u001b[0mv1_variable_initialization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3007\u001b[0m     return tf_keras_backend.function(inputs, outputs,\n\u001b[0;32m   3008\u001b[0m                                      \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mv1_variable_initialization\u001b[1;34m()\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mv1_variable_initialization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m     \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[1;34m()\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[1;34m'`get_session` is not available when '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m             'TensorFlow is executing eagerly.')\n\u001b[1;32m--> 385\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf_keras_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mget_session\u001b[1;34m(op_input_list)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m \u001b[1;33m@\u001b[0m\u001b[0mtf_export\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keras.backend.get_session'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m   \"\"\"Returns the TF session to be used by the backend.\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[1;34m(session)\u001b[0m\n\u001b[0;32m    877\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 879\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    880\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mis_placeholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m   \"\"\"Returns whether `x` is a placeholder.\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0mpartial_run\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mcalls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m     \u001b[0mThe\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0margument\u001b[0m \u001b[0mallows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcaller\u001b[0m \u001b[0mto\u001b[0m \u001b[0moverride\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[0mthe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mSee\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0minformation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[0mArgs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1173\u001b[1;33m       \u001b[0mfetches\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mto\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1174\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdetails\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mallowable\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mfeed_list\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOptional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mSee\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1368\u001b[0m       \u001b[0mfeeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m       \u001b[0mfetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0mdeleter_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_handle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors_to_delete\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m         holder, deleter = session_ops._get_handle_deleter(\n\u001b[0;32m   1372\u001b[0m             self.graph, deleter_key, tensor_handle)\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: No OpKernel was registered to support Op 'CudnnRNN' used by node bidirectional_1/CudnnRNN (defined at C:\\Users\\funny\\anaconda3\\lib\\site-packages\\keras\\layers\\cudnn_recurrent.py:517) with these attrs: [seed=2021, dropout=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\", seed2=0, is_training=true]\nRegistered devices: [CPU]\nRegistered kernels:\n  <no registered kernels>\n\n\t [[bidirectional_1/CudnnRNN]]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node bidirectional_1/CudnnRNN:\n bidirectional_1/transpose (defined at C:\\Users\\funny\\anaconda3\\lib\\site-packages\\keras\\layers\\cudnn_recurrent.py:484)\t\n bidirectional_1/ExpandDims_2 (defined at C:\\Users\\funny\\anaconda3\\lib\\site-packages\\keras\\layers\\cudnn_recurrent.py:488)\t\n bidirectional_1/ExpandDims_1 (defined at C:\\Users\\funny\\anaconda3\\lib\\site-packages\\keras\\layers\\cudnn_recurrent.py:487)\t\n bidirectional_1/concat (defined at C:\\Users\\funny\\anaconda3\\lib\\site-packages\\keras\\layers\\cudnn_recurrent.py:60)"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.rnn import *\n",
    "gc.collect()\n",
    "fold_id = -1\n",
    "print(\"\\n\\n[!] fold_id = {} starting\".format(fold_id))\n",
    "cfg[\"filename\"] = cfg[\"raw_filename\"].format(cfg[\"verbose\"], FOLD_NUM, fold_id)\n",
    "cfg[\"num_example\"] = len(train_data)\n",
    "\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "seed(SEED - fold_id)\n",
    "np.random.seed(SEED - fold_id)\n",
    "tf.random.set_random_seed(SEED - fold_id)\n",
    "train_D = data_generator(train_data)\n",
    "print(cfg)\n",
    "model = build_model(cfg, summary=True, \n",
    "                    word_embedding_matrix=word_embedding_matrix,\n",
    "                    )\n",
    "evaluator = Evaluate(filename=cfg[\"filename\"])\n",
    "model.fit_generator(train_D.__iter__(),\n",
    "                          steps_per_epoch=len(train_D),\n",
    "                          epochs=MAX_EPOCH,\n",
    "                          callbacks=[evaluator],\n",
    "                          shuffle=True\n",
    "                          )\n",
    "print(\"\\n\\n[!] fold_id = {} finish\".format(fold_id))\n",
    "del model, evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 1}\n",
    "    )\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "with open('../data/test_data.pkl', 'rb') as outp:\n",
    "    test_data = pickle.load(outp)\n",
    "\n",
    "f = cfg[\"filename\"] + \".h5\"\n",
    "if \"bert\" in cfg[\"verbose\"]:\n",
    "    custom_objects = get_custom_objects()\n",
    "    model = load_model(f, custom_objects=custom_objects)  \n",
    "else:\n",
    "    model = load_model(f)\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28830/28830 [09:27<00:00, 50.77it/s]\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "result = defaultdict(list)\n",
    "for i in trange(len(test_data)):\n",
    "    d = test_data.iloc[i]\n",
    "    qid = d['query_id']\n",
    "    pid = d['product_id']\n",
    "    text = d['query']\n",
    "    label_words = d['label_words']\n",
    "    t1, t2 = token2id_X(text, x_dict=word_index, maxlen=cfg[\"maxlen\"])\n",
    "\n",
    "    image = np.array(d['feature_convert'], dtype=\"float32\")\n",
    "    image = image[: cfg[\"max_box\"]]\n",
    "    img_mask = [1 for _ in image[: cfg[\"max_box\"]]]                   \n",
    "    pos = np.array(d['pos'], dtype=\"float32\")\n",
    "    pos = pos[: cfg[\"max_box\"]]\n",
    "\n",
    "    image_char = [token2id_X(ent, x_dict=word_index)[0] for ent in label_words.split(IMAGE_LABEM_CONCAT_TOKEN)]\n",
    "    image_char = image_char[: cfg[\"max_box\"]]\n",
    "    image_char = pad_sequences(image_char, \n",
    "                               maxlen=cfg[\"max_char\"], \n",
    "                               dtype='int32',\n",
    "                               padding='post',\n",
    "                               truncating='post',\n",
    "                               value=cfg[\"x_pad\"]) \n",
    "    output = model.predict([[t1], [t2], [image], [img_mask], [pos], [image_char]])\n",
    "    result[qid].append((pid, output[0][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_id,product1,product2,product3,product4,product5 = [],[],[],[],[],[]\n",
    "for key in result.keys():\n",
    "    rlist = result[key]\n",
    "    rlist.sort(key=lambda x: x[1], reverse=True)\n",
    "    query_id.append(key)\n",
    "    product1.append(rlist[0][0])\n",
    "    product2.append(rlist[1][0])\n",
    "    product3.append(rlist[2][0])\n",
    "    product4.append(rlist[3][0])\n",
    "    product5.append(rlist[4][0])\n",
    "\n",
    "sub = pd.DataFrame({'query-id':query_id,\n",
    "                    'product1':product1,\n",
    "                    'product2':product2,\n",
    "                    'product3':product3,\n",
    "                    'product4':product4,\n",
    "                    'product5':product5,\n",
    "\n",
    "})\n",
    "\n",
    "sub.to_csv('../result/submission.csv',index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query-id</th>\n",
       "      <th>product1</th>\n",
       "      <th>product2</th>\n",
       "      <th>product3</th>\n",
       "      <th>product4</th>\n",
       "      <th>product5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>103032634</td>\n",
       "      <td>103053460</td>\n",
       "      <td>103006984</td>\n",
       "      <td>103036197</td>\n",
       "      <td>103035030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>103047501</td>\n",
       "      <td>103058164</td>\n",
       "      <td>103050362</td>\n",
       "      <td>103037741</td>\n",
       "      <td>103000844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>103040094</td>\n",
       "      <td>103047101</td>\n",
       "      <td>103045222</td>\n",
       "      <td>103003414</td>\n",
       "      <td>103054005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>103020362</td>\n",
       "      <td>103064876</td>\n",
       "      <td>103007742</td>\n",
       "      <td>103032254</td>\n",
       "      <td>103031371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>103061976</td>\n",
       "      <td>103032810</td>\n",
       "      <td>103064801</td>\n",
       "      <td>103054847</td>\n",
       "      <td>103022805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query-id   product1   product2   product3   product4   product5\n",
       "0         0  103032634  103053460  103006984  103036197  103035030\n",
       "1         1  103047501  103058164  103050362  103037741  103000844\n",
       "2         2  103040094  103047101  103045222  103003414  103054005\n",
       "3         3  103020362  103064876  103007742  103032254  103031371\n",
       "4         4  103061976  103032810  103064801  103054847  103022805"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
